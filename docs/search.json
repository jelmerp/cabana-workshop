[
  {
    "objectID": "rstudio.html#steps-to-start-an-rstudio-session-at-osc",
    "href": "rstudio.html#steps-to-start-an-rstudio-session-at-osc",
    "title": "Starting an RStudio session at OSC",
    "section": "Steps to start an RStudio session at OSC",
    "text": "Steps to start an RStudio session at OSC\n\nIf you haven’t already: create an OSC account by following the link in the email you received from the Ohio Supercomputer Center (OSC).\nGo to https://ondemand.osc.edu and log in — use the simple username + password login on the left.\nClick on Interactive Apps (top bar) and then RStudio Server (all the way at the bottom)\nFill out the form as follows:\n\nCluster: Pitzer\nR version: 4.3.0\nProject: PAS2250\nNumber of hours: 4\nNode type: any\nNumber of cores: 2\n\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nClick the big blue Launch button at the bottom\nNow, you should be sent to a new page with a box at the top for your RStudio Server “job”, which should initially be “Queued” (waiting to start).\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nYour job should start running very soon, with the top bar of the box turning green and saying “Running”.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nClick Connect to RStudio Server at the bottom of the box, and an RStudio Server instance will open in a new browser tab. You’re ready to go!\n\n\n\n\n\n\n\n\nHaving trouble installing the tidyverse package?\n\n\n\nIf you are using RStudio at OSC for your homework, and the installation of the tidyverse package (or another package) with install.packages() fails, please load the packages that the Carpentries lesson uses as follows (no installation needed):\n\ncustom_library &lt;- \"/fs/scratch/PAS2250/ENT6703/share/rlib\"\nlibrary(tidyverse, lib.loc = custom_library)\nlibrary(hexbin, lib.loc = libdir)\nlibrary(patchwork, lib.loc = libdir)\nlibrary(RSQLite, lib.loc = libdir)"
  },
  {
    "objectID": "open_shell.html",
    "href": "open_shell.html",
    "title": "Opening a Unix Shell at OSC",
    "section": "",
    "text": "If you’re going to use a Unix shell at OSC for your homework, make sure to also run the step at the bottom of this page, to copy the data set for the Software Carpentry lession."
  },
  {
    "objectID": "open_shell.html#open-a-unix-shell-at-the-ohio-supercomputer-center-osc",
    "href": "open_shell.html#open-a-unix-shell-at-the-ohio-supercomputer-center-osc",
    "title": "Opening a Unix Shell at OSC",
    "section": "Open a Unix shell at the Ohio Supercomputer Center (OSC)",
    "text": "Open a Unix shell at the Ohio Supercomputer Center (OSC)\n\nIf you haven’t already: create an OSC account by following the link in the email you received from OSC.\nGo to https://ondemand.osc.edu and log in — use the simple username + password login on the left.\nOnce you are logged in, you should see a landing page similar to the one below:\n\n\n\n\n\nClick on the Clusters dropdown menu in the top blue bar, and then on Pitzer Shell Access:\n\n\n\n\n\n\n\nThis should open a new browser tab that looks approximately like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour screen will be slightly different from the screenshot above:\n\n\n\n\nYou should see even more “welcome messages” (which is everything except for the last line), and some of these are personalized, so they will differ for you.\nThe last line in the screenshot (jelmer@pitzer-login04 ~]$) is your shell “prompt”, which is personalized as well:\n\nYou will see your own username (not jelmer) before the @\nYou may be on a different “login node” so may e.g. see @pitzer-login03"
  },
  {
    "objectID": "open_shell.html#copy-the-software-carpentry-data-set",
    "href": "open_shell.html#copy-the-software-carpentry-data-set",
    "title": "Opening a Unix Shell at OSC",
    "section": "Copy the Software Carpentry data set",
    "text": "Copy the Software Carpentry data set\nIf you opened a shell at OSC to do the Unix shell homework, then please also perform the following step:\n\nCopy and paste the following code in your Unix shell, and then press Enter:\n\nmkdir Desktop\ncp -r /fs/scratch/PAS2250/cabana/homework/shell/shell-lesson-data Desktop/\nThe above code created a folder (directory) called “Desktop” in your home directory, and copied the Software Carpentry dataset into the new Desktop folder. Now, you should be able to skip everything on the Software Carpentry “Summary and Setup” page.\n\n\n\n\n\n\n\nIf you run into any problems, don’t hesitate to email Jelmer Poelstra."
  },
  {
    "objectID": "04_amplicon.html#data-secuenciada",
    "href": "04_amplicon.html#data-secuenciada",
    "title": "Amplicon sequencing for Phytophthora infestans populations",
    "section": "1 Data Secuenciada",
    "text": "1 Data Secuenciada\nListado de muestras (n=55) a analizar por alumno."
  },
  {
    "objectID": "04_amplicon.html#servidor",
    "href": "04_amplicon.html#servidor",
    "title": "Amplicon sequencing for Phytophthora infestans populations",
    "section": "2 Servidor",
    "text": "2 Servidor\n\n\n\n\n\nIngresar SupercomputerCenter. Ingresar su usuario y contraseña:\n\n\n\n\n\n\n2.1 Definiendo el directorio de trabajo\nEl primer paso es definir un directorio de trabajo. El directorio o carpeta de trabajo es aquel en que se va a buscar los archivos\n\n2.1.1 Dar click en Files y escoger /fs/scratch/PAS2250/mizarra\n\n\n2.1.2 Buscar su usuario y entrar al folder\n\n\n\n2.1.3 Abrir VS Code\n\nYou should already have a VS Code session, but if not, see the instructions below.\n\n\n\n\n\n\n\nStarting VS Code at OSC - Instructions in brief (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2250\nThe starting directory: /fs/scratch/PAS2250/ENT6703/&lt;your_personal_dir&gt;\nNumber of hours: 10\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\nIngresar a su carpeta:\n\ncd /fs/scratch/PAS2250/cabana/$USER\n\n\n\n2.2 Archivos Ingresar a cada carpeta del usuario según sus apellidos\n\nReads:\n\nls ../mizarra/***usuario_nuevo***/reads\n\n# Por ejemplo: ../mizarra/01_RFlores/reads\n\nContig de secuencia de referencia:\n\nls ../mizarra/***usuario_nuevo***/ref\n\n\n2.3 Copiar directorios (folders)\n\nPara copiar directorios (folders) a una nueva ubicación en Linux, puedes usar el comando cp (copy)\nVerificar que el nombre usuario_nuevo tenga el nombre de carpeta creada por ustedes en el OSC\nCrear carpeta llamada Phytophthora en su carpeta nueva\n\nmkdir Phytophthora\n\n2.3.1 Comando 1: Reads\ncp -rv ../mizarra/***usuario_nuevo***/reads Phytophthora/reads\n\n\n2.3.2 Comando 2: Referencia\ncp -rv ../mizarra/***usuario_nuevo***/ref Phytophthora/ref"
  },
  {
    "objectID": "04_amplicon.html#softwares-utilizados",
    "href": "04_amplicon.html#softwares-utilizados",
    "title": "Amplicon sequencing for Phytophthora infestans populations",
    "section": "3 Softwares utilizados",
    "text": "3 Softwares utilizados\n\nbwa: Mapeo de secuencias de ADN contra el genoma de referencia.\nsamtools: Para manipular alineaciones en formato SAM.\nbcftools: Para llamar variantes y manipular VCF y BCF."
  },
  {
    "objectID": "04_amplicon.html#script-a-ser-corrido",
    "href": "04_amplicon.html#script-a-ser-corrido",
    "title": "Amplicon sequencing for Phytophthora infestans populations",
    "section": "4 Script a ser corrido",
    "text": "4 Script a ser corrido\n\n4.1 Cargar los softwares\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/cabana\n\n\n4.2 Ubicación en la carpeta de la referencia\ncd Phytophthora/ref\n\n4.2.1 BWA index para crear un índice para un genoma de referencia\nbwa index Pinf2021refs.fa\n\n\n4.2.2 Hacer un índice de archivo fasta del genoma de referencia\nsamtools faidx Pinf2021refs.fa\n\n\n\n4.3 Ubicación en la carpeta reads\ncd ../reads\n\n4.3.1 Alineamiento a la referencia\nfor file in *.fq.gz; do\n    bwa mem ../ref/Pinf2021refs.fa \"$file\" &gt; \"${file/fq.gz/sam}\"\ndone\n\n\n4.3.2 Ordenar los alineamientos\nfor file in *.sam; do\n    samtools sort -o \"${file/sam/sorted.sam}\" \"$file\"\ndone\n\n\n4.3.3 Convertir archivos SAM a BAM\nfor file in *sorted.sam; do\n    samtools view -bt ../ref/Pinf2021refs.fa.fai -o \"${file/sorted.sam/bam}\" \"$file\"\ndone\n\n\n4.3.4 Remover archivos SAM\nrm *.sam\n\n\n4.3.5 Copiar los archivos bam (n=25) a una carpeta en común\ncp *.bam /fs/scratch/PAS2250/cabana/Phytophthora/reads\n\n\n4.3.6 Crear un listado de los archivos bam de todos los alumnos (n=25). Esperar que esten los 25 archivos bam.\n\nUbicarnos en la carpeta que tiene todos los archivos bam\n\nls /fs/scratch/PAS2250/cabana/Phytophthora/reads/*.bam &gt; listbam\n\n\n4.3.7 Llamado de Variantes SNPs\n\nUbicarnos en la carpeta que contiene el archivolistbamgenerado\n\nbcftools mpileup -Ou -f ../ref/Pinf2021refs.fa -b listbam |\n    bcftools call -mv -o perupop.vcf\n\n\n4.3.8 Filtrar posiciones de SNP\nbcftools filter -Ov -o perupop_filtered.vcf -s LOWQUAL \\\n  -e 'QUAL&lt;10 || DP &lt;10'  --SnpGap 5 --set-GTs \\\n  . perupop.vcf\n\n\n4.3.9 Mantener sólo los SNP con calidad chequeada\nbcftools view -f PASS perupop_filtered.vcf &gt; perupop_filtered_final.vcf"
  },
  {
    "objectID": "04_amplicon.html#rstudio",
    "href": "04_amplicon.html#rstudio",
    "title": "Amplicon sequencing for Phytophthora infestans populations",
    "section": "5 Rstudio",
    "text": "5 Rstudio\n\n5.1 Crear una carpeta llamada Rstudio\nmkdir ../Rstudio\n\n\n5.2 Copiar el archivo *.vcf\ncp perupop_filtered_final.vcf ../Rstudio/\n\ncp /fs/scratch/PAS2250/cabana/Phytophthora/new_names_peru.csv ../../Rstudio/\n\n\n5.3 Cargar Rstudio en el OSC OnDemand\nIngresar SupercomputerCenter\n\n5.3.1 Seleccionar Rstudio Server\n\n\n5.3.2 Iniciar Rstudio : cluster pitzer 5.3.3 “click en Launch” -setear número de horas\n\n\n5.3.3 Seleccionar el directorio de trabajo: /fs/scratch/PAS2250/cabana/$USER/Rstudio\n\n\n\n\n\n\n\n\n5.4 Cargar las librerias\n\n# Cargar modulos - &lt;https://www.osc.edu/resources/available_software/software_list/r#import_modules&gt;\nsource(file.path(Sys.getenv(\"LMOD_PKG\"), \"init/R\"))\nmodule(\"load\", \"gdal/3.3.1\")\nmodule(\"load\", \"proj/8.1.0\")\nmodule(\"load\", \"geos/3.9.1\")\nmodule(\"load\", \"sqlite/3.36.0\")\n\n# Cargar links de libreria dinamica\ndyn.load(\"/apps/proj/8.1.0/lib/libproj.so.22\", local=FALSE)\ndyn.load(\"/apps/gdal/3.3.1/lib/libgdal.so.29\", local=FALSE)\ndyn.load(\"/apps/geos/3.9.1/lib/libgeos_c.so\", local=FALSE)\n\n# Definir y configurar el directorio de la libreria\nlibdir &lt;- \"/fs/ess/PAS0471/jelmer/R/peru\"\n.libPaths(libdir)\n\nlibrary(vcfR)\nlibrary(poppr)\nlibrary(ape)\nlibrary(adegenet)\nlibrary(RColorBrewer)\nlibrary(viridis)\nlibrary(ggplot2)\nlibrary(ggtree) \nlibrary(dartR)\nlibrary(directlabels)\nlibrary(viridisLite)\n\n\n\nAnalysis\n\n# Replace usuario_nuevo with your username!\nsetwd(\"/fs/scratch/PAS2250/cabana/usuario_nuevo\")\n\n\n# Cargar el archivo vcf generado con bcftools\nperupop.VCF &lt;- read.vcfR(\"perupop_filtered_final.vcf\")\n\n\n# Convertir el conjunto de datos en un objeto genlight\ngl.perupop &lt;- vcfR2genlight(perupop.VCF)\n\n\nploidy(gl.perupop) &lt;- 2\n\n\n# Renombrar las muestras\ngl.perupop_recoded &lt;- gl.recode.ind(gl.perupop,\n                                    ind.recode=\"new_names_peru.csv\", \n                                    mono.rm = T, verbose=0)\n\ngl.perupop_recoded\n\n\n#Asignar el linaje clonal de las muestras en el orden delos genotipos reportados\n\npop(gl.perupop_recoded) &lt;- as.factor(c(\"US-1\",\n\"US-1\",\n\"PE-3\",\n\"PE-3\",\n\"PE-3\",\n\"EC-1\",\n\"EC-1\",\n\"EC-1\",\n\"EC-1\",\n\"PE-7\",\n\"PE-7\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"EC-1\",\n\"EC-1\",\n\"EC-1\",\n\"EC-1\",\n\"EC-1\",\n\"EC-1\",\n\"EC-1\",\n\"PE-3\",\n\"PE-7\",\n\"PE-7\",\n\"PE-7\",\n\"PE-7\",\n\"PE-7\",\n\"PE-7\",\n\"PE-3\",\n\"PE-3\",\n\"PE-3\",\n\"PE-3\",\n\"PE-3\",\n\"PE-3\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\"))\n\n\n\n5.5 Dendograma basado en el algoritmo UPGMA\n\n#La función aboot se utiliza para evaluar la precisión del árbol de\n#decisión ajustado mediante la validación cruzada bootstrap.\n\ntree &lt;- aboot(gl.perupop_recoded, tree = \"upgma\", distance = bitwise.dist, \n              sample = 100, showtree = F, cutoff = 50, quiet = T)\n\ncols &lt;- turbo(n = nPop(gl.perupop_recoded))\nplot.phylo(tree, cex = 0.8, font = 2, adj = 0, tip.color =  cols[pop(gl.perupop_recoded)])\nnodelabels(tree$node.label, adj = c(1.3, -0.5), frame = \"n\", cex = 0.8,font = 3, xpd = TRUE)\nlegend('topleft', legend = c(\"EC-1\",\"PE-3\", \"PE-7\", \"PE-7-Puno\", \"US-1\"), fill = cols, \n       border = FALSE, bty = \"n\", cex = 0.55)\naxis(side = 1)\ntitle(xlab = \"Genetic distance (proportion of loci that are different)\")\n\ndev.off()\n\n\n# Guardar el Gráfico\npng(\"Tree_alldata.png\", res = 300, width = 9.57, height = 9.34, units = 'in')\nplot.phylo(tree, cex = 0.8, font = 2, adj = 0, tip.color =  cols[pop(gl.perupop_recoded)])\nnodelabels(tree$node.label, adj = c(1.3, -0.5), frame = \"n\", cex = 0.8,font = 3, xpd = TRUE)\nlegend('topleft', legend = c(\"EC-1\",\"PE-3\", \"PE-7\", \"PE-7-Puno\", \"US-1\"), fill = cols, \n       border = FALSE, bty = \"n\", cex = 0.55)\naxis(side = 1)\ntitle(xlab = \"Genetic distance (proportion of loci that are different)\")\n\ndev.off()\n\n\n# Reporte de la calidad de la data\ngl.report.callrate(gl.perupop_recoded, method =\"loc\")\n\ngl.report.callrate(gl.perupop_recoded, method =\"ind\")\n\n\n# Filtrar la data 0.8189447\ngl.perupop2 &lt;- gl.filter.callrate(gl.perupop_recoded,\n                                  method=\"ind\", \n                                  t=0.8189447,\n                                  recalc = T)\n\n\nnames(gl.perupop2) \ngl.perupop2$ind.names\ngl.perupop2$pop\n\n\npop(gl.perupop2) &lt;- as.factor(c(\"US-1\",\n\"US-1\",\n\"PE-3\",\n\"EC-1\",\n\"EC-1\",\n\"PE-7\",\n\"PE-7\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"EC-1\",\n\"EC-1\",\n\"PE-3\",\n\"PE-7\",\n\"PE-7\",\n\"PE-7\",\n\"PE-7\",\n\"PE-7\",\n\"PE-7\",\n\"PE-3\",\n\"PE-3\",\n\"PE-3\",\n\"PE-3\",\n\"PE-3\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"US-1\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\",\n\"PE-7-Puno\"\n))\n\n\n# Árbol de distancia basado en el algoritmo UPGMA, con 100 replicados de bootstrap\ntree2 &lt;- aboot(gl.perupop2, tree = \"upgma\", distance = bitwise.dist, \n              sample = 100, showtree = F, cutoff = 50, quiet = T)\n\ncols &lt;- turbo(n = nPop(gl.perupop2))\nplot.phylo(tree2, cex = 0.8, font = 2, adj = 0, tip.color =  cols[pop(gl.perupop2)])\nnodelabels(tree2$node.label, adj = c(1.3, -0.5), frame = \"n\", cex = 0.8,font = 3, xpd = TRUE)\nlegend('topleft', legend = c(\"EC-1\",\"PE-3\", \"PE-7\", \"PE-7-Puno\", \"US-1\"), fill = cols, \n       border = FALSE, bty = \"n\", cex = 0.65)\naxis(side = 1)\ntitle(xlab = \"Genetic distance (proportion of loci that are different)\")\n\ndev.off()\n\n\n# Guardar el Gráfico\npng(\"Tree_filter0.8189447.png\", res = 300, width = 9.57, height = 9.34, units = 'in')\nplot.phylo(tree2, cex = 0.8, font = 2, adj = 0, tip.color =  cols[pop(gl.perupop2)])\nnodelabels(tree2$node.label, adj = c(1.3, -0.5), frame = \"n\", cex = 0.8,font = 3, xpd = TRUE)\nlegend('topleft', legend = c(\"EC-1\",\"PE-3\", \"PE-7\", \"PE-7-Puno\", \"US-1\"), fill = cols, \n       border = FALSE, bty = \"n\", cex = 0.65)\naxis(side = 1)\ntitle(xlab = \"Genetic distance (proportion of loci that are different)\")\n\ndev.off()\n\n\n\n5.6 Analisis de Coordenadas Principales (PCoA) usando la matriz de distancias\n\n# \nsubset_phytoph &lt;- gl.recalc.metrics(gl.perupop2)\nsubset_phytoph$other$loc.metrics &lt;- as.data.frame(subset_phytoph$other$loc.metrics)\n\n\npcoa &lt;- gl.pcoa(subset_phytoph, nfactors = 5)\n\n\ngl.pcoa.plot(pcoa, gl.perupop2)"
  },
  {
    "objectID": "ref_shell.html#basic-shell-commands",
    "href": "ref_shell.html#basic-shell-commands",
    "title": "Unix shell reference/overview",
    "section": "1 Basic shell commands",
    "text": "1 Basic shell commands\n\n\n\nCommand\nDescription\nExamples / options\n\n\n\n\npwd\nPrint current working directory (dir).\npwd\n\n\nls\nList files in working dir (default) or elsewhere.\nls data/      -l long format      -h human-readable file sizes      -a show hidden files\n\n\ncd\nChange working dir. As with all commands, you can use an absolute path (starting from the root dir /) or a relative path (starting from the current working dir).\ncd /fs/ess/PAS1855 (With absolute path)  cd ../.. (Two levels up)  cd - (To previous dir)\n\n\ncp\nCopy files or, with -r, dirs and their contents (i.e., recursively).  If target is a dir, file will keep same name; otherwise, a new name can be provided.\ncp *.fq data/ (All .fq files into dir data)  cp my.fq data/new.fq (With new name)  cp -r data/ ~ (Copy dir and contents to home dir) \n\n\nmv\nMove/rename files or dirs (-r not needed).  If target is a dir, file will keep same name; otherwise a new name can be provided.\nmv my.fq data/ (Keep same name)  mv my.fq my.fastq (Simple rename)  mv file1 file2 mydir/ (Last arg is destination)\n\n\nrm\nRemove files or dirs/recursively (with -r).  With -f (force), any write-protections that you have set will be overridden.\nrm *fq (Remove all matching files)  rm -r mydir/ (Remove dir & contents)      -i Prompt for confirmation      -f Force remove\n\n\nmkdir\nCreate a new dir.  Use -p to create multiple levels at once and to avoid an error if the dir exists.\nmkdir my_new_dir  mkdir -p new1/new2/new3\n\n\ntouch\nIf file does not exist: create empty file.  If file exists: change last-modified date.\ntouch newfile.txt\n\n\ncat\nPrint file contents to standard out (screen).\ncat my.txt  cat *.fa &gt; concat.fq (Concatenate files)\n\n\nhead\nPrint the first 10 lines of a file or specify number with -n &lt;n&gt; or shorthand -&lt;n&gt;.\nhead -n 40 my.fq (print 40 lines)  head -40 my.fq (equivalent)\n\n\ntail\nLike head but print the last lines.\ntail -n +2 my.csv (“trick” to skip first line)  tail -f slurm.out (“follow” file)\n\n\nless\nView a file in a file pager; type q to exit. See below for more details.\nless myfile      -S disable line-wrapping\n\n\ncolumn -t\nView a tabular file with columns nicely lined up in the shell.\nNice viewing of a CSV file:  column -s \",\" -t my.csv\n\n\nhistory\nPrint previously issued commands.\nhistory | grep \"cut\" (Find previous cut usage)\n\n\nchmod\nChange file permissions for file owner (user, u), “group” (g), others (o) or everyone (all; a). Permissions can be set for reading (r), writing (w), and executing (x).  ddddddddddddddddddddddddddddddddddddd\nchmod u+x script.sh (Make script executable)  chmod a=r data/raw/* (Make data read-only)      -R recursive  ddddddddddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref_shell.html#data-tools",
    "href": "ref_shell.html#data-tools",
    "title": "Unix shell reference/overview",
    "section": "2 Data tools",
    "text": "2 Data tools\n\n\n\n\n\n\n\n\nCommand\nDescription\nExamples and options\n\n\n\n\nwc -l\nCount the number of lines in a file.\nwc -l my.fq\n\n\ncut\nSelect one or more columns from a file.\nSelect columns 1-4:  cut -f 1-4 my.csv      -d \",\" comma as delimiter\n\n\nsort\nSort lines.\nThe -V option will successfully sort chr10 after chr2. etc.\nSort column 1 alphabetically,  column 2 reverse numerically:  sort -k1,1 -k2,2nr my.bed      -k 1,1 by column 1 only      -n numerical sorting      -r reverse order      -V recognize number with string\n\n\nuniq\nRemove consecutive duplicate lines (often from single-column selection): i.e., removes all duplicates if input is sorted.\nUnique values for column 2:  cut -f2 my.tsv | sort | uniq\n\n\nuniq -c\nIf input is sorted, create a count table for occurrences of each line (often from single-column selection).\nCount table for column 3:  cut -f3 my.tsv | sort | uniq -c\n\n\ntr\nSubstitute (translate) characters or character classes (like A-Z for uppercase letters). Does not take files as argument; piping or redirection needed.\nTo “squeeze” (-s) is to remove consecutive duplicates (akin to uniq).\nTSV to CSV:  cat my.csv | tr \"\\t\" \",\"  Uppercase to lowercase: tr A-Z a-z &lt; in.txt &gt; out.txt      -d delete      -s squeeze\n\n\ngrep\nSearch files for a pattern and print matching lines (or only the matching string with -o).\nDefault regex is basic (GNU BRE): use -E for extended regex (GNU ERE) and -P for Perl-like regex.\nTo print lines surrounding a match, use -A n (n lines after match) or -B n (n lines before match) or -C n (n lines before and after match).\nddddddddddddddddddddddddddddddddddddddd\nMatch AAC or AGC:  grep \"A[AG]C\" my.fa  Omit comment lines:  grep -v \"^# my.gff      -c count      -i ignore case      -r recursive      -v invert      -o print match only"
  },
  {
    "objectID": "ref_shell.html#miscellaneae",
    "href": "ref_shell.html#miscellaneae",
    "title": "Unix shell reference/overview",
    "section": "3 Miscellaneae",
    "text": "3 Miscellaneae\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nexample\n\n\n\n\n/\nRoot directory.\ncd /\n\n\n.\nCurrent working directory.\ncp data/file.txt . (Copy to working dir)  Use ./ to execute script if not in $PATH:  ./myscript.sh\n\n\n..\nOne directory level up.\ncd ../.. (Move 2 levels up)\n\n\n~ or $HOME\nHome directory.\ncp myfile.txt ~ (Copy to home)\n\n\n$USER\nUser name.\nmkdir $USER\n\n\n&gt;\nRedirect standard out to a file.\necho \"My 1st line\" &gt; myfile.txt\n\n\n&gt;&gt;\nAppend standard out to a file.\necho \"My 2nd line\" &gt;&gt; myfile.txt\n\n\n2&gt;\nRedirect standard error to a file.\nSend standard out and standard error for a script to separate files:  myscript.sh &gt;log.txt 2&gt; err.txt\n\n\n&&gt;\nRedirect standard out and standard error to a file.\nmyscript.sh &&gt; log.txt\n\n\n|\nPipe standard out (output) of one command into standard in (input) of a second command\nThe output of the sort command will be piped into head to show the first lines:  sort myfile.txt | head\n\n\n{}\nBrace expansion. Use .. to indicate numeric or character ranges (1..4 =&gt; 1, 2, 3, 4) and , to separate items.\nmkdir Jan{01..31} (Jan01, Jan02, …, Jan31)  touch fig1{A..F} (fig1A, fig1B, …, fig1F)  mkdir fig1{A,D,H} (fig1A, fig1D, fig1D)\n\n\n$()\nCommand substitution. Allows for flexible usage of the output of any command: e.g., use command output in an echo statement or assign it to a variable.\nReport number of FASTQ files:  echo \"I see $(ls *fastq | wc -l) files\"  Substitute with date in YYYY-MM-DD format:  mkdir results_$(date +%F)  nlines=$(wc -l &lt; $infile)\n\n\n$PATH\nContains colon-separated list of directories with executables: these will be searched when trying to execute a program by name.  ddddddddddddddddddddddddddddddddddddd\nAdd dir to path:  PATH=$PATH:/new/dir  (But for lasting changes, edit the Bash configuration file ~./bashrc.) dddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref_shell.html#shell-wildcards",
    "href": "ref_shell.html#shell-wildcards",
    "title": "Unix shell reference/overview",
    "section": "4 Shell wildcards",
    "text": "4 Shell wildcards\n\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n\n*\nAny number of any character, including nothing.\nls data/*fastq.gz (Matches any file ending in “fastq.gz”)  ls *R1* (Matches any file containing “R1” somewhere in the name.)\n\n\n?\nAny single character.\nls sample1_?.fastq.gz (Matches sample1_A.fastq.gz but not sample1_AA.fastq.gz)\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets.  ddddddddddddddddddddddddddddddddddddd\nls fig1[A-C] (Matches fig1A, fig1B, fig1C)  ls fig[0-3] (Matches fig0, fig1, fig2, fig3)  ls fig[^4]* (Does not match files with a “4” after “fig”)  ddddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref_shell.html#regular-expressions",
    "href": "ref_shell.html#regular-expressions",
    "title": "Unix shell reference/overview",
    "section": "5 Regular expressions",
    "text": "5 Regular expressions\nNote: ERE = GNU “Extended Regular Expressions”. If “yes” in ERE column, then the symbol needs ERE to work1: use a -E flag for grep and sed (note that awk uses ERE by default) to turn on ERE.\n\n\n\n\n\n\n\n\n\nSymbol\nERE\nMatches\nExample\n\n\n\n\n.\n\nAny single character\nMatch Olfr with none or any characters after it:  grep -o \"Olfr.*\"\n\n\n*\n\nQuantifier: matches preceding character any number of times\nSee previous example.\n\n\n+\nyes\nQuantifier: matches preceding character at least once\nAt least two consecutive digits:  grep -E [0-9]+\n\n\n?\nyes\nQuantifier: matches preceding character at most once\nOnly a single digit:  grep -E [0-9]?\n\n\n{m} / {m,} / {m,n}\nyes\nQuantifier: match preceding character m times / at least m times / m to n times\nBetween 50 and 100 consecutive Gs:  grep -E \"G{50,100}\"\n\n\n^ / $\n\nAnchors: match beginning / end of line\nExclude empty lines:  grep -v \"^$\"  Exclude lines beginning with a “#”:  grep -v \"^#\"\n\n\n\\t\n\nTab (To match in grep, needs -P flag for Perl-like regex)\necho -e \"column1 \\t column2\"\n\n\n\\n\n\nNewline (Not straightforward to match since Unix tools are line-based.)\necho -e \"Line1 \\n Line2\"\n\n\n\\w\n(yes)\n“Word” character: any alphanumeric character or “_”. Needs -E (ERE) in grep but not in sed.\nMatch gene_id followed by a space and a “word”:  grep -E -o 'gene_id \"\\w+\"'  Change any word character to X:  sed s/\\w/X/\n\n\n|\nyes\nAlternation / logical or: match either the string before or after the |\nFind lines with either intron or exon:  grep -E \"intron|exon\"\n\n\n()\nyes\nGrouping\nFind “AAG” repeated 10 times:  grep (AAG){10}\n\n\n\\1, \\2, etc.\nyes\nBackreferences to groups captured with (): first group is \\1, second group is \\2, etc.  ddddddddddddddddddddddddddddddddddddd\nInvert order of two words:  sed -E 's/(\\w+) (\\w+)/\\2 \\1/'  ddddddddddddddddddddddddddddddddddddd"
  },
  {
    "objectID": "ref_shell.html#more-details-for-a-few-commands",
    "href": "ref_shell.html#more-details-for-a-few-commands",
    "title": "Unix shell reference/overview",
    "section": "6 More details for a few commands",
    "text": "6 More details for a few commands\n\n6.1 less\n\n\n\n\n\n\n\nKey\nFunction\n\n\n\n\nq\nExit less\n\n\nspace / b\nGo down / up a page. (pgup / pgdn usually also work.)\n\n\nd / u\nGo down / up half a page.\n\n\ng / G\nGo to the first / last line (home / end also work).\n\n\n/&lt;pattern&gt; or ?&lt;pattern&gt;\nSearch for &lt;pattern&gt; forwards / backwards: type your search after / or ?.\n\n\nn / N\nWhen searching, go to next / previous search match.\ndddddddddddddddddddddddddddddddddddddddddddddddddddd\n\n\n\n\n\n6.2 sed\n\n\n6.3 sed flags:\n\n\n\nFlag\nMeaning\n\n\n\n\n-E\nUse extended regular expressions\n\n\n-e\nWhen using multiple expressions, precede each with -e\n\n\n-i\nEdit a file in place\n\n\n-n\nDon’t print lines unless specified with p modifier\n\n\n\n\nsed examples\n# Replace \"chrom\" by \"chr\" in every line,\n# with \"i\": case insensitive, and \"g\": global (&gt;1 replacements per line)\nsed 's/chrom/chr/ig' chroms.txt\n\n# Only print lines matching \"abc\":\nsed -n '/abc/p' my.txt\n\n# Print lines 20-50:\nsed -n '20,50p'\n\n# Change the genomic coordinates format chr1:431-874 (\"chrom:start-end\")\n# ...to one that has a tab (\"\\t\") between each field:\necho \"chr1:431-874\" | sed -e 's/:/\\t/' -e 's/-/\\t/'\n#&gt; chr1    431     874\n\n# Invert the order of two words:\necho \"inverted words\" | sed -E 's/(\\w+) (\\w+)/\\2 \\1/'\n#&gt; words inverted\n\n# Capture transcript IDs from a GTF file (format 'transcript_id \"ID_I_WANT\"'):\n# (Needs \"-n\" and \"p\" so lines with no transcript_id are not printed.) \ngrep -v \"^#\" my.gtf | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p'\n\n# When a pattern contains a `/`, use a different expression delimiter:\necho \"data/fastq/sampleA.fastq\" | sed 's#data/fastq/##'\n#&gt; sampleA.fastq\n\n\n\n6.4 awk\n\nRecords and fields: by default, each line is a record (assigned to $0). Each column is a field (assigned to $1, $2, etc).\nPatterns and actions: A pattern is a condition to be tested, and an action is something to do when the pattern evaluates to true.\n\nOmit the pattern: action applies to every record.\nawk '{ print $0 }' my.txt     # Print entire file\nawk '{ print $3,$2 }' my.txt  # Print columns 3 and 2 for each line\nOmit the action: print full records that match the pattern.\n# Print all lines for which:\nawk '$3 &lt; 10' my.bed          # Column 3 is less than 10\nawk '$1 == \"chr1\"' my.bed     # Column 1 is \"chr1\"\nawk '/chr1/' my.bed           # Regex pattern \"chr1\" matches\nawk '$1 ~ /chr1/' my.bed      # Column 1 _matches_ \"chr1\"\n\n\n\nawk examples\n# Count columns in a GTF file after excluding the header\n# (lines starting with \"#\"):\nawk -F \"\\t\" '!/^#/ {print NF; exit}' my.gtf\n\n# Print all lines for which column 1 matches \"chr1\" and the difference\n# ...between columns 3 and 2 (feature length) is less than 10:\nawk '$1 ~ /chr1/ && $3 - $2 &gt; 10' my.bed\n\n# Select lines with \"chr2\" or \"chr3\", print all columns and add a column \n# ...with the difference between column 3 and 2 (feature length):\nawk '$1 ~ /chr2|chr3/ { print $0 \"\\t\" $3 - $2 }' my.bed\n\n# Caclulate the mean value for a column:\nawk 'BEGIN{ sum = 0 };            \n     { sum += ($3 - $2) };             \n     END{ print \"mean: \" sum/NR };' my.bed\n\n\nawk comparison and logical operators\n\n\n\n\n\n\n\nComparison\nDescription\n\n\n\n\na == b\na is equal to b\n\n\na != b\na is not equal to b\n\n\na &lt; b\na is less than b\n\n\na &gt; b\na is greater than b\n\n\na &lt;= b\na is less than or equal to b\n\n\na &gt;= b\na is greater than or equal to b\n\n\na ~ /b/\na matches regular expression pattern b\n\n\na !~ /b/\na does not match regular expression pattern b\n\n\na && b\nlogical and: a and b\n\n\na || b\nlogical or: a or b [note typo in Buffalo]\n\n\n!a\nnot a (logical negation)\n\n\n\n\n\nawk special variables and keywords\n\n\n\n\n\n\n\nkeyword/variable\nmeaning\n\n\n\n\nBEGIN\nUsed as a pattern that matches the start of the file\n\n\nEND\nUsed as a pattern that matches the end of the file\n\n\nNR\nNumber of Records (running count; in END: total nr. of lines)\n\n\nNF\nNumber of Fields (for each record)\n\n\n$0\nContains entire record (usually a line)\n\n\n$1 - $n\nContains one column each\n\n\nFS\nInput Field Separator (default: any whitespace)\n\n\nOFS\nOutput Field Separator (default: single space)\n\n\nRS\nInput Record Separator (default: newline)\n\n\nORS\nOutput Record Separator (default: newline)\n\n\n\n\n\nawk functions\n\n\n\n\n\n\n\nFunction\nMeaning\n\n\n\n\nlength(&lt;string&gt;)\nReturn number of characters\n\n\ntolower(&lt;string&gt;)\nConvert to lowercase\n\n\ntoupper(&lt;string&gt;)\nConvert to uppercase\n\n\nsubstr(&lt;string&gt;, &lt;start&gt;, &lt;end&gt;)\nReturn substring\n\n\nsplit(&lt;string&gt;, &lt;array&gt;, &lt;delimiter&gt;)\nSplit into chunks in an array\n\n\nsub(&lt;from&gt;, &lt;to&gt;, &lt;string&gt;)\nSubstitute (replace) regex\n\n\ngsub(&lt;from&gt;, &lt;to&gt; &lt;string&gt;)\n&gt;1 substitution per line\n\n\nprint\nPrint, e.g. column: print $1\n\n\nexit\nBreak out of record-processing loop;  e.g. to stop when match is found\n\n\nnext\nDon’t process later fields: to next iteration"
  },
  {
    "objectID": "ref_shell.html#keyboard-shortcuts",
    "href": "ref_shell.html#keyboard-shortcuts",
    "title": "Unix shell reference/overview",
    "section": "7 Keyboard shortcuts",
    "text": "7 Keyboard shortcuts\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl+Shift+C\nCopy selected text\n\n\nCtrl+Shift+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U / Ctrl+K\nCut from cursor to beginning / end of line2\n\n\nCtrl+W\nCut word before before cursor3\n\n\nCtrl+Y\nPaste (“yank”)\n\n\nAlt+.\nLast argument of previous command (very useful!)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nKill (stop) currently active command\n\n\nCtrl+D\nExit (a program or the shell depending on the context)\n\n\nCtrl+Z\nSuspend (pause) a process: then use bg to move to background."
  },
  {
    "objectID": "ref_shell.html#footnotes",
    "href": "ref_shell.html#footnotes",
    "title": "Unix shell reference/overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen using the default regular expressions in grep and sed, Basic Regular Expressions (BRE), the symbol would need to be preceded by a backslash to work.↩︎\nCtrl+K doesn’t work by default in VS Code, but can be set there.↩︎\nDoesn’t work by default in VS Code, but can be set there.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this website",
    "section": "",
    "text": "This website contains the teaching materials of Jelmer Poelstra (MCIC Wooster, Ohio State University) and Myriam Izarra (CGIAR) in the CABANAnet/STC-CGIAR workshop Latin American workshop on high throughput sequencing for plant pathogen identification and analysis.\nThis workshop was held from Feb 5th to Feb 9th, 2024, in Chachapoyas, Peru.\nThe source code for this website can be found at https://github.com/jelmerp/cabana-workshop.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "06_assembly.html#introduction",
    "href": "06_assembly.html#introduction",
    "title": "Genome assembly and assembly QC",
    "section": "Introduction",
    "text": "Introduction\nWe will now create a genome assembly from the preprocessed reads with the program SPAdes.\nAfter the assembly, we will run several assembly QC and filtering steps to:\n\nGet some basic assembly summary stats with BBtools\nRun Busco to check for genome completeness\nIdentify and remove contaminant contigs with Kraken2\n\n\n Setting up\nYou should have an active VS Code session with an open terminal. In that terminal, you should be be in your dir /fs/scratch/PAS2250/cabana/$USER/bact/bact."
  },
  {
    "objectID": "06_assembly.html#the-fasta-format",
    "href": "06_assembly.html#the-fasta-format",
    "title": "Genome assembly and assembly QC",
    "section": "1 The FASTA format",
    "text": "1 The FASTA format\nThe genome assembly that we will create will be in the FASTA formet. FASTA files contain one or more DNA or amino acid sequences, with no limits on the number of sequences or the sequence lengths.\nThe following example FASTA file contains two entries:\n&gt;unique_sequence_ID Optional description\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAAAA\n&gt;unique_sequence_ID2\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAATG\nEach entry consists of a header line and the sequence itself. Header lines start with a &gt; (greater-than sign) and are otherwise “free form”, though the idea is that they provide an identifier for the sequence that follows.1\n\n\n\n\n\n\nFASTA file name extensions are variable: .fa, .fasta, .fna, .faa (Click to expand)\n\n\n\n\n\n\n“Generic” extensions are .fasta and .fa (e.g: my_assembly.fasta)\nAlso used are extensions that explicitly indicate whether sequences are nucleotides (.fna) or amino acids (.faa)"
  },
  {
    "objectID": "06_assembly.html#batch-jobs-with-sbatch",
    "href": "06_assembly.html#batch-jobs-with-sbatch",
    "title": "Genome assembly and assembly QC",
    "section": "2 “Batch jobs” with sbatch",
    "text": "2 “Batch jobs” with sbatch\nSo far, we having been running programs by directly typing/pasting the commands in the terminal. Because SPAdes needs more computing resources, we will run it differently.\nWe will submit it as a so-called “batch” (non-interactive) job using the sbatch command. This job will then be run on a compute node that we ourselves never move to (!).\nTo first see a simple example, say that we just wanted to run echo Hello there $USER as a batch job, where we’ll use these options:\n\n-A for the OSC Project we want to use\n-t for the time in minutes that we need\n-c for the number of cores that we need\n-o for the name of the log file (where “Hello there ” will be printed)\nThe command that we want to run in the batch job is wrapped in wrap=\"&lt;command&gt;\"\n\nsbatch -A PAS2250 -t 1 -c 1 -o slurm-hello.out \\\n  --wrap=\"echo Hello there $USER\"\nSubmitted batch job 25928455\n# [You will get a different number, each job has a unique ID]\nNow, the output of our command (ls -lh) is not printed to the screen, but will end up in a file in our working directory, whose name starts with slurm and contains the job ID number:\nls\ndata  README.md  results  slurm-hello.out\n\n\n\n\n\n\nThe slurm file will only show up once the job has started running, which can take up to a minute or so.\n\n\n\n\n\n\nLet’s take a look at that “Slurm log file”:\ncat slurm-hello.out\nHello there jelmer"
  },
  {
    "objectID": "06_assembly.html#assembly-with-spades",
    "href": "06_assembly.html#assembly-with-spades",
    "title": "Genome assembly and assembly QC",
    "section": "3 Assembly with SPAdes",
    "text": "3 Assembly with SPAdes\nSPAdes is a well-performing and very flexible assembler that can be used to do many kinds of assemblies, including metagenomes and transcriptomes. It has a special “mode” for bacterial isolate assembly, which can be activated with the --isolate flag.\n\n\n3.1 Our SPAdes command\nWe will run SPAdes with the following options:\n\n-1 and -2 for the R1 and R2 FASTQ files\n-o for the output dir, which should be sample-specific, and should not yet exist\n--isolate to activate the bacterial isolate mode\n-k 21,33,55,77,99,127 to assemble with a variety of kmer sizes\n--threads 20 and --memory 80 to use 20 threads and 80 GB of memory\n\n# (Don't run this yet)\nspades.py \\\n  -1 results/trimgalore/SM04_R1_val_1.fq.gz \\\n  -2 results/trimgalore/SM04_R2_val_2.fq.gz \\\n  -o results/spades/SM04 \\\n  -k 21,33,55,77,99,127 \\\n  --isolate \\\n  --threads 20 \\\n  --memory 80\n\n\n\n3.2 Running SPAdes\nFirst, we’ll have to switch back to the cabana Conda environment, since we activated a different Conda environment for TrimGalore earlier.\nsource activate /fs/ess/PAS0471/jelmer/conda/cabana\n\n\nSubmitting the SPAdes sbatch job\nNow we’re ready to submit our SPAdes job, with 30 minutes (-t 30) and 20 cores (-c 20):\nsbatch -A PAS2250 -t 30 -c 20 -o slurm-spades.out --wrap=\"\n    spades.py \\\n      -1 results/trimgalore/SM04_R1_val_1.fq.gz \\\n      -2 results/trimgalore/SM04_R2_val_2.fq.gz \\\n      -o results/spades/SM04 \\\n      -k 21,33,55,77,99,127 \\\n      --isolate \\\n      --threads 20 \\\n      --memory 80\n\"\n\n\n Exercise: Monitor the SPAdes run\n\nUse less to check the slurm-spades.out file, which will have the SPAdes “log”.\nIn less, press G (capital G!) to look at the end of the file.\n\nWhen you first check the slurm-spades.out file, SPAdes should still be running. You know it will be done when you see the following line at the end:\nThank you for using SPAdes!\n\nTo monitor the progress of the SPAdes run, you can use tail -f slurm-spades.out, which will “follow” the file and add any new text in real-time! To exit this, press Ctrl+c.\n\n\n\n\n\n\n\nSPAdes may take 5-10 minutes to complete\n\n\n\n\n\n\n\n\n\n\n3.3 SPAdes output files\nLet’s check the files in the output dir:\nls -lh results/spades/SM04\n\n\nClick to show the output\n\ntotal 47M\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 assembly_graph_after_simplification.gfa\n-rw-r--r-- 1 jelmer PAS0471  12M Feb  4 21:49 assembly_graph.fastg\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 assembly_graph_with_scaffolds.gfa\n-rw-r--r-- 1 jelmer PAS0471 5.9M Feb  4 21:49 before_rr.fasta\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 contigs.fasta\n-rw-r--r-- 1 jelmer PAS0471 9.8K Feb  4 21:49 contigs.paths\n-rw-r--r-- 1 jelmer PAS0471   79 Feb  4 21:42 dataset.info\n-rw-r--r-- 1 jelmer PAS0471  278 Feb  4 21:42 input_dataset.yaml\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:49 K127\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:43 K21\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:44 K33\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:45 K55\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:47 K77\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:48 K99\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Feb  4 21:49 logs\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Feb  4 21:49 misc\n-rw-r--r-- 1 jelmer PAS0471 1.5K Feb  4 21:42 params.txt\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Feb  4 21:49 pipeline_state\n-rw-r--r-- 1 jelmer PAS0471 3.4K Feb  4 21:42 run_spades.sh\n-rw-r--r-- 1 jelmer PAS0471 4.9K Feb  4 21:42 run_spades.yaml\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 scaffolds.fasta\n-rw-r--r-- 1 jelmer PAS0471 8.9K Feb  4 21:49 scaffolds.paths\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 SM04\n-rw-r--r-- 1 jelmer PAS0471 194K Feb  4 21:49 spades.log\n\nThere are quite some files, as well as subdirs for different k-mer sizes, but we’re really only interested in the assembly FASTA file, which is contigs.fasta. Let’s take a look at that file:\nless results/spades/SM04/contigs.fasta\n&gt;NODE_1_length_1267796_cov_33.239498\nACCTTGAGTTCCCTAAAGGGCCGTCGAAGACTACGACGTTGATAGGTTGGGTGTGTAAGC\nGCTGTGAGGCGTTGAGCTAACCAATACTAATTGCCCGTGAGGCTTGACCATATAACACCC\nAAGCAATTTGCGTTGAATGAGCAGATTGCGGTGACTGTGAAGATGACACGAACCGAAAGT\nTTGCGTCACGAACGACACCTGAACCAGCTTGCTATCACATACCCGATTTGCTGAAGCGCG\nCCGCAAGGCACGATTCGGTACCCGAATTTCTTGACGACCATAGAGCATTGGAACCACCTG\nATCCCATCCCGAACTCAGTAGTGAAACGATGTATCGCCGATGGTAGTGTGGGGTTTCCCC\nATGTGAGAGTAGGTCATCGTCAAGATTAAATTCCAGAAACCCTCATCGCTTACGCGTTGA\nGGGTTTTTGTTTGTCTGGGGTTCCAGAAACCTCTGCATTCTCTATCTGGCTCATCTCATT\nGCAATGCAGCCGCATTGGCGCCAGAGACCCCCAAGGTTTAGTGAAACGCCCCCATCCCTG\nIn this file, each contig is one FASTA entry. The contig headers have some metadata, such as its length in base pairs, and its depth of coverage (cov_; i.e. how many reads, on average, cover each base).\nWe can see a few more headers by using the grep command, which will print lines matching a search pattern (in our case, the &gt; from the header), as follows:\n# (Do NOT omit the quotes around the \"&gt;\"!)\ngrep \"&gt;\" results/spades/SM04/contigs.fasta | head\n&gt;NODE_1_length_1267796_cov_33.239498\n&gt;NODE_2_length_902255_cov_32.000245\n&gt;NODE_3_length_697265_cov_34.901625\n&gt;NODE_4_length_534491_cov_32.088021\n&gt;NODE_5_length_350317_cov_33.463137\n&gt;NODE_6_length_339735_cov_31.812540\n&gt;NODE_7_length_291220_cov_35.951730\n&gt;NODE_8_length_274792_cov_32.455031\n&gt;NODE_9_length_167931_cov_33.795917\n&gt;NODE_10_length_164349_cov_34.581646\n\n\n\n\n\n\nIf you don’t pipe (|) the grep output to head, you will see all headers\n\n\n\n\n\n\nIf we use grep’s -c option, it will count the number of matching lines, which will give us a count of the number of contigs:\ngrep -c \"&gt;\" results/spades/SM04/contigs.fasta\n86"
  },
  {
    "objectID": "06_assembly.html#basic-assembly-stats",
    "href": "06_assembly.html#basic-assembly-stats",
    "title": "Genome assembly and assembly QC",
    "section": "4 Basic assembly stats",
    "text": "4 Basic assembly stats\nWe can use the tools stats.sh from the BBTools suite of genomics tools to get some (more) basic statistics for our genome assembly:\nstats.sh results/spades/SM04/contigs.fasta \nA       C       G       T       N       IUPAC   Other   GC      GC_stdev\n0.2048  0.2969  0.2945  0.2038  0.0000  0.0000  0.0000  0.5914  0.0875\n\nMain genome scaffold total:             86\nMain genome contig total:               86\nMain genome scaffold sequence total:    5.968 MB\nMain genome contig sequence total:      5.968 MB        0.000% gap\nMain genome scaffold N/L50:             4/534.491 KB\nMain genome contig N/L50:               4/534.491 KB\nMain genome scaffold N/L90:             14/97.943 KB\nMain genome contig N/L90:               14/97.943 KB\nMax scaffold length:                    1.268 MB\nMax contig length:                      1.268 MB\nNumber of scaffolds &gt; 50 KB:            18\n% main genome in scaffolds &gt; 50 KB:     96.64%"
  },
  {
    "objectID": "06_assembly.html#assembly-completeness-check-with-busco",
    "href": "06_assembly.html#assembly-completeness-check-with-busco",
    "title": "Genome assembly and assembly QC",
    "section": "5 Assembly completeness check with Busco",
    "text": "5 Assembly completeness check with Busco\nWe will use the program Busco (Manni et al. 2021, documentation) to check how complete our genome assembly is. Busco checks for the presence of genes that are expected to be universally present in a single copy in a specific taxonomic lineage.\n\n\n\n\n\n\nOther tool options\n\n\n\nAnother commonly used program to check assembly completeness and also contamination is CheckM (Parks et al. 2015, documentation), but in the interest of time, we will only run Busco.\n\n\nBecause Busco will always output a number of files in your working directory, we will move into our desired output dir in advance:\ncd results/busco\nWe will also have to load a different Conda environment, again:\nsource activate /fs/ess/PAS0471/jelmer/conda/busco\nUsing the --lineage_dataset option, we have to tell Busco which lineage’s reference database it should use: there is a list of lineages on Busco’s website.\n\n Exercise: Busco database\nTake a look at the website linked to above. Which lineage dataset should we pick?\n\n\nClick for the solution\n\nThere is a database for the order that Pseudomonas is in: pseudomonadales.\nAlternatively, we could use the database for all bacteria: bacteria.\n\n\nOtherwise, we will use the following options:\n\n--in — input file\n--out — output ID (not the full filename)\n--mode genome — our input file is a genome assembly, not a transcriptome assembly or proteome\n\n# Run Busco\nbusco \\\n    --in ../spades/SM04/contigs.fasta \\\n    --out SM04 \\\n    --lineage_dataset pseudomonadales \\\n    --mode genome\n# (Only showing the bit of outkey results output)\n        ---------------------------------------------------\n        |Results from dataset pseudomonadales_odb10        |\n        ---------------------------------------------------\n        |C:99.6%[S:99.6%,D:0.0%],F:0.1%,M:0.3%,n:782       |\n        |779    Complete BUSCOs (C)                        |\n        |779    Complete and single-copy BUSCOs (S)        |\n        |0      Complete and duplicated BUSCOs (D)         |\n        |1      Fragmented BUSCOs (F)                      |\n        |2      Missing BUSCOs (M)                         |\n        |782    Total BUSCO groups searched                |\n        ---------------------------------------------------\n\n\nClick to see the full expected Busco output\n\n2024-02-05 17:31:00 INFO:       ***** Start a BUSCO v5.5.0 analysis, current time: 02/05/2024 17:31:00 *****\n2024-02-05 17:31:00 INFO:       Configuring BUSCO with local environment\n2024-02-05 17:31:00 INFO:       Mode is genome\n2024-02-05 17:31:00 INFO:       Downloading information on latest versions of BUSCO data...\n2024-02-05 17:31:03 INFO:       Input file is /fs/scratch/PAS2250/cabana/jelmer_prep/results/spades/SM04/contigs.fasta\n2024-02-05 17:31:03 INFO:       Downloading file 'https://busco-data.ezlab.org/v5/data/lineages/pseudomonadales_odb10.2024-01-08.tar.gz'\n2024-02-05 17:31:06 INFO:       Decompressing file '/fs/scratch/PAS2250/cabana/jelmer_prep/busco_downloads/lineages/pseudomonadales_odb10.tar.gz'\n2024-02-05 17:31:24 INFO:       Running BUSCO using lineage dataset pseudomonadales_odb10 (prokaryota, 2024-01-08)\n2024-02-05 17:31:24 INFO:       Running 1 job(s) on bbtools, starting at 02/05/2024 17:31:24\n2024-02-05 17:31:26 INFO:       [bbtools]       1 of 1 task(s) completed\n2024-02-05 17:31:26 INFO:       ***** Run Prodigal on input to predict and extract genes *****\n2024-02-05 17:31:26 INFO:       Running Prodigal with genetic code 11 in single mode\n2024-02-05 17:31:26 INFO:       Running 1 job(s) on prodigal, starting at 02/05/2024 17:31:26\n2024-02-05 17:31:44 INFO:       [prodigal]      1 of 1 task(s) completed\n2024-02-05 17:31:45 INFO:       Genetic code 11 selected as optimal\n2024-02-05 17:31:45 INFO:       ***** Run HMMER on gene sequences *****\n2024-02-05 17:31:45 INFO:       Running 782 job(s) on hmmsearch, starting at 02/05/2024 17:31:45\n2024-02-05 17:31:55 INFO:       [hmmsearch]     79 of 782 task(s) completed\n2024-02-05 17:32:04 INFO:       [hmmsearch]     157 of 782 task(s) completed\n2024-02-05 17:32:13 INFO:       [hmmsearch]     235 of 782 task(s) completed\n2024-02-05 17:32:21 INFO:       [hmmsearch]     313 of 782 task(s) completed\n2024-02-05 17:32:28 INFO:       [hmmsearch]     392 of 782 task(s) completed\n2024-02-05 17:32:34 INFO:       [hmmsearch]     470 of 782 task(s) completed\n2024-02-05 17:32:42 INFO:       [hmmsearch]     548 of 782 task(s) completed\n2024-02-05 17:32:47 INFO:       [hmmsearch]     626 of 782 task(s) completed\n2024-02-05 17:32:53 INFO:       [hmmsearch]     704 of 782 task(s) completed\n2024-02-05 17:33:01 INFO:       [hmmsearch]     782 of 782 task(s) completed\n2024-02-05 17:33:19 INFO:       Results:        C:99.6%[S:99.6%,D:0.0%],F:0.1%,M:0.3%,n:782        \n\n2024-02-05 17:33:21 INFO:\n\n        ---------------------------------------------------\n        |Results from dataset pseudomonadales_odb10        |\n        ---------------------------------------------------\n        |C:99.6%[S:99.6%,D:0.0%],F:0.1%,M:0.3%,n:782       |\n        |779    Complete BUSCOs (C)                        |\n        |779    Complete and single-copy BUSCOs (S)        |\n        |0      Complete and duplicated BUSCOs (D)         |\n        |1      Fragmented BUSCOs (F)                      |\n        |2      Missing BUSCOs (M)                         |\n        |782    Total BUSCO groups searched                |\n        ---------------------------------------------------\n2024-02-05 17:33:21 INFO:       BUSCO analysis done. Total running time: 138 seconds\n2024-02-05 17:33:21 INFO:       Results written in /fs/scratch/PAS2250/cabana/jelmer_prep/SM04\n2024-02-05 17:33:21 INFO:       For assistance with interpreting the results, please consult the userguide: https://busco.ezlab.org/busco_userguide.html\n\n2024-02-05 17:33:21 INFO:       Visit this page https://gitlab.com/ezlab/busco#how-to-cite-busco to see how to cite BUSCO\n\nThat is looking pretty good, 99.6% (n=779) of expected genes are indeed present completely and as a single copy. Only 0.1% (n=1) of genes are fragmented and 0.3% (n=2) are missing.\nFinally, we should move back to your main project dir, and load the main Conda environment\ncd ../..\n\nsource activate /fs/ess/PAS0471/jelmer/conda/cabana"
  },
  {
    "objectID": "06_assembly.html#bonus-assembly-filtering-with-kraken2",
    "href": "06_assembly.html#bonus-assembly-filtering-with-kraken2",
    "title": "Genome assembly and assembly QC",
    "section": "6 Bonus: Assembly filtering with Kraken2",
    "text": "6 Bonus: Assembly filtering with Kraken2\nYou’ll probably want to filter your assembly, based on:\n\nMinimum contig size\nMinimum contig depth of coverage\nInferred contamination from other organisms\n\nHere, we will perform the third and most complex of these.\n\n\n\n\n\n\nFiltering on contig size (Click to expand)\n\n\n\n\n\n200 bp is the minimum contig size when you upload a genome assembly to NCBI. But you may want to be more stringent, e.g. using a 300 or 500 bp threshold.\nTODO - Add seqkit command\n\n\n\n\n\n6.1 Kraken2\nTo identify contaminant contigs, we will run the program Kraken2 (Wood et al. 2019, manual). This is a general purpose lowest-common ancestor (LCA) taxonomic classifier of sequences (can be reads, contigs, etc).\n(It is also possible to run Kraken2 or a similar program on the reads rather than on the assembly, but this can be more error-prone due to errors and their shorter lengths compared to (most) contigs.)\nKraken requires a reference database. Ready-made databases can be downloaded from this site — in this case, I already downloaded one for you, which we can use.\nCheck that you are back in your bact dir, and have the cabana Conda environment active.\npwd\n# Should be:\n/fs/scratch/PAS2250/cabana/$USER/bact/bact\nWe will start by creating an output dir for Kraken:\n# (If we don't create the output dir, Kraken will not produce output files!)\nmkdir results/kraken\n\n\n\n6.2 Running Kraken\nWe’ll run Kraken2 with the following options:\n\n--db /fs/scratch/PAS2250/cabana/databases/kraken_std — the database for Kraken2\n--minimum-hit-groups 3 (Following recommendations from Lu et al. 2022)\n--confidence 0.15 (Following recommendations from Wright et al. 2023)\n--report and --output to indicate where the output files should go\n--threads 20 to use 20 threads\nAnd finally, the input file (our assembly) is passed as an argument at the end of the command\n\n# Run Kraken2 -- like with Spades, we will run it as a batch job\nsbatch -A PAS2250 -t 5 -c 20 -o slurm-kraken.out --wrap=\"\n  kraken2 \\\n    --db /fs/scratch/PAS2250/cabana/databases/kraken_std \\\n    --minimum-hit-groups 3 \\\n    --confidence 0.15 \\\n    --threads 20 \\\n    --report results/kraken/SM04_report.txt \\\n    --output results/kraken/SM04_main.txt \\\n    results/spades/SM04/contigs.fasta\n\"\nOnce its done (this should only take 1-2 minutes), the Slurm log file should contain the following:\ncat slurm-kraken.out\nLoading database information... done.\n86 sequences (5.97 Mbp) processed in 0.619s (8.3 Kseq/m, 578.49 Mbp/m).\n  84 sequences classified (97.67%)\n  2 sequences unclassified (2.33%)\n\n\n\n6.3 Interpreting the Kraken output\nLet’s take a look at the output files:\nls -lh results/kraken\n-rw-r--r-- 1 jelmer PAS0471 3.0M Feb  5 15:41 SM04_main.txt\n-rw-r--r-- 1 jelmer PAS0471 3.5K Feb  5 15:41 SM04_report.txt\nThe report file (report.txt) file has a summary of taxonomic assignments, whereas the main output file (main.txt) has one line for each contig with its taxonomic assignment.\nWe’ll first take a look at the report file, which has the following columns:\n\nPercentage of fragments covered by the clade rooted at this taxon\nNumber of fragments covered by the clade rooted at this taxon\nNumber of fragments assigned directly to this taxon\nA rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies.\nNCBI taxonomic ID number\nIndented scientific name\n\nless -S results/kraken/SM04_report.txt\n\n\nClick to show the contents of the file\n\n  2.33  2   2   U   0   unclassified\n 97.67  84  0   R   1   root\n 97.67  84  0   R1  131567    cellular organisms\n 73.26  63  0   D   2       Bacteria\n 72.09  62  3   P   1224          Pseudomonadota\n 68.60  59  2   C   1236            Gammaproteobacteria\n 58.14  50  0   O   72274             Pseudomonadales\n 58.14  50  1   F   135621              Pseudomonadaceae\n 56.98  49  12  G   286               Pseudomonas\n 43.02  37  1   G1  136849                  Pseudomonas syringae group\n 41.86  36  0   G2  251695                    Pseudomonas syringae group genomosp. 1\n 41.86  36  36  S   317                     Pseudomonas syringae\n  8.14  7   0   O   91347             Enterobacterales\n  8.14  7   2   F   543             Enterobacteriaceae\n  5.81  5   5   G   590               Salmonella\n  1.16  1   0   D1  1783272       Terrabacteria group\n  1.16  1   0   P   201174          Actinomycetota\n  1.16  1   0   C   1760              Actinomycetes\n  1.16  1   0   O   85009               Propionibacteriales\n  1.16  1   0   F   31957                 Propionibacteriaceae\n  1.16  1   0   G   1912216                 Cutibacterium\n  1.16  1   1   S   1747                      Cutibacterium acnes\n 24.42  21  0   D   2759        Eukaryota\n 24.42  21  0   D1  33154         Opisthokonta\n 24.42  21  0   K   33208           Metazoa\n 24.42  21  0   K1  6072              Eumetazoa\n 24.42  21  0   K2  33213               Bilateria\n 24.42  21  0   K3  33511                 Deuterostomia\n 24.42  21  0   P   7711                    Chordata\n 24.42  21  0   P1  89593                     Craniata\n 24.42  21  0   P2  7742                        Vertebrata\n 24.42  21  0   P3  7776                          Gnathostomata\n 24.42  21  0   P4  117570                          Teleostomi\n 24.42  21  0   P5  117571                            Euteleostomi\n 24.42  21  0   P6  8287                                Sarcopterygii\n 24.42  21  0   P7  1338369                               Dipnotetrapodomorpha\n 24.42  21  0   P8  32523                                   Tetrapoda\n 24.42  21  0   P9  32524                                     Amniota\n 24.42  21  0   C   40674                                       Mammalia\n 24.42  21  0   C1  32525                                         Theria\n 24.42  21  0   C2  9347                                            Eutheria\n 24.42  21  0   C3  1437010                                           Boreoeutheria\n 24.42  21  0   C4  314146                                              Euarchontoglires\n 24.42  21  0   O   9443                                                  Primates\n 24.42  21  0   O1  376913                                                  Haplorrhini\n 24.42  21  0   O2  314293                                                    Simiiformes\n 24.42  21  0   O3  9526                                                        Catarrhini\n 24.42  21  0   O4  314295                                                        Hominoidea\n 24.42  21  0   F   9604                                                            Hominidae\n 24.42  21  0   F1  207598                                                            Homininae\n 24.42  21  0   G   9605                                                                Homo\n 24.42  21  21  S   9606                                                                  Homo sapiens\n\n\n Exercise: Contamination?\nTry to interpret the Kraken report — are there any contaminant contigs?\n\n\nClick for the solution\n\nOuch! While the majority of our contigs have been classified as Pseudomonas syringae, we also have a few other bacteria (including the human skin bacterium Cutibacterium acnes), and no fewer than 21 human contigs!\n\n\n\nThe 5th column of the Kraken report has the NCBI taxonomic IDs, and that of human (on the last line) is 9606.\nThe main.txt output file reports the taxonomic ID in the 3rd column, so we can use the following command to print just the lines where the 3rd column is 9606:\nawk '$3 == 9606' results/kraken/SM04_main.txt\nC       NODE_28_length_766_cov_1.003130 9606    766     9606:4 131567:5 9606:1 131567:36 9606:11 131567:1 9606:20 131567:47 9606:16 131567:2 9606:5 131567:1 9606:1 131567:1 9606:6 131567:32 9606:68 131567:18 9606:95 131567:146 9606:12 131567:4 9606:11 131567:96 9606:28 131567:5 9606:11 131567:2 9606:29 131567:2 9606:1 131567:5 9606:10\nC       NODE_33_length_621_cov_0.706478 9606    621     9606:180 0:1 9606:3 0:17 9606:1 0:2 9606:12 0:32 9606:199 131567:5 9606:21 131567:2 1:1 9606:111\nC       NODE_36_length_567_cov_0.784091 9606    567     0:6 9606:527\nC       NODE_37_length_563_cov_0.779817 9606    563     9606:271 0:29 9606:229\nC       NODE_39_length_553_cov_0.809859 9606    553     0:1 9606:7 0:3 9606:261 0:3 9606:1 0:15 9606:3 0:5 9606:220\nC       NODE_40_length_552_cov_0.809412 9606    552     9606:41 131567:1 9606:1 131567:17 9606:10 131567:5 9606:56 131567:5 9606:89 131567:6 9606:63 131567:3 9606:84 131567:1 9606:1 131567:17 9606:10 131567:5 9606:103\nC       NODE_42_length_521_cov_0.746193 9606    521     9606:349 0:9 9606:1 0:21 9606:107\nC       NODE_44_length_510_cov_0.906005 9606    510     9606:476\nC       NODE_45_length_497_cov_0.772973 9606    497     9606:218 0:47 9606:1 0:28 9606:1 0:15 9606:1 0:4 9606:140 0:8\nC       NODE_47_length_480_cov_0.866856 9606    480     9606:446\nC       NODE_48_length_479_cov_0.849432 9606    479     9606:445\nC       NODE_50_length_470_cov_1.011662 9606    470     9606:117 0:1 9606:5 0:1 9606:3 0:24 9606:285\nC       NODE_51_length_470_cov_0.915452 9606    470     9606:436\nC       NODE_52_length_466_cov_1.017699 9606    466     9606:20 0:17 9606:6 0:9 9606:135 0:30 9606:215\nC       NODE_54_length_458_cov_1.000000 9606    458     9606:213 0:4 9606:5 0:15 9606:1 0:5 9606:125 0:20 9606:2 0:5 9606:3 0:5 9606:21\nC       NODE_55_length_455_cov_1.179878 9606    455     9606:2 131567:3 9606:218 131567:3 9606:195\nC       NODE_60_length_444_cov_0.933754 9606    444     9606:410\nC       NODE_61_length_442_cov_0.907937 9606    442     9606:21 131567:1 9606:386\nC       NODE_65_length_438_cov_0.405145 9606    438     0:24 9606:2 0:8 9606:80 131567:2 9606:31 131567:19 9606:10 131567:3 9606:13 131567:7 9606:205\nC       NODE_66_length_433_cov_1.133987 9606    433     9606:399\nC       NODE_67_length_432_cov_0.718033 9606    432     9606:398\n\n Exercise: Which contigs are contaminants?\nIn the output above, the contig IDs are in the second column. Do you notice anything about these? Does that provide some independent support for the idea that they are contaminants?\n\n\nClick for the solution\n\nAll of these contigs are small (&lt;600 bp) and have very low coverage (&lt;1.2x, versus the &gt;30x we saw for the contigs IDs that we printed earlier).\nNote that Kraken doesn’t use this kind of information at all, so this provides independent evidence that this is contamination.\n\n\n\n\n\n6.4 Removing contaminant contigs\nWe will use the Kraken’s companion program KrakenTools (paper, documentation) to remove the contaminant contigs, with options:\n\n-k — main Kraken output file\n-s — input sequence file to be filtered\no — output sequence file\n-t — NCBI taxonomic ID (9606 = human)\n--exclude — exclude (rather than extract) contigs with the specified taxonomic ID\n\nmkdir results/decontam\nextract_kraken_reads.py \\\n    -k results/kraken/SM04_main.txt \\\n    -s results/spades/SM04/contigs.fasta \\\n    -o results/decontam/SM04.fasta \\\n    -t 9606 \\\n    --exclude\nPROGRAM START TIME: 02-05-2024 22:16:21\n        1 taxonomy IDs to parse\n&gt;&gt; STEP 1: PARSING KRAKEN FILE FOR READIDS results/kraken/SM04.main.txt\n        0.00 million reads processed\n        65 read IDs saved\n&gt;&gt; STEP 2: READING SEQUENCE FILES AND WRITING READS\n        65 read IDs found (0.00 mill reads processed)\n        65 reads printed to file\n        Generated file: results/decontam/SM04.fasta\nPROGRAM END TIME: 02-05-2024 22:16:21\nLet’s check that we indeed have 65 contigs left:\ngrep -c \"&gt;\" results/decontam/SM04.fasta\n65"
  },
  {
    "objectID": "06_assembly.html#footnotes",
    "href": "06_assembly.html#footnotes",
    "title": "Genome assembly and assembly QC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that because individual sequence entries are commonly spread across multiple lines, FASTA entries do not necessarily cover 2 lines (cf. FASTQ).↩︎"
  },
  {
    "objectID": "ref_data.html#making-your-valuable-data-read-only",
    "href": "ref_data.html#making-your-valuable-data-read-only",
    "title": "Data management",
    "section": "1 Making your valuable data read-only",
    "text": "1 Making your valuable data read-only\nYour raw FASTQ data is extremely invaluable as it contains the result of your experiment and was produced by an expensive sequencing process. You’ll always want to keep these files around, and will for example also need to make them publicly available when you publish your results (they are typically deposited at the NCBI’s SRA).\nYou should therefore always have at least one backup copy of your data, and store data in supercomputer location that is backed-up (like the /fs/ess dirs at OSC).\nEven with those automatic back-ups and extra copies, it’s still good practice to be careful with every copy of your data. File removal with (e.g.) rm is permanent and irreversible, so to avoid disaster after an accidental removal, it’s a good idea to “write-protect” your FASTQ files (or, phrased differently, to make them “read-only”).\nFirst, let’s briefly recap and expand our knowledge of file permissions:\n\nRead permissions allow you to read and copy files/dirs\nWrite permissions allow you to move, rename, modify, overwrite, or delete\nExecute permissions allow you to directly execute a file (e.g. running a program, or a script as a command).\n\nThese permissions can be most easily set for three different groups of people:\n\nOwner (or “user”) — By default, this the person that created the file or dir. After you have copied or downloaded some FASTQ files, for example, you are the owner of these copies.\nGroup — When you create a file in the PAS0471 project dir, its “group” will include all members of the OSC project PAS0471.\nOther — In the example above, anyone with access to OSC that is not a member of PAS0471.\n\nWe can see what these permissions are for any file or dir by looking at the first column of the output of ls -l. Let’s practice write-protection and its consequences with a dummy file — first, we create the dummy file and check the default permissions:\n# Create a new, empty file with the 'touch' command\ntouch permission_test_file\n\nls -l permission_test_file\n# (Note, the very first dash below is about file _type_, not permissions:)\n -rw-r--r-- 1 jelmer PAS0471 0 Aug  7 16:27 permission_test_file\n\n\n\n\n\nThe command to set (read/write/execute) permissions for these categories of people (user/group/others, or everyone together) is chmod. There are multiple ways of using this command, but a common one is along the lines of chmod &lt;who&gt;&lt;operation&gt;&lt;permission&gt;, with e.g.:\n\nchmod a-w meaning “all minus write”: remove write-permissions for all\nchmod o+r meaning “others plus read”: add read-permissions for others.\n\nTo remove write-permission for everyone, including yourself, we will use:\nchmod a-w permission_test_file\n\nls -l permission_test_file\n# (Notice that there is no longer a 'w' in the sequence below:)\n-r--r--r-- 1 jelmer PAS0471 0 Aug  7 16:27 permission_test_file\nNow, let’s see what happens when we try to remove this file:\nrm permission_test_file\nrm: remove write-protected regular empty file ‘permission_test_file’? n\nNote that we can still remove this file by answering y, but we will hopefully not act that carelessly, and the question will make us reconsider and press n (also, if you made a mistake in a script that you run non-interactively, it will fail to remove write-protected files)."
  },
  {
    "objectID": "ref_data.html#checking-file-integrity-after-transfer",
    "href": "ref_data.html#checking-file-integrity-after-transfer",
    "title": "Data management",
    "section": "2 Checking file integrity after transfer",
    "text": "2 Checking file integrity after transfer\nWhen you receive your FASTQ files from a sequencing facility, a small text file will usually accompany your FASTQ files, and will have a name along the lines of md5.txt, md5checksums.txt, or shasums.txt.\nSuch a file contains so-called checksums, a sort of digital fingerprints for your FASTQ files, which can be used to check whether your copy of these files is completely intact. Checksums are extremely compact summaries of files, computed so that even if just one character is changed in the data, the checksum will be different.\n\n\n\n\n\n\nMore on checksums\n\n\n\nSeveral algorithms and their associated shell commands can compute checksums. Like in our case, you’ll most often see md5 checksums accompany genomic data files, which can be computed and checked with the md5sum command (the newer SHA-1 checksums can be computer and checked with the very similar shasum command).\nChecksums consist of hexadecimal characters only: numbers and the letters a-f.\nWe typically compute or check checksums for one or more files, but we can even do it for a string of text — the example below demonstrates that the slightest change in a string (or file alike) will generate a completely different checksum:\necho \"bioinformatics is fun\" | md5sum\n010b5ebf7e207330de0e3fb0ff17a85a  -\necho \"bioinformatic is fun\" | md5sum\n45cc2b76c02b973494954fd664fc0456  -\n\n\nYou can check your checksums as follows:\ncd data/fastq\nmd5sum -c md5sums.txt \nASPC1_A178V_R1.fastq.gz: OK\nASPC1_A178V_R2.fastq.gz: OK\nASPC1_G31V_R1.fastq.gz: OK\nASPC1_G31V_R2.fastq.gz: OK\nMiapaca2_A178V_R1.fastq.gz: OK\nMiapaca2_A178V_R2.fastq.gz: OK\nMiapaca2_G31V_R1.fastq.gz: OK\nMiapaca2_G31V_R2.fastq.gz: OK\nIf there were any differences, the md5sum command would clearly warn you about them, as you can see in the exercise below."
  },
  {
    "objectID": "05_preprocess.html#introduction",
    "href": "05_preprocess.html#introduction",
    "title": "Read QC & preprocessing",
    "section": "Introduction",
    "text": "Introduction\nThe first series of steps our bacterial whole-genome analysis workflow concerns the quality control (QC) & “preprocessing” of the reads.\nThe QC part will leave the data untouched, whereas the preprocessing involved the removal of unwanted bits of sequence. After the preprocessing, we will still have FASTQ files, just with somewhat less content.\nWe will preprocess our reads with the following steps:\n\nQC with FastQC\nSummarizing FastQC results with MultiQC\nRemoving adapters and low-quality bases from our reads with TrimGalore\n\n\n\n Setting up\n\nStart a new VS Code session with an open terminal:\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2250\nThe starting directory: /fs/scratch/PAS2250/cabana/&lt;user&gt;\nNumber of hours: 10\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal.\nIn the terminal, move into your bact/bact dir:\ncd bact/bact # Full path: /fs/scratch/PAS2250/cabana/$USER/bact/bact\nOptional: open a file to save your commands. In the Explorer (file browser) on the left, you should be able to find the workflow.sh script you created yesterday."
  },
  {
    "objectID": "05_preprocess.html#the-fastq-format",
    "href": "05_preprocess.html#the-fastq-format",
    "title": "Read QC & preprocessing",
    "section": "1 The FASTQ format",
    "text": "1 The FASTQ format\nFASTQ is a very common output format of high-throughput sequencing machines — at least from Illumina sequencing, you will almost always receive the sequences in this format. Like most genomic data files, these are plain text files, and each sequence that is read by the sequencer (i.e., each “read”) forms one FASTQ entry represented by four lines. The lines contain, respectively:\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe sequence itself\nA + (plus sign)\nOne-character quality scores for each base in the sequence\n\n\n\n\n\nOne entry (read) in a FASTQ file covers 4 lines. The header line is annotated, with some of the more useful components highlighted in red. For viewing purposes, this read (at only 56 bp) is shorter than what is typical.\n\n\n\nThe “Q” in FASTQ stands for “quality”, to contrast this format with FASTA, a more basic and generic format that does not include base quality scores. FASTQ files have the extension .fastq or .fq, but they are very commonly gzip-compressed, in which case their name ends in .fastq.gz or .fq.gz.\n\n\n\n\n\n\nFASTQ quality scores (Click to expand)\n\n\n\n\n\nThe quality scores we saw in the read above represent an estimate of the error probability of the base call. Specifically, they correspond to a numeric “Phred” quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\nFor some specific probabilities and their rough qualitative interpretations for Illumina data:\n\n\n\n\n\n\n\n\n\nPhred quality score\nError probability\nRough interpretation\nASCII character\n\n\n\n\n10\n1 in 10\nterrible\n+\n\n\n20\n1 in 100\nbad\n5\n\n\n30\n1 in 1,000\ngood\n?\n\n\n40\n1 in 10,000\nexcellent\n?\n\n\n\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character” (last column in the table). This allows for a single-character representation of each possible score — as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read. (For your reference, here is a complete lookup table — look at the top table, “BASE=33”)."
  },
  {
    "objectID": "05_preprocess.html#our-fastq-files",
    "href": "05_preprocess.html#our-fastq-files",
    "title": "Read QC & preprocessing",
    "section": "2 Our FASTQ files",
    "text": "2 Our FASTQ files\n\n2.1 Listing your FASTQ files\nFirst, let’s take another look at your list of FASTQ files:\nls -lh data/fastq\ntotal 6.1G\n-rw-r--r-- 1 jelmer PAS2250 205M Feb  4 11:47 SM04_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 242M Feb  4 11:46 SM04_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 188M Feb  4 11:46 SM1030_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 221M Feb  4 11:46 SM1030_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 187M Feb  4 11:46 SM1031_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 221M Feb  4 11:46 SM1031_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 187M Feb  4 11:46 SM1038_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 224M Feb  4 11:46 SM1038_R2.fastq.gz\n# [...output truncated...]\nAs we discussed yesterday, in the file listing above:\n\nThe files all have a .gz extension, indicating they have been compressed with the gzip utility.\nThere are two files per sample: _R1 (forward reads) and _R2 (reverse reads).\n\n\n\n\n2.2 Paired-end data\nAs mentioned, we’ll be working with paired-end (PE) Illumina sequencing data. Therefore, we have two files for each sequenced sample:\n\nThe file with forward reads has _R1 (or sometimes _1) in its name, e.g. SM04_R1.fastq.gz\nThe file with reverse reads has _R2 (or sometimes _2) in its name, e.g. SM04_R2.fastq.gz\n(i.e., the file name should be identical to that for the forward read, except for the read direction identifier).\n\n\n\n\n2.3 Viewing FASTQ files\nNext, we’ll take a peak inside one of these FASTQ files.\n\nBasic commands to view files\nLet’s consider our options to look inside one of these files:\n\ncat prints the entire contents of a file to screen — probably not a good idea for such a large file.\nhead and tail print just the first and last lines of a file, respectively, which could perhaps be a good start?\n\nLet’s try to print 8 lines, which should show us two reads:\nhead -n 8 data/fastq/SM04_R1.fastq.gz\n�\nԽے�8�E��_1f�\"�QD�J��D�fs{����Yk����d��*��\n|��x���l޴�j�N������?������ٔ�bUs�Ng�Ǭ���i;_��������������|&lt;�v����3��������|���ۧ��3ĐHyƕ�bIΟD�%����Sr#~��7��ν��1y�Ai,4\nw\\]\"b�#Q����8��+[e�3d�4H���̒�l�9LVMX��U*�M����_?���\\[\"��7�s\\&lt;_���:�$���N��v�}^����sw�|�n;&lt;�&lt;�oP����\ni��k��q�ְ(G�ϫ��L�^��=��&lt;���K��j�_/�[ۭV�ns:��U��G�z�ݎ�j����&��~�F��٤ZN�'��r2z}�f\\#��:�9$�����H�݂�\"�@M����H�C�\n�0�pp���1�O��I�H�P됄�.Ȣe��Q�&gt;���\n�'�;@D8���#��St�7k�g��|�A䉻���_���d�_c������a\\�|�_�mn�]�9N������l�٢ZN�c�9u�����n��n�`��\n\"gͺ�\n    ���H�?2@�FC�S$n���Ԓh�       nԙj��望��f      �?N@�CzUlT�&�h�Pt!�r|��9~)���e�A�77�h{��~��     ��\n# [...output truncated...]\n\n\nOuch! 😳 What went wrong here? (Click for the solution)\n\nWhat happened here is that we are directly seeing the contents of the compressed file, which is simply not human-readable.\n\n\nTo get around this, we might be inclined to uncompress these files, which we could do with the gunzip command. However, uncompressed files take up several times as much disk storage space as compressed ones.\nFortunately, we don’t need to decompress them: - Almost any bioinformatics tool will accept compressed FASTQ files. - We can still view these files in compressed form, as shown below.\n\n\n\nThe less pager\nHere, we’ll use the less command, which will automatically display gzip-compressed files in human-readable form — let’s try it:\nless -S data/fastq/SM04_R1.fastq.gz\n\n\n\n\n\n\nAvoid line-wrapping with less -S (Click to expand)\n\n\n\n\n\nDepending on your zoom level and the length of reads in your FASTQ file, some lines may contain too many characters to fit on your screen. If that’s the case, less will by default “wrap” those lines onto the next line on your screen, so characters won’t run off the screen on the right-hand side.\nThat may be useful when the file contains text you’re trying to read in full, but it is often confusing for files like FASTQ as well as for tabular files. That’s why we turned off line-wrapping with the -S option.\n\n\n\n\n\n Exercise: Explore the file with less\nless doesn’t print stuff to screen but instead opens it in a “pager”. After running the command above, you should be viewing the file inside the less pager.\nYou can move around in the file in several ways: by scrolling with your mouse, with up and down arrows, or, if you have them, PgUp and PgDn keys (also, u will move up half a page and d down half a page).\nIf you find yourself scrolling down and down to try and reach the end of the file, you can instead press G to go to the very end right away (and g to go back to the top).\nNotice you won’t get your shell prompt back until you press q to quit less."
  },
  {
    "objectID": "05_preprocess.html#running-fastqc",
    "href": "05_preprocess.html#running-fastqc",
    "title": "Read QC & preprocessing",
    "section": "3 Running FastQC",
    "text": "3 Running FastQC\n\n3.1 Intro to FastQC\nFastQC is a ubiquitous tools for quality control of FASTQ files. Running FastQC or a similar program is the first step in nearly any high-throughput sequencing project. FastQC is also a good first example of a tool with a command-line interface.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser with about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: A FastQC per-base quality score graph for files with reasonably good (left) and very poor (right) quality reads. The y-axis shows Phred quality scores (higher is better, see also the color-coding) and the x-axis shows the position along the read.\n\n\n\n\n\n\n3.2 Building our FastQC command\nTo run FastQC, we can use the command fastqc.\nIf you want to analyze one of your FASTQ files with default FastQC settings, a complete FastQC command to do so would simply be fastqc followed by the name of the file:\n# (Don't run this)\nfastqc data/fastq/SM04_R1.fastq.gz\nHowever, an annoying FastQC default behavior is that it writes its output files in the dir where the input files are — in general, it’s not great practice to directly mix your primary data and results like that!\nTo figure out how we can change that behavior, first consider that many commands and bioinformatics tools alike have an option -h and/or --help to print usage information to the screen.\nLet’s try that:\nfastqc -h\nbash: fastqc: command not found...\nHowever, there is a wrinkle, as you can see above. While FastQC is installed at OSC1, we have to “load it” with the module load command before we can use it.\nHowever, we will be using a whole array of bioinformatics programs, and we are going to use a (nearly) one-stop solution: a so-called “Conda environment” that has all of those programs installed.\n\n\n\n\n\n\nConda and software management\n\n\n\nWe won’t have time to get into this now, but you want to learn more about Conda / software usage at supercomputers, see this reference page elsewhere on the website.\n\n\nHere’s how we can load that Conda software environment — we first load OSC’s (mini)conda installation, and then we can load (“activate”) the Conda environment that I created for you:\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/cabana\n\n Exercise: FastQC help and output dir\nAgain try to print FastQC’s help info, and figure out which option you can use to specify a custom output directory.\n\n\nClick for the solution\n\nfastqc -h and fastqc --help will both work to show the help info.\nYou’ll get quite a bit of output printed to screen, including the snippet about output directories that is reproduced below:\nfastqc -h\n  -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\nSo, you can use -o or equivalently, --outdir to specify an output dir.\n\n\n\nWith the added --outdir (or -o) option, let’s try to run the following FastQC command:\nfastqc \\\n  --outdir results/fastqc_pretrim \\\n  data/fastq/SM04_R1.fastq.gz\nSpecified output directory 'results/fastqc_pretrim' does not exist\n\n\n\n\n\n\nSpreading commands across multiple lines with \\\n\n\n\nAbove, I spread the command across multiple lines, which makes it a little easier to read. You can run the command exactly like that: we use the backslashes (\\) at the end of all but the last line to indicate to the shell that our command will continue on the next line.\n\n\n\n\n Exercise: Fixing the FastQC error\nNow what is going on this time? 😨 Or had you perhaps seen this coming given the help text we saw earlier? At any rate, can you try to fix the problem?\n\n\nClick here for a hint\n\nYou’ll need to create a new directory, which you can do either by using the buttons in the VS Code side bar, or with the mkdir command — here, try it as mkdir -p followed by the name (path) of the directory you want to create.\n\n\n\nClick here for the solution\n\n\nThe problem, as the error fairly clearly indicates, is that the output directory that we specified with --outdir does not currently exist. We might have expected FastQC to be smart/flexible enough to create this dir for us (many bioinformatics tools are), but alas. On the other hand, if we had read the help text clearly, it did warn us about this.\nWith the mkdir command, to create “two levels” of dirs at once, like we need to here (both results and then fastqc within there), we need its -p option:\n\nmkdir -p results/fastqc_pretrim\n\n\n\nAnd for our final try before we give up and throw our laptop out of the window (make sure to run the code in the exercise solution before you retry!):\nfastqc \\\n  --outdir results/fastqc_pretrim \\\n  data/fastq/SM04_R1.fastq.gz\napplication/gzip\nStarted analysis of SM04_R1.fastq.gz\nApprox 5% complete for SM04_R1.fastq.gz\nApprox 10% complete for SM04_R1.fastq.gz\nApprox 15% complete for SM04_R1.fastq.gz\n[...truncated...]\nAnalysis complete for SM04_R1.fastq.gz\nSuccess!! 🎉\n\n\n\n3.3 FastQC output files\nLet’s take a look at the files in the output dir we specified:\nls -lh results/fastqc_pretrim\ntotal 1.2M\n-rw-r--r-- 1 jelmer PAS0471 713K Feb  4 14:02 SM04_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 431K Feb  4 14:02 SM04_R1_fastqc.zip\n\nThere is a .zip file, which contains tables with FastQC’s data summaries\nThere is an .html (HTML) file, which contains plots — this is what we’ll look at next\n\n\n\n Exercise: Another FastQC run\nRun FastQC for the corresponding R2 FASTQ file. Would you use the same output dir?\n\n\nClick for the solution\n\nYes, it makes sense to use the same output dir, since as you could see above, the output file names have the input file identifiers in them. As such, we don’t need to worry about overwriting files, and it will be easier to have all the results in a single dir.\nTo run FastQC for the R2 (reverse-read) file:\nfastqc \\\n  --outdir results/fastqc_pretrim \\\n  data/fastq/SM04_R2.fastq.gz\nStarted analysis of SM04_R2.fastq.gz\nApprox 5% complete for SM04_R2.fastq.gz\nApprox 10% complete for SM04_R2.fastq.gz\nApprox 15% complete for SM04_R2.fastq.gz\n[...truncated...]\nAnalysis complete for SM04_2.fastq.gz\nls -lh results/fastqc_pretrim\n-rw-r--r-- 1 jelmer PAS0471 241K Jan 21 21:50 SM04_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 256K Jan 21 21:50 SM04_R1_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 234K Jan 21 21:53 SM04_R2_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 244K Jan 21 21:53 SM04_R2_fastqc.zip\nNow, we have four files: two for each of our preceding successful FastQC runs."
  },
  {
    "objectID": "05_preprocess.html#interpreting-fastqc-output",
    "href": "05_preprocess.html#interpreting-fastqc-output",
    "title": "Read QC & preprocessing",
    "section": "4 Interpreting FastQC output",
    "text": "4 Interpreting FastQC output\n\n4.1 FastQC HTML modules\nWe’ll now go through a couple of the FastQC plots/modules, with first some example plots2 with good/bad results for reference.\n\nOverview of module results\nFastQC has “pass” (checkmark in green), “warning” (exclamation mark in orange), and “fail” (cross in red) assessments for each module, as you can see below.\nThese are handy and typically at least somewhat meaningful, but it is important to realize that a “warning” or a “fail” is not necessarily the bad news that it may appear to be, because, e.g.:\n\nSome of these modules could perhaps be called overly strict.\nSome warnings and fails are easily remedied or simply not a very big deal.\nFastQC assumes that your data is derived from whole-genome shotgun sequencing — some other types of data like RNA-seq data will always trigger a couple of warnings and files based on expected differences.\n\n\n\n\n\n\n\nBasic statistics\nThis shows, for example, the number of sequences (reads) and the read length range for your file:\n\n\n\n\n\n\nPer base quality sequence quality\nThis figure visualize the mean per-base quality score (y-axis) along the length of the reads (x-axis). Note that:\n\nA decrease in sequence quality along the reads is normal.\nR2 (reverse) reads are usually worse than R1 (forward) reads.\n\n\n\nGood / acceptable:\n\n\n\n\nBad:\n\n\n\n\n\nTo interpret the quality scores along the y-axis, note the color scaling in the graphs (green is good, etc.), and see this table for details:\n\n\n\nPhred quality score\nError probability\nRough interpretation\n\n\n\n\n10\n1 in 10\nterrible\n\n\n20\n1 in 100\nbad\n\n\n30\n1 in 1,000\ngood\n\n\n40\n1 in 10,000\nexcellent\n\n\n\n\n\n\nPer sequence quality scores\nThis shows the same quality scores we saw above, but now simply as a density plot of per-read averages, with the quality score now along the x-axis, and the number of reads with that quality score along the y-axis:\n\n\nGood:\n\n\n\n\nBad:\n\n\n\n\n\n\n\n\nSequence length distribution\nWill throw a warning as soon as not all sequences are of the same length (like below), but this is quite normal.\n\n\n\n\n\n\nAdapter content\nChecks for known adapter sequences. When some of the insert sizes are shorter than the read length, adapters can end up in the sequence – these should be removed!\n\n\nGood:\n\n\n\n\nBad:\n\n\n\n\n\n\n\n\n\n4.2 Checking your FastQC results\nFirst, you’ll unfortunately have to download FastQC’s output HTML files to your computer:\n\nFind the FastQC HTML files in the file explorer in the VS Code side bar.\nRight-click on one of them, click Download... and follow the prompt to download the file somewhere to your computer (doesn’t matter where).\nRepeat this for the second file\nThen, open your computer’s file browser, find the downloaded files, and double-click on one. It should be opened in your default web browser.\n\n\n Exercise: Interpreting your FastQC results\n\nOpen the HTML file for the R1 FASTQ file and go through the modules we discussed above. Can you make sense of it? Does the data look good to you, overall?\nNow open the HTML file for the R2 FASTQ file and take a look just at the quality scores. Does it look any worse than the R1?"
  },
  {
    "objectID": "05_preprocess.html#summarizing-qc-results-with-multiqc",
    "href": "05_preprocess.html#summarizing-qc-results-with-multiqc",
    "title": "Read QC & preprocessing",
    "section": "5 Summarizing QC results with MultiQC",
    "text": "5 Summarizing QC results with MultiQC\nHere are some challenges you may run into after running FastQC:\n\nWhen you have many FASTQ files, you’ll generate a lot of FastQC HTML files to sort through. Our dataset is small with only 16 samples, but this still means 32 FastQC outputs. Other datasets may easily have dozens or even hundreds of samples, in which case checking all of the output becomes a very unpleasant task.\nEven if you do diligently go through each file, it’s not that easy to compare samples, since they are not drawn in the same graphs.\n\nMultiQC addresses these problems as it aggregates FastQC results from many files, and summarizes them into a single HTML file with (still) one graph per FastQC module.\n\n\n\n\n\n\nNot just for FastQC results!\n\n\n\nAnd while MultiQC is most widely used for FastQC aggregation, it can recognize and process the (often “log”-type) output of dozens of bioinformatics tools, including several others that we will be using.\n\n\nMultiQC’s graphs are also interactive, but here is a static example:\n\n\n\n\n\n\n\n5.1 Running MultiQC\nTo run MultiQC, use the command multiqc. Let’s start by running it with the --help option:\nmultiqc --help\n# (Only the top part of the output is shown in the screenshot below)\n\n\n\n\n\nAs the first couple of help lines in the paler gray color explain, MultiQC will search the [ANALYSIS DIRECTORY], a dir that we pass to it as an argument at the end of the command line.\nThat is, if we tell MultiQC about the results/fastqc_pretrim directory like so, it should find and then aggregate all the FastQC results in there:\n# (Don't run this - we'll complete the command in a second)\nmultiqc /fs/scratch/PAS2250/cabana/bact_results/fastqc_pretrim\n\n\n\n\n\n\nSince you ran FastQC on only sample, we’ll be using the FastQC results for all files that I generated for you.\n\n\n\n\n\n\nThe default output directory of MultiQC is the current working directory, so just like with FastQC, we do want to use that option as well3:\n# Run MultiQC to summarize the FastQC results\nmultiqc \\\n  --outdir results/multiqc_fastqc \\\n  /fs/scratch/PAS2250/cabana/bact_results/fastqc_pretrim\n\n\n\n\n\n\n\n\n5.2 MultiQC output\nThen, you should have some files in the output dir:\nls -lh results/multiqc_fastqc\ntotal 1.7M\ndrwxr-xr-x 2 jelmer PAS2250 4.0K Feb  4 14:57 multiqc_data\n-rw-r--r-- 1 jelmer PAS2250 1.7M Feb  4 14:57 multiqc_report.html\nGo ahead and find the HTML file in VS Code’s file browser, right-click on it and then download it to your computer, and click on the file in your own computer to open it in your browser (i.e., just like we did with the FastQC output).\n\n Exercise: Explore the MultiQC results\nCheck for example whether patterns are consistent across samples, or if there are any outliers."
  },
  {
    "objectID": "05_preprocess.html#trimming-with-trimgalore",
    "href": "05_preprocess.html#trimming-with-trimgalore",
    "title": "Read QC & preprocessing",
    "section": "6 Trimming with TrimGalore",
    "text": "6 Trimming with TrimGalore\nWe will run TrimGalore to filter our FASTQ files, removing:\n\nAny adapter sequences that may be present in the reads\nPoor-quality bases at the start and end of the reads\nVery short reads (in most cases made short by the prior two steps)\n\n\n\n\n\n\n\nAlternative trimming programs (Click to expand)\n\n\n\n\n\nSeveral largely equivalent tools exist for this kind of FASTQ preprocessing — Trimmomatic and fastp are two other commonly used ones. (And TrimGalore itself is mostly a wrapper around another tool called CutAdapt.)\nTwo advantages of of TrimGalore are that it will auto-detect the adapters that are present in your reads (e.g., different library prep protocols use different adapters), and that it can automatically run FastQC on the processed sequences.\n\n\n\nUnfortunately, we’ll first have to switch Conda environments for TrimGalore:\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n\n6.1 Running TrimGalore\nWe will run TrimGalore for one sample, but because we have paired-end reads:\n\nWe’ll specify two FASTQ file names: one with the forward (R1) reads, and one with the reverse (R2) reads.\nWe’ll have to use the --paired option (see the box below for details if you’re interested)\n\n\n\n\n\n\n\nMore on paired-end trimming (Click to expand)\n\n\n\n\n\nWhen we have paired-end reads, much of the trimming happens separately for the R1 (forward) and R2 (reverse) files, but at the end of the run, TrimGalore will make sure that every R1 read still has its R2 counterpart, and vice versa.\nHowever, we have to use the --paired option for this: without it, TrimGalore will only process the R1 and R2 files separately, and omit the final step where it removes orphaned reads.\nWith the --paired option, any “orphaned” reads will by default be removed, because R1 and R2 files for the same samples always need to contain all the same reads. (TrimGalore does have an option to retain these orphaned reads into separate files, but we won’t use that.)\n\n\n\nSo far, our command looks like this:\n# (Don't run this)\ntrim_galore \\\n  --paired \\\n  data/fastq/SM04_R1.fastq.gz \\\n  data/fastq/SM04_R2.fastq.gz\n\nIn terms of other TrimGalore options:\n\nUse the default settings for adapters (auto-detection and removal) and the base quality threshold (a Phred score of 20)\nUse a longer read length (36 bp) than the default of 36 bp: --length 36.\nSpecify the output directory4: --output_dir results/trimgalore.\nHave TrimGalore run FastQC for us on the filtered FASTQ files, so we can e.g. check if adapters were successfully removed.\n\n\n\n\n\n\n\nChecking the TrimGalore help (Click to expand)\n\n\n\n\n\nYou might also want to run TrimGalore with the --help option to learn how to run it:\ntrim_galore --help\n# Note: Below I am only showing (truncated) output for the key options!\n USAGE:\ntrim_galore [options] &lt;filename(s)&gt;\n\n--paired                This option performs length trimming of quality/adapter/RRBS trimmed reads for\n                        paired-end files.\n-o/--output_dir &lt;DIR&gt;   If specified all output will be written to this directory instead of the current\n                        directory. If the directory doesn't exist it will be created for you.\n--fastqc                Run FastQC in the default mode on the FastQ file once trimming is complete.\n--fastqc_args \"&lt;ARGS&gt;\"  Passes extra arguments to FastQC.\n-a/--adapter &lt;STRING&gt;   Adapter sequence to be trimmed. If not specified explicitly, Trim Galore will\n                        try to auto-detect whether the Illumina universal, Nextera transposase or Illumina\n                        small RNA adapter sequence was used.\n-q/--quality &lt;INT&gt;      Trim low-quality ends from reads in addition to adapter removal.\n--length &lt;INT&gt;          Discard reads that became shorter than length INT because of either\n                        quality or adapter trimming. A value of '0' effectively disables\n                        this behaviour. Default: 20 bp.\n\n\n\nA final test command to run TrimGalore on our actual (but small, subsetted) FASTQ files in data/fastq could therefore look as follows:\n # Once again, we have to make the FastQC outdir!\nmkdir results/fastqc_posttrim\n\n# Run TrimGalore\ntrim_galore \\\n    --paired \\\n    --length 36 \\\n    --output_dir results/trimgalore \\\n    --fastqc_args \"--outdir results/fastqc_posttrim\" \\\n    data/fastq/SM04_R1.fastq.gz \\\n    data/fastq/SM04_R2.fastq.gz\nMulticore support not enabled. Proceeding with single-core trimming.\nPath to Cutadapt set as: 'cutadapt' (default)\nCutadapt seems to be working fine (tested command 'cutadapt --version')\nCutadapt version: 4.4\nsingle-core operation.\nigzip command line interface 2.30.0\nigzip detected. Using igzip for decompressing\n\nNo quality encoding type selected. Assuming that the data provided uses Sanger encoded Phred scores (default)\n\nOutput directory results/trimgalore/ doesn't exist, creating it for you...\n\nOutput will be written into the directory: /fs/scratch/PAS2250/cabana/jelmer/bact/results/trimgalore/\n\nAUTO-DETECTING ADAPTER TYPE\n===========================\nAttempting to auto-detect adapter type from the first 1 million sequences of the first file (&gt;&gt; data/fastq/SM04_R1.fastq.gz &lt;&lt;)\n# [...output truncated...]\n\n\n\n\n\n\nTrimming strictness\n\n\n\nThe exact choice of trimming strictness parameters, especially regarding the minimum read length and minimum base quality is fairly arbitrary, and may not have a large effect on the final assembly. That said, while read lengths of 20 may still be useful in a read mapping context, they are less so in a de novo assembly context, so we use a higher read length threshold.\n\n\n\n\n\n6.2 TrimGalore output\nAfter you ran the command above, a lot of logging output should have been printed to screen.\nFor example, it reports the adapter that it detected, the final parameters passed to Cutadapt (which does the actual trimming), and results on how much sequence was removed.\n\n Exercise: Check the TrimGalore logging output\nLook for === Summary === sections (two of them, one for the R1 and one for the R2) to answer the following questions:\n\nWhat percentage of the reads had adapter sequences?\nWhat percentage of basepairs were quality-trimmed from the R1 and R2 files, respectively?\n\nLook near the end of the output to answer the following question:\n\nHow many reads were removed due to the length-filter?\n\n\n\nClick for the solution\n\nSome of the more relevant information that should have been printed:\n\nFor the R1 file:\n\n=== Summary ===\n\nTotal reads processed:               1,031,129\nReads with adapters:                   398,726 (38.7%)\nReads written (passing filters):     1,031,129 (100.0%)\n\nTotal basepairs processed:   275,173,541 bp\nQuality-trimmed:               7,733,000 bp (2.8%)\nTotal written (filtered):    266,857,947 bp (97.0%)\n\nFor the R2 file:\n\n=== Summary ===\n\nTotal reads processed:               1,031,129\nReads with adapters:                   424,883 (41.2%)\nReads written (passing filters):     1,031,129 (100.0%)\n\nTotal basepairs processed:   276,647,678 bp\nQuality-trimmed:              42,814,676 bp (15.5%)\nTotal written (filtered):    233,141,370 bp (84.3%)\n\nAnd at the end:\n\nNumber of sequence pairs removed because at least one read was shorter than the length cutoff (36 bp): 6672 (0.65%)\n\n\n\nMuch of the above information is also saved in the output dir in files that end in *_trimming_report.txt.\nThe main output files, however, are a new pair of FASTQ files with trimmed reads — we will use those FASTQ files for the next step (assembly). Let’s take a look:\nls -lh results/trimgalore\ntotal 342M\n-rw-r--r-- 1 jelmer PAS0471 5.2K Feb  4 16:48 SM04_R1.fastq.gz_trimming_report.txt\n-rw-r--r-- 1 jelmer PAS0471 169M Feb  4 16:52 SM04_R1_val_1.fq.gz\n-rw-r--r-- 1 jelmer PAS0471 5.2K Feb  4 16:52 SM04_R2.fastq.gz_trimming_report.txt\n-rw-r--r-- 1 jelmer PAS0471 174M Feb  4 16:52 SM04_R2_val_2.fq.gz\nAlso, we should have FastQC output files that will be good to check:\nls -lh results/fastqc_posttrim\ntotal 2.4M\n-rw-r--r-- 1 jelmer PAS0471 705K Feb  4 17:00 SM04_R1_val_1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 437K Feb  4 17:00 SM04_R1_val_1_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 724K Feb  4 17:01 SM04_R2_val_2_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 470K Feb  4 17:01 SM04_R2_val_2_fastqc.zip\n\n Exercise: Check the trimming results with FastQC\n\nCheck the FastQC output HTML to see if adapters were removed, and if the overall quality looks better."
  },
  {
    "objectID": "05_preprocess.html#appendix",
    "href": "05_preprocess.html#appendix",
    "title": "Read QC & preprocessing",
    "section": "7 Appendix",
    "text": "7 Appendix\n\n7.1 zcat and related commands\ncat has a counterpart that prints the uncompressed contents of a compressed file: zcat.\nOf course, cat’s behavior of printing all the contents of a file isn’t great for large FASTQ files. But zcat is nevertheless very useful for FASTQ files when we combine it with the pipe, |.\nFor example, we can pipe the output of zcat to the head command to view the first 8 lines (2 reads) as follows:\nzcat data/fastq/SM04_R1.fastq.gz | head -n 8\n@M06129:165:000000000-K9L4J:1:1101:13571:1001 1:N:0:24\nNTGCTGACATCCACTATTGAGCCCATCAGGAACCGAGCATGTCAGACGCTATTCACTCCTACGAGCCATCCAAGGGTCACGGCCTGCCGCACGACCCGTTCAACGCGATTGTAGGCCCACGCCCGATTGGCTGGATTTCGTCGCAGGATGCCAACGGCAAACTCAACCTGGCACCTTACAGCTTCTTCAACGCGTTCAACTACGTGCCGCCGATCATCGGCTTCTGCAGCGTGGGTCGCAAAGACAGCCTCAATAATATCGAGCAAACCGGCCAGTTCGTCTGGAACCTGGCGACCCGCC\n+\n#8ACCGGGFGGGGGGGGDGGGGEGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGEGGGGGGGGGGGGGGGGGGCFGGGGGGGGGGGF:FDFGD@FFGGGGGGGDG=FDGFF=FGGFFGCFFGFGGGEGGGGCAFGGGGGDGGEEEFFGGGFGGD&gt;8DFGGGGGGGFGFGGC55CCEEFGGGG?&lt;FGGGGGGGGDGGGFC&lt;D7;FGFG?F?EDFF:&gt;55;&gt;;B@(06::?(37?&gt;FA46)4.,(,4&lt;??B(1((-1,640-,.(34,(-(.(14,(3((((\n@M06129:165:000000000-K9L4J:1:1101:9227:1001 1:N:0:24\nNGAAACCTATGGTTTCGTCAATCGCCTTGCAGACGGTCGCTACATGCTCGCGAGCGAAGTCATGCGCCTCAATGCGGTCTATCAGGATGCCCTGGATCTTGAGCGGCATGTGTTGCCAAGGTTGCACAAGCTGACCGGAGAAACCGGCGAGACCGCCTCTTTTTATGTGAAACACGGTGCCTATCGGTTGTGTCAGTACCGGGTCAACTCTCCTCACCGTCTGCGATTGCACATGCAGCCTGGAGACATGCGCCCCATGGACGGTGCGGCGAGTGCCGAAGCATTGCGTACACCGTATGC\n+\n#8BCCGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGFGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGGGGGCGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGDEFGGGGDGGGGGGGGGGGFFGBGFFFFFFFFFFFFFFFF&lt;FFFFF&lt;&gt;9EFFFFFBBF?BFFBF9;61364,6A9;B&gt;BF&lt;A?&lt;&lt;9(((.:;?::?F\nAlong the same lines, we can count the number of lines in a FASTQ file like so:\nzcat data/fastq/SM04_R1.fastq.gz | wc -l\n4124516\n\n\n\n\n\n\nMore about the Unix pipe, | (Click to expand)\n\n\n\n\n\nThe | (pipe) takes the output of one command and passes it on (“pipes”) as input for a subsequent command.\nThe most common way to use many shell commands that take input (which is most of them) is to pass that input as a file name — below, wc -l will operate on the FASTA file:\nwc -l data/fastq/SM04_R1.fastq.gz \nHowever, wc -l (and almost any other command) will also operate on input that is passed to it though a pipe, so the above and below are completely equivalent:\ncat data/fastq/SM04_R1.fastq.gz | wc -l\nThe pipe is incredibly useful because it avoids the need write/read intermediate files — this saves typing and also makes the operation much quicker.\n\n\n\n\n\n Exercise : The number of reads in a FASTQ file\n\nGiven the output of the command above, how many reads are in this FASTQ file?\n\n\n\nSolution\n\nThere are 4,124,516 / 4 = 1,031,129 reads in this file, so a little more than a million.\n\n\nWhat line count do you get when you run wc -l directly on the compressed file?\n\n\n\nSolution\n\nYou’ll get a line count of 880,721, quite a ways off from the number of lines in the uncompressed file! So, don’t do this when you want to count lines/reads!\nwc -l data/fastq/SM04_R1.fastq.gz \n880721 data/fastq/SM04_R1.fastq.gz"
  },
  {
    "objectID": "05_preprocess.html#footnotes",
    "href": "05_preprocess.html#footnotes",
    "title": "Read QC & preprocessing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a full list of installed software at OSC: https://www.osc.edu/resources/available_software/software_list↩︎\n Attribution: Some of the FastQC example plots were taken from here.↩︎\n I will specify the dir results/multiqc_fastqc, indicating that this is a MultiQC run that summarizes FastQC output↩︎\nOnce again, the default output dir is the current working dir which is not convenient↩︎"
  },
  {
    "objectID": "01_osc.html#a-computational-infrastructure-for-genomics",
    "href": "01_osc.html#a-computational-infrastructure-for-genomics",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 A computational infrastructure for genomics",
    "text": "1 A computational infrastructure for genomics\nDue in large part to the amount of data involved, a laptop or desktop computer is often not sufficient to work with large-scale genomics data.\nAdditionally, many of the specialized programs that help you analyze your data can only be run through a “command-line interface”.\nThose are some of the reasons that a typical computational infrastructure to do what we may call “command-line genomics” involves:\n\nA supercomputer1 — in our case, the Ohio Supercomputer Center (OSC) [this session]\nA text editor — I recommend and will demonstrate VS Code [next session]\nThe Unix shell (terminal) [third session]\nR (or perhaps Python) for interactive statistical analysis and visualization [this afternoon]\n\nThis session will provide an introduction to supercomputers in general and to the Ohio Supercomputer Center (OSC) specifically. In all of today’s and tomorrow’s sessions at this workshop, we’ll continue to work at OSC, so you will get a fair bit of experience with working at a supercomputer."
  },
  {
    "objectID": "01_osc.html#high-performance-computing",
    "href": "01_osc.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Owens, one of the OSC supercomputers, physically looks like:\n\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of CPUs, or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs.\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you’re working with genomics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio in the US. It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\n\n\n\n\n\n\nOSC websites and “projects” (Click to expand)\n\n\n\n\n\nOSC has three main websites — we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage.\n\n\nAccess to OSC’s computing power and storage space goes through OSC “Projects”:\n\nA project can be tied to a research project or lab, or be educational like the project PAS2250 you have been added to.\nEach project has a budget in terms of “compute hours” and storage space2.\nAs a user, it’s possible to be a member of multiple different projects."
  },
  {
    "objectID": "01_osc.html#the-structure-of-a-supercomputer-center",
    "href": "01_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet’s start with some terminology, going from smaller things to bigger things:\n\nCore / Processor / CPU / Thread\nComponents of a computer (node) that can each (semi-)indendepently be asked to perform a computing task like running a bioinformatics program. For our purposes, we can treat these terms as synonyms.\nNode\nA single computer that is a part of a supercomputer and has dozens of cores3.\nSupercomputer / Cluster\nA collection of computers connected by a high-speed network. OSC has two: “Pitzer” and “Owens”.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two clusters!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\nLet’s take those in order.\n\n\nFile systems\nOSC has several distinct file systems — we will only see the following two:\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\n\nIn today’s and tomorrow’s sessions, we will be working in the scratch directory of the OSC Project PAS2250: /fs/scratch/PAS22504.\n\n\n\n\n\n\nDirectory is just another word for folder, and written as “dir” for short\n\n\n\n\n\n\n\n\n\nLogin Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, and they are shared among everyone and cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing, which should be done on the compute nodes.\n\n\n\nCompute Nodes\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a “job”). A job scheduler5 will then assign resources to your request.\n\n\n\n\n\n\nInteractive and batch use of compute nodes\n\n\n\nRequests for compute node jobs can be made through the OnDemand website or with commands like srun and sbatch.\nJobs can either be interactive (like running Rstudio or interactive shell jobs) or be a “batch” job (sending a script away to be run on a compute node). Only with interactive jobs do you “move” to a compute node yourself.\n\n\nCompute nodes come in different shapes and sizes. You mostly don’t have to worry about this but sometimes non-standard nodes are needed, such as when you need a lot of RAM memory or need GPUs6."
  },
  {
    "objectID": "01_osc.html#osc-ondemand",
    "href": "01_osc.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n“Interactive Apps”: programs such as RStudio, Jupyter, VS Code and QGIS.\n\n Go to https://ondemand.osc.edu and log in (use the box on the left-hand side)\nYou should see a landing page similar to the one below:\n\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files: File system access\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2250, you should only have two directories listed7:\n\nA Home directory (starts with /users/)\nA project “scratch” directory (starts with /fs/scratch/) PAS2250\n\n Click on the directory /fs/scratch/PAS2250, where we’ll be working today and next week:\n\n\n\n\n\nOnce there, you should see whichever directories and files are present at the selected location (here: just two dirs, cabana and ENT6703), and you can click on the directories to explore the contents further:\n\n\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files8 — see the buttons across the top.\n\n Exercise: Create your own folder\n\nClick your way into cabana within /fs/scratch/PAS2250 if you’re not already there.\nYou should see a number of directories, e.g. homework, jelmer and mizarra.\nCreate your own by clicking the New Directory button towards the top.\nPlease give it the exact same name as your OSC username (also match the capitalization!).\n\n(If you’re not sure what your username is — look at the right side of the blue top bar, “Logged in as”:)\n\n\n\n\n\n\n\n\n\n4.2 Clusters: Unix shell access\n\n\n\n\n\n\nSide note: System Status within Clusters (Click to expand)\n\n\n\n\n\nIn the “Clusters” dropdown menu, click on the item at the bottom, “System Status”:\n\n\n\n\n\nThis page shows an overview of the live, current usage of the two clusters — that can be interesting to get a good idea of the scale of the supercomputer center, which cluster is being used more, what the size of the “queue” (which has jobs waiting to start) is, and so on.\n\n\n\n\n\n\n\n\nInteracting with a supercomputer is most commonly done using a Unix shell. Under the Clusters dropdown menu, you can access a Unix shell either on Owens or Pitzer:\n\n\n\n\n\nI’m selecting a shell on the Pitzer supercomputer (“Pitzer Shell Access”), which will open a new browser tab, where the bottom of the page looks like this:\n\n\n\n\n\nHowever, from now on, we’ll be accessing a Unix shell inside the VS Code text editor, which also gives us some additional functionality in a user-friendly way.\n\n\n\n4.3 Interactive Apps\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu — let’s select VS Code using the “Code Server” button:\n\n\n\n\n\nInteractive Apps like VS Code and RStudio run on compute nodes (not login nodes). Because compute nodes always need to be “reserved”, we have to fill out a form and specify the following details:\n\n\n\n\n\n\n\nOption\nValue\n\n\n\n\nThe OSC Project that should be billed for the compute resource usage\nPAS2250\n\n\nThe Number of hours we want to make a reservation for9\n10\n\n\nThe Working Directory10 for the program\nyour newly-created personal folder in /fs/scratch/PAS2250/cabana (e.g. /fs/scratch/PAS2250/cabana/jelmer)\n\n\nThe Codeserver Version\n4.8\n\n\n\n\n\n\n\n\nClick on Launch at the bottom, which will send your request to the “compute job” scheduler.\nFirst, your job will be “Queued” — that is, waiting for the job scheduler to allocate compute node resources to it:\n\n\n\n\n\nYour job is typically granted resources within a few seconds (the card will then say “Starting”), and should be ready for usage (“Running”) in another couple of seconds:\n\n\n\n\n\nOnce it appears, click on the blue Connect to VS Code button to open VS Code in a new browser tab.\nWhen VS Code opens, you may get these two pop-ups (and possibly some others) — click “Yes” (and check the box) and “Don’t Show Again”, respectively:"
  },
  {
    "objectID": "01_osc.html#further-reading",
    "href": "01_osc.html#further-reading",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Further reading",
    "text": "Further reading\n\n\n\n\n\n\nWhat works differently on a supercomputer like at OSC?\n\n\n\nCompared to command-line computing on a laptop or desktop, the following aspects are different when working on a supercomputer like at OSC:\n\nLogin versus compute nodes\n“Login nodes”, the nodes you end up on after logging in, are not meant for heavy computing and you have to request access to “compute nodes” to run most analyses.\n“Non-interactive” computing is common\nIt is common to write and “submit” scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can’t install “the regular way”, and a lot of installed software needs to be “loaded” (as we’ll see today).\nOperating system\nSupercomputers run on the Linux operating system\n\n\n\n\nOSC’s learning resources\n\nAn extended version of this introduction\nOSC’s online asynchronous courses\nOSC’s new User Resource Guide 11"
  },
  {
    "objectID": "01_osc.html#footnotes",
    "href": "01_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCloud computing is an alternative, but won’t be covered here.↩︎\nBut we don’t have to pay anthing for educational projects like this one. Otherwise, if you’re interested in OSC’s rates for academic research, see this page.↩︎\nI.e., these nodes tend to be more powerful than a personal laptop or desktop↩︎\nIf you’d be doing research on OSC, though, you would mostly interact with a third kind of file system: the Project directories: this is because for most files, you’ll want a permanent and backed-up location (i.e., not Scratch or Compute storage), and the Home directory offers relatively limited storage as well as challenges with file sharing.↩︎\nOSC uses the Slurm job scheduler↩︎\nGPUs are e.g. used for Nanopore basecalling↩︎\nIf you had been added to another project than PAS2250,you would have had at least 3: PAS2250 does not have a “project dir”, but most projects do.↩︎\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available for those but are outside the scope of this introduction↩︎\nNote that we’ll be kicked off as soon as that amount of time has passed!↩︎\nThis will be your starting location in the file system, we’ll talk more about working dirs in a little bit.↩︎\n Attribution: This page uses material from an OSC Introduction written by Mike Sovic and from OSC’s Kate Cahill Software Carpentry introduction to OSC.↩︎"
  },
  {
    "objectID": "02_vscode.html#what-is-vs-code",
    "href": "02_vscode.html#what-is-vs-code",
    "title": "The VS Code text editor",
    "section": "1 What is VS Code?",
    "text": "1 What is VS Code?\nVS Code is basically a fancy text editor. Its full name is Visual Studio Code, but at OSC, it is also referred to as “Code Server”.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. The RStudio program is another good example of an IDE. For our purposes:\n\nVS code will be our IDE for Unix shell code\nRStudio will be our IDE for R"
  },
  {
    "objectID": "02_vscode.html#connecting-to-vs-code",
    "href": "02_vscode.html#connecting-to-vs-code",
    "title": "The VS Code text editor",
    "section": "2 Connecting to VS Code",
    "text": "2 Connecting to VS Code\nWe saw on the previous page (link) how to start a VS Code session at OSC, and you should have one active now.\n\n\n\n\n\n\nStarting VS Code at OSC - Instructions in brief (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2250\nThe starting directory: /fs/scratch/PAS2250/cabana/&lt;your_personal_dir&gt;\nNumber of hours: 10\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code."
  },
  {
    "objectID": "02_vscode.html#the-vs-code-user-interface",
    "href": "02_vscode.html#the-vs-code-user-interface",
    "title": "The VS Code text editor",
    "section": "3 The VS Code User Interface",
    "text": "3 The VS Code User Interface\n\n\n\n\n\n\nSide bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA  (“hamburger menu”), which has menu items like File that you often find in a top bar.\nA  (cog wheel icon) in the bottom, through which you can mainly access settings.\nIcons to toggle (wide) Side Bar options — but we’ll only use the default selection, the Explorer (file browser)\n\n\n\nEditor pane and Welcome document\nThe main part of the VS Code is the editor pane. Here, we can open files like scripts and other types of text files, and images. (Whenever you open VS Code, an editor tab with a Welcome document is automatically opened. This provides some help and some shortcuts like to recently opened files and folders.)\n\n\nTerminal (with a Unix shell)\n Open a terminal by clicking      =&gt; Terminal =&gt; New Terminal.\n\n\n\n Exercise: Try a few color themes\n\nAccess the “Color Themes” option by clicking  =&gt; Color Theme.\nTry out a few themes and see pick one you like!"
  },
  {
    "objectID": "02_vscode.html#a-folder-as-a-starting-point",
    "href": "02_vscode.html#a-folder-as-a-starting-point",
    "title": "The VS Code text editor",
    "section": "4 A folder as a starting point",
    "text": "4 A folder as a starting point\nConveniently, VS Code takes a specific directory as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\n(If you need to switch folders, click      =&gt;   File   =&gt;   Open Folder.)\n\n\n\n\n\n\n\nSome VS Code tips and tricks\n\n\n\n\nResizing panes\nYou can resize panes (the terminal, editor, and side bar) by hovering your cursor over the borders and then dragging.\nThe Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for. To access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P).\nKeyboard shortcuts\nFor a single-page PDF overview of keyboard shortcuts for your operating system:      =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.) A couple of useful keyboard shortcuts are highlighted below.\n\n\n\n\n\n\n\n\n\nSpecific useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nWorking with keyboard shortcuts for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, in some case, you’ll have to replace Ctrl with ⌘):\n\nOpen a terminal: Ctrl+` (backtick) or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nToggle the (wide) Side Bar: Ctrl+B\nLine actions:\n\nCtrl+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl+Shift+K will delete a line\nAlt+⬆/⬇ will move lines up or down."
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "R and Unix Shell Homework",
    "section": "",
    "text": "If you have very limited or no experience with R and/or the Unix shell, please go through the following homework before you need these skills in the workshop on Wednesday.\nIf you do have some experience with R and the Unix shell, we would suggest that you at least take a look at the material below, to check that you are familiar with most or all of its contents."
  },
  {
    "objectID": "homework.html#r",
    "href": "homework.html#r",
    "title": "R and Unix Shell Homework",
    "section": "R",
    "text": "R\nPlease work your way through the first part of the Data Carpentry “Data Analysis and Visualisation in R for Ecologists” lesson. Specifically, go through:\n\nThe introductory page\nThe first lesson (“Before we start”)\nThe second lesson (“Introduction to R”)\nThe third lesson (“Starting with data”) — until (not including) the “Formatting dates” section.\n\n\n\n\n\n\n\n\nHaving trouble installing R or RStudio?\n\n\n\nYou should be able to install R and RStudio regardless of your operating system (Windows, Mac, or Linux), and the Carpentry lesson contain instructions on how to do so.\nBut in case you are having trouble, you can also use R and RStudio through your browser with the Ohio Supercomputer Center (OSC). To start an RStudio session at OSC, follow these instructions elsewhere on this website."
  },
  {
    "objectID": "homework.html#unix-shell",
    "href": "homework.html#unix-shell",
    "title": "R and Unix Shell Homework",
    "section": "Unix shell",
    "text": "Unix shell\nPlease work your way through the Software Carpentry “Shell Novice” lesson up until and including at least episode 3 (“Working With Files and Directories”).\nYou can optionally make your way through the remaining episodes, but that content will in part be taught in the workshop itself, and in part not be used at all.\n\n\n\n\n\n\n\nUsing Windows?\n\n\n\nIf your computer uses the Windows operating system, you will not have a Unix shell installed by default1. To go through the lesson above, you have two options:\n\nInstall a Unix shell\nFollow the instructions in the Software Carpentry lesson that I linked to above (and here is a direct link to the installation instructions). If this fails for you, please use the next option instead.\nUse a Unix shell at the Ohio Supercomputer Center (OSC)\nFollow these instructions elsewhere on this website."
  },
  {
    "objectID": "homework.html#footnotes",
    "href": "homework.html#footnotes",
    "title": "R and Unix Shell Homework",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you have a Mac or Linux computer, you should have a Unix shell installed.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Feb ’24 CABANAnet/STC-CGIAR workshop",
    "section": "",
    "text": "Day\nTime\nInstructor\nTopic and link\n\n\n\n\nWed Feb 7th\n08:15-09:00\nJelmer\nIntro to supercomputing / OSC\n\n\n\n09:00-09:30\nJelmer\nVS Code\n\n\n\n09:30-12:00\nJelmer\nUnix shell refresher (with coffee break)\n\n\n\n13:30-17:00\nMyriam\nAmplicon sequencing to characterize P. infestans populations\n\n\n\n\n\n\n\n\nThu Feb 8th\n08:15-09:15\nMyriam\nAmplicon sequencing: R part\n\n\n\n09:15-09:45\nJelmer\nBacterial whole-genome sequencing (slides)\n\n\n\n10:00-12:00\nJelmer\nRead QC & preprocessing\n\n\n\n13:30-15:40\nJelmer\nGenome assembly and assembly QC\n\n\n\n16:00-17:00\nJelmer\nGenome annotation\n\n\n\n\n\n\n\n\n\n\n\nOther pages on this website\n\n\n\nOpening a Unix shell at OSC\nStarting an RStudio session at OSC\nHomework\n\n\n\nUnix shell overview/reference\nContinued Unix shell intro\nSoftware management\nData management\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "03_shell.html#introduction",
    "href": "03_shell.html#introduction",
    "title": "Unix Shell refresher",
    "section": "Introduction",
    "text": "Introduction\nMany of the things you typically do by pointing and clicking can alternatively be done by typing commands. The Unix shell allows you to interact with computers via commands.\nWorking effectively on a remote supercomputer tends to require using a command line interface. But there are more reasons to command line computing, such as:\n\nWorking efficiently with large files\nMaking it easier to repeat (& automate) similar tasks across files, samples, and projects\nAchieving better reproducibility in research\nAt least in bioinformatics, being able to use access the largest and most recent set of approaches and all their options — many graphical user interface programs lag behind in functionality and may cost money as well.\n\n\n\n\n\n\n\nSide note: Some Unix shell terminology (Click to expand)\n\n\n\n\n\nWe’re going to focus on the practice of doing command line computing, and not get too bogged down in terminology, but let’s highlight a few interrelated terms you’re likely to run across:\n\nCommand Line — the most general term, an interface1 where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the types of shells on Unix family (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\n\nWhile it might not fly for a computer science class, for day-to-day computing/bioinformatics, you’ll probably hear all these terms used somewhat interchangeably.\n\n\n\n\n\nStarting a VS Code session in OSC OnDemand\nWe’ll use a Unix shell at OSC inside VS Code. You should have an active VS Code session in the folder /fs/scratch/PAS2250/cabana/&lt;user&gt;, and with an open Terminal — if not, see the instructions right below.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2250\nThe starting directory: /fs/scratch/PAS2250/cabana/&lt;user&gt;\nNumber of hours: 10\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nType pwd to check where you are. If you are not in /fs/scratch/PAS2250/cabana/&lt;user&gt; (where &lt;user&gt; is your OSC username), click Open folder... in the Welcome tab, or      =&gt;   File   =&gt;   Open Folder, then type/select /fs/scratch/PAS2250/cabana/&lt;user&gt; and press OK."
  },
  {
    "objectID": "03_shell.html#recap-of-the-basics",
    "href": "03_shell.html#recap-of-the-basics",
    "title": "Unix Shell refresher",
    "section": "1 Recap of the basics",
    "text": "1 Recap of the basics\n\n1.1 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example:\n[jelmer@p0080 jelmer]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n1.2 A few simple commands: date, whoami, pwd\nThe Unix shell comes with hundreds of “commands”: small programs that perform specific actions. If you’re familiar with R or Python, a Unix command is like an R/Python function.\nLet’s start with a few simple commands:\n\nThe date command prints the current date and time:\n\ndate\nWed Feb 7 09:11:51 EST 2024\n\nThe whoami (who-am-i) command prints your username:\n\nwhoami\njelmer\n\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\n\npwd\n/fs/scratch/PAS2250/cabana/jelmer\nAll 3 of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n\n\n\nWorking directory and paths (we’ll talk more about paths later)\n\n\n\n\nWhen working in a Unix shell, you are always “in” a specific directory: your working directory (“working dir” for short).\nIn a path (= location of a file or directory) such as that output by pwd, directories are separated by forward slashes /.\n\n\n\n\n\n\n\n\n\nCase and spaces\n\n\n\n\nEverything in the shell is case-sensitive, including commands and file names.\nAvoid spaces in file and directory names!2 Use e.g. underscores to distinguish words (my_long_filename).\n\n\n\n\n\n\n1.3 cd and command actions & arguments\nIn the above three command line expressions:\n\nWe merely typed a command and nothing else\nThe command provided some information, which was printed to screen\n\nBut many commands perform an action other than providing information. For example, you can use the command cd to Change Directory (i.e. change your working dir). And like many commands that perform an action, cd normally has no output at all.\nLet’s use cd to move to another directory by specifying the path to that directory after the cd command:\ncd /fs/scratch/PAS2250/cabana/bact\npwd\n/fs/scratch/PAS2250/cabana/bact\n\n\n\n\n\n\nI will demonstrate “tab completion”!\n\n\n\n\n\n\nIn more abstract terms, what we did above was to provide cd with an argument, namely the path of the dir to move to. Arguments generally tell commands what file(s) or directory/ies to operate on.\nAs we’ve seen, then, cd gives no output when it succesfully changed the working directory. But let’s also see what happens when it does not succeed — it gives an error:\ncd /fs/scratch/pas2250\nbash: cd: /fs/scratch/pas2250: No such file or directory\n\n\nWhat was the problem with the path we specified? (Click to see the answer)\n\nWe used lowercase in /pas2250/ — this should have been /PAS2250/.\nAs pointed out above, everything, including paths, is case-sensitive in the Unix shell!\n\n\n\n\n1.4 ls and command options\n\nThe default behavior of ls\nThe ls command, short for “list”, will list files and directories:\nls\ndata  README.md\n(You should still be in /fs/scratch/PAS2250/cabana/bact. If not, cd there first.)\n\n\n\n\n\n\nSide note: ls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell — the most common ones are:\n\nEntries in blue are directories (like data and metadata above)\nEntries in black are regular files (like README.md above)\nEntries in red are compressed files (we’ll see an example soon).\n\n\n\n\nBy default, ls will list files and dirs in your current working dir, and in the way shown above. For which dir ls lists files and dirs can be changed with arguments, and how ls shows the output can be changed with options.\n\n\nIntermezzo: cat and a quick intro to the data\nTo find out what data is contained in this dir, let’s take a look at the README.md file.\nThere are several commands to view the contents of files — the simplest is cat, which will print the entire contents of a file to screen:\ncat README.md\n# README for /fs/scratch/PAS2250/cabana/bact\n\nThis directory contains:\n\n- Illumina FASTQ files for 16 _Pseudomonas syringae pv. syringae_ samples (`data/fastq`)\n- 10 _Pseudomonas_ genome assemblies downloaded from NCBI (`data/ref`)\n- Metadata on both the focal samples and the downloaded genomes (`data/meta`)\nThis is the dataset we will work with (mostly) tomorrow for our bacterial whole-genome analysis sessions.\n\n\n\nOptions\nIn general, whereas arguments tell a command what to operate on, options will modify its behavior. For example, we can call ls with the option -l (a dash followed by a lowercase L):\nls -l \ntotal 17\ndrwxr-xr-x 5 jelmer PAS2250 4096 Feb  4 11:46 data\n-rw-r--r-- 1 jelmer PAS2250  318 Feb  6 08:57 README.md\nNotice that it lists the same items as above, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet’s add another option, -h:\nls -l -h\ntotal 17K\ndrwxr-xr-x 5 jelmer PAS2250 4.0K Feb  4 11:46 data\n-rw-r--r-- 1 jelmer PAS2250  318 Feb  6 08:57 README.md\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have “Human-readable filesizes” (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be really useful especially for very large files.\n\nConveniently, options can be “pasted together” as follows:\nls -lh\n\n\n\nArguments\nArguments to ls should be dirs or files to operate on. For example, if we wanted to see what’s inside the data dir, instead of inside our working dir, we could type3:\nls data\nfastq  meta  ref\nThe data dir appears to contain three (sub)dirs with different kinds of data. We’ll talk in detail about that later, but for now let’s look inside the fastq dir:\nls data/fastq\nSM04_R1.fastq.gz    SM1031_R1.fastq.gz  SM1042_R1.fastq.gz  SM155_R1.fastq.gz  SM181_R1.fastq.gz  SM191_R1.fastq.gz  SM207_R1.fastq.gz  SM51_R1.fastq.gz\nSM04_R2.fastq.gz    SM1031_R2.fastq.gz  SM1042_R2.fastq.gz  SM155_R2.fastq.gz  SM181_R2.fastq.gz  SM191_R2.fastq.gz  SM207_R2.fastq.gz  SM51_R2.fastq.gz\nSM1030_R1.fastq.gz  SM1038_R1.fastq.gz  SM109_R1.fastq.gz   SM156_R1.fastq.gz  SM190_R1.fastq.gz  SM205_R1.fastq.gz  SM226_R1.fastq.gz  SM914_R1.fastq.gz\nSM1030_R2.fastq.gz  SM1038_R2.fastq.gz  SM109_R2.fastq.gz   SM156_R2.fastq.gz  SM190_R2.fastq.gz  SM205_R2.fastq.gz  SM226_R2.fastq.gz  SM914_R2.fastq.gz\nAh, FASTQ files! These contain our sequence data, and we’ll go and explore them in a bit.\n\n\n\nCombining options and arguments\nWe’ll combine options and arguments to take a closer look at our dir with FASTQ files — now the -h option is especially useful and allows us to see that the FASTQ files are roughly around 200 Mb in size:\nls -lh data/fastq\ntotal 6.1G\n-rw-r--r-- 1 jelmer PAS2250 205M Feb  4 11:47 SM04_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 242M Feb  4 11:46 SM04_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 188M Feb  4 11:46 SM1030_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 221M Feb  4 11:46 SM1030_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 187M Feb  4 11:46 SM1031_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 221M Feb  4 11:46 SM1031_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 187M Feb  4 11:46 SM1038_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2250 224M Feb  4 11:46 SM1038_R2.fastq.gz\n# [...output truncated...]\n\n\n\n Exercise: Listing files\nList the files in the data/ref dir:\n\nWhat are the file sizes?\nDo you know what kind of files these are?\n\n\n\nClick for the solution\n\nls -lh data/ref\ntotal 60M\n-rw-r--r-- 1 jelmer PAS2250 6.4M Feb  4 11:46 GCA_000007805.fna\n-rw-r--r-- 1 jelmer PAS2250 6.0M Feb  4 11:46 GCA_000012205.fna\n-rw-r--r-- 1 jelmer PAS2250 5.9M Feb  4 11:46 GCA_000012245.fna\n-rw-r--r-- 1 jelmer PAS2250 5.7M Feb  4 11:46 GCA_000145825.fna\n-rw-r--r-- 1 jelmer PAS2250 5.6M Feb  4 11:46 GCA_000156995.fna\n-rw-r--r-- 1 jelmer PAS2250 5.8M Feb  4 11:46 GCA_000988395.fna\n-rw-r--r-- 1 jelmer PAS2250 5.9M Feb  4 11:46 GCA_000988485.fna\n-rw-r--r-- 1 jelmer PAS2250 6.5M Feb  4 11:46 GCA_002763655.fna\n-rw-r--r-- 1 jelmer PAS2250 6.1M Feb  4 11:46 GCA_018603495.fna\n-rw-r--r-- 1 jelmer PAS2250 6.0M Feb  4 11:46 GCA_023277945.fna\n\nThe files are round 5-6 Mb in size.\nThese are FASTA files with nucleotide sequences (hence the extension .fna).\n(More specifically, they are genome assembly sequences downloaded from NCBI: the GCA_ prefix indicates that they are from NCBI’s GenBank.)\n\n\n\n\n\n\n1.5 Miscellaneous tips\n\nCommand history: If you hit the ⇧ (up arrow) once, you’ll retrieve your most recent command, and if you keep hitting it, you’ll go further back. The ⇩ (down arrow) will go the other way: towards the present.\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\nAny text that comes after a # is considered a comment instead of code!\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/fs/scratch/PAS2250/cabana/bact\n\n\n\nIf your prompt is “missing”, the shell is still busy executing your command, or you typed an incomplete command. To abort in either of these scenarios, press Ctrl+C and you will get your prompt back.\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nOr, use Ctrl + C after running this example of an incomplete command (an opening parenthesis ():\n(\n\n\n\n Exercise: Changing dirs\nA) Move into your personal dir in /fs/scratch/PAS2250/cabana, and then back into the bact dir.\n\n\n(Click for the solution)\n\ncd /fs/scratch/PAS2250/cabana/jelmer\ncd /fs/scratch/PAS2250/cabana/bact\n\nB) Use the command history (up arrow) to repeat the previous exercise without retyping your commands."
  },
  {
    "objectID": "03_shell.html#more-key-commands-and-concepts",
    "href": "03_shell.html#more-key-commands-and-concepts",
    "title": "Unix Shell refresher",
    "section": "2 More key commands and concepts",
    "text": "2 More key commands and concepts\n\n2.1 Paths\n\nAbsolute (full) paths versus relative paths\n\nPaths with a leading / begin from the computer’s root directory, and are called “absolute” or “full paths”.\n(They are equivalent to GPS coordinates for a geographical location, as they work regardless of where you are).\nPaths without a leading / begin from your current working directory, and are called “relative paths”.\n(These work like directions along the lines of “take the second left:” they depend on your current location.)\n\n# Move into the 'cabana' dir with an absolute path:\ncd /fs/ess/PAS2250/cabana\n\n# Then, move into the 'share' dir with a relative path:\ncd bact                   # Absolute path is /fs/ess/PAS2250/cabana/bact\n\n\nPath shortcuts\n\n~ (a tilde)\nThis is a shortcut for your Home directory — for example, cd ~ would move you to your Home dir.\n. (a single period)\nThis is a shortcut representing the current working directory. Therefore, ls ./data is functionally the same as ls data.\n.. (two periods)\nThis means the directory “one level up” (towards the computer’s root dir). Use .. to go up in the dir hierarchy in a relative path:\n\nls ..               # One level up, listing /fs/scratch/PAS2250/cabana\nbact          databases  jelmer       mizarra       software\nbact_results  homework   jelmer_prep  Phytophthora\n# (Note: you will see more dirs than this!)\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\nls ../..            # Two levels up, listing /fs/scratch/PAS2250/cabana\ncabana  ENT6703\n\n\n\n\n\n\nThese shortcuts work with all commands\n\n\n\nAll of the above shortcuts (., .., ~) are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise: Path shortcuts\n\nA) Use relative paths with .. to move into your personal dir and back to bact once again.\n\n\n\n(Click for the solution)\n\ncd ../jelmer\ncd ../bact\n\n\nB) List the files in your Home dir without moving there.\n\n\n\n(Click for the solution)\n\nls ~\n\n\n\n\n\n2.2 Environment variables\nYou are likely familiar with the concept of variables in either the Unix shell, R, or another language.\n\nAssigning and printing the value of a variable in R:\n\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\n\nAssigning and printing the value of a variable in the Unix shell:\n\nx=5\necho $x\n5\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need the echo command, a general command to print text, to print the value of $x (cf. in R).\nYou need a $ prefix to reference (but not to assign) variables in the shell4.\n\n\n\nEnvironment variables are pre-existing variables that have been assigned values automatically. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nA) Use the environment variable $USER to list the contents of your personal dir in /fs/ess/PAS2250/cabana. What do you expect to see printed?\n\n\n\n\n\n\nIf you get an error, make sure to check the solution!\n\n\n\n\n\n\n\n\nClick to see the solution\n\nls /fs/ess/PAS2250/cabana/$USER\n \nYou should not get any output, since you didn’t create or copy any files into your personal dir yet.\nHowever, if you get an error along these lines…\nbash: cd: /fs/ess/PAS2250/cabana/jelmer: No such file or directory\n…then the dir you created earlier in the OnDemand’s Files menu does not (exactly) match your user name!\nYou can try to rename the folder with the mv command or do so in VS Code’s file explorer on the left-hand side. Alternatively, you can just (also) create the correct dir as follows:\nmkdir /fs/ess/PAS2250/cabana/$USER\nAnd then you can optionally find and remove your other, misnamed dir (you can remove it in the OnDemand File Browser or with the rmdir command).\n\n\nB) Print “Hello there &lt;your username&gt;” (e.g. “Hello there marcus”) to the screen:\n\n\nClick to see the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello there $USER\"\nHello there jelmer\n\n\n\n\n\n2.3 Create dirs with mkdir\nThe mkdir command creates new directories. For example, to create a bact dir in your personal dir (for tomorrow’s bacterial whole-genome analyses):\ncd /fs/scratch/PAS2250/cabana/$USER\n\nmkdir bact\nOr two directories at once:\nmkdir bact/scripts bact/workflow\nLet’s check what we did:\nls\nbact\nls bact\nscripts workflow\nOr get a nice overview with the tree command:\ntree\n.\n└── bact\n    ├── scripts\n    └── workflow\n\n3 directories, 0 files\n\n\n\n\n\n\nSide note: Recursive mkdir (Click to expand)\n\n\n\n\n\nBy default, mkdir does not work recursively: that is, it will refuse to make a dir inside a dir that does not yet exist. And if you try to do so, the resulting error might confuse you:\nmkdir bact/sandbox/20240207\nmkdir: cannot create directory ‘bact/sandbox/20240207’: No such file or directory\n\nWhy won’t you do your job, mkdir!? 😡\n\nInstead, we need to use the -p option to mkdir:\nmkdir -p bact/sandbox/20240207\nThe -p option also changes mkdir’s behavior when you try to create a dir that already exists. Without -p that will result in an error, and with -p it doesn’t complain about that (and it won’t recreate/overwrite the dir either).\n\n\n\n\n\n\n2.4 Copy files and dirs with cp\nAbove, you created your own directory — now, let’s get you a copy of the data we saw in the bact dir.\nThe cp command copies files and/or directories from one location to another. It has two required arguments: what you want to copy (the source), and where you want to copy it to (the destination). So, we can summarize its basic syntax as cp &lt;source&gt; &lt;destination&gt;.\ncp is not recursive by default, so if you want to copy a directory and all of its contents, we need to use its -r option We’ll use that option to copy the dir with FASTQ files:\ncp -rv ../bact bact\n‘../bact’ -&gt; ‘bact’\n‘../bact/README.md’ -&gt; ‘bact/README.md’\n‘../bact/data’ -&gt; ‘bact/data’\n‘../bact/data/meta’ -&gt; ‘bact/data/meta’\n‘../bact/data/meta/ncbi_asm_metadata.tsv’ -&gt; ‘bact/data/meta/ncbi_asm_metadata.tsv’\n‘../bact/data/meta/metadata.tsv’ -&gt; ‘bact/data/meta/metadata.tsv’\n‘../bact/data/ref’ -&gt; ‘bact/data/ref’\n‘../bact/data/ref/GCA_000145825.fna’ -&gt; ‘bact/data/ref/GCA_000145825.fna’\n‘../bact/data/ref/GCA_000156995.fna’ -&gt; ‘bact/data/ref/GCA_000156995.fna’\n# [...output truncated...]\n\n\n\n\n\n\nWe also used the -v option, short for verbose, to make cp tell us what it did\n\n\n\n\n\n\nLet’s get an overview with tree again:\ntree -C                 # '-C' for colors\n\n\n\n\n\n\n\n\n2.5 Globbing with shell wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing. Wildcards are symbols that have a special meaning.\nIn globbing, the * wildcard matches any number of any character, including nothing.\nThe example below will match any files that contain the string “_R1”:\nls data/fastq/*_R1*\ndata/fastq/SM04_R1.fastq.gz    data/fastq/SM1038_R1.fastq.gz  data/fastq/SM155_R1.fastq.gz  data/fastq/SM190_R1.fastq.gz  data/fastq/SM207_R1.fastq.gz  data/fastq/SM914_R1.fastq.gz\ndata/fastq/SM1030_R1.fastq.gz  data/fastq/SM1042_R1.fastq.gz  data/fastq/SM156_R1.fastq.gz  data/fastq/SM191_R1.fastq.gz  data/fastq/SM226_R1.fastq.gz\ndata/fastq/SM1031_R1.fastq.gz  data/fastq/SM109_R1.fastq.gz   data/fastq/SM181_R1.fastq.gz  data/fastq/SM205_R1.fastq.gz  data/fastq/SM51_R1.fastq.gz\nSome more file matching examples with * — if you would be in your data/fastq dir, then:\n\n\n\nPattern\nMatches files whose names…\n\n\n\n\n*\nContain anything (matches all files) 5\n\n\n*fastq.gz\nEnd in “.fastq.gz”\n\n\nSM1*\nStart with “SM1”\n\n\n*_R1*\nContain “_R1”\n\n\n\n\n\n Exercise: Practice with *\nWhat pattern would you use if you wanted to select FASTQ files for the 4 samples (8 files) whose IDs end in a 1 (e.g. SM1031)?\n\n\nClick here for the solutions\n\nWe’ll need a * on either side of our pattern, because the file names neither start not end with the pattern:\nls data/fastq/*1_R*\ndata/fastq/SM1031_R1.fastq.gz  data/fastq/SM181_R1.fastq.gz  data/fastq/SM191_R1.fastq.gz  data/fastq/SM51_R1.fastq.gz\ndata/fastq/SM1031_R2.fastq.gz  data/fastq/SM181_R2.fastq.gz  data/fastq/SM191_R2.fastq.gz  data/fastq/SM51_R2.fastq.gz\n\n\n\n\n\n2.6 For loops\nLoops are a universal element of programming languages, and are used to repeat operations. Here, we’ll only cover the most common type of loop: the for loop.\nA for loop iterates over a collection, such as a list of files, and allows you to perform one or more actions for each element in the collection. In the example below, our “collection” is just a short list of numbers (1, 2, and 3):\n\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\n\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\n\n\nThe indented lines between do and done contain the code that is being executed as many times as there are items in the collection: in this case 3 times, as you can tell from the output above.\n\n\n\n\n\n\nWhat was actually run under the hood is the following:\n\n\n\n# (Don't run this)\na_number=1\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=2\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=3\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\n\n\nCombining loops and globbing\nA very useful strategy is to loop over files with globbing, for example:\nfor fastq_file in data/fastq/*fastq.gz; do\n    echo \"Running an analysis for file $fastq_file\"...\n    # Additional commands to process the FASTQ file\ndone\nRunning an analysis for file data/fastq/SM04_R1.fastq.gz...\nRunning an analysis for file data/fastq/SM04_R2.fastq.gz...\nRunning an analysis for file data/fastq/SM1030_R1.fastq.gz...\nRunning an analysis for file data/fastq/SM1030_R2.fastq.gz...\nRunning an analysis for file data/fastq/SM1031_R1.fastq.gz...\n#[...output truncated...]\n\n\n\n\n\n\nA further explanation of for loop syntax (Click to expand)\n\n\n\n\n\nOn the first and last, unindented lines, for loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name (an arbitrary name; above we used a_number)\n\n\nin\nAfter in, we specify the collection (list of items) we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\n\n Exercise: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\nClick for the solution\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom"
  },
  {
    "objectID": "03_shell.html#further-reading",
    "href": "03_shell.html#further-reading",
    "title": "Unix Shell refresher",
    "section": "Further reading",
    "text": "Further reading\nSee the following two reference pages on this website:\n\nContinued introduction\nOverview of shell commands"
  },
  {
    "objectID": "03_shell.html#footnotes",
    "href": "03_shell.html#footnotes",
    "title": "Unix Shell refresher",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCommand-line Interface (CLI), as opposed to Graphical User Interface (GUI)↩︎\nIt’s certainly possible to have spaces in file names, but it’s a bad idea, and will get you into trouble sooner or later.↩︎\nBeginners will often cd into a dir just to list its contents, but the method shown below is much quicker.↩︎\nAnytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.↩︎\nHowever, it will not match so-called “hidden files” whose names start with a ..↩︎"
  },
  {
    "objectID": "07_annotation.html#introduction",
    "href": "07_annotation.html#introduction",
    "title": "Genome annotation",
    "section": "Introduction",
    "text": "Introduction\n\n Setting up\nYou should have an active VS Code session with an open terminal. In that terminal, you should be be in your dir /fs/scratch/PAS2250/cabana/$USER/bact/bact."
  },
  {
    "objectID": "07_annotation.html#the-gffgtf-format",
    "href": "07_annotation.html#the-gffgtf-format",
    "title": "Genome annotation",
    "section": "1 The GFF/GTF format",
    "text": "1 The GFF/GTF format\nThe GTF and GFF formats are very similar tab-delimited tabular files that contain genome annotations, with:\n\nOne row for each annotated “genomic feature” (gene, exon, etc.)\nOne column for each piece of information about a feature, like its genomic coordinates\n\nSee the sample below, with an added header line (not normally present) with column names:\nseqname     source  feature start   end     score  strand  frame    attributes\nNC_000001   RefSeq  gene    11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"\"; db_xref \"GeneID:100287102\"; db_xref \"HGNC:HGNC:37102\"; description \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; gbkey \"Gene\"; gene \"DDX11L1\"; gene_biotype \"transcribed_pseudogene\"; pseudo \"true\"; \nNC_000001   RefSeq  exon    11874   12227   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; \nSome details on the more important/interesting columns:\n\nseqname — Name of the chromosome, scaffold, or contig\nfeature — Name of the feature type, e.g. “gene”, “exon”, “intron”, “CDS”\nstart & end — Start & end position of the feature\nstrand — Whether the feature is on the + (forward) or - (reverse) strand\nattribute — A semicolon-separated list of tag-value pairs with additional information"
  },
  {
    "objectID": "07_annotation.html#annotation-with-prokka",
    "href": "07_annotation.html#annotation-with-prokka",
    "title": "Genome annotation",
    "section": "2 Annotation with Prokka",
    "text": "2 Annotation with Prokka\nWe will now annotate our genome assembly with the program Prokka (Seemann 2014, documentation).\nAnnotation consists of two main steps:\n\nStructural annotation: the identification of genes (and other genomic features)\nFunctional annotation: the assigment of gene names and functions to genes based on homology with known genes\n\nProkka will perform both of these steps for us, and its most important output files are a proteome (amino acid) FASTA file, and a GFF annotation file.\n\n\n\n\n\n\nBakta\n\n\n\nA more recent bacterial annotation program, Bakta ( Schwenger et al. 2021, documentation) , is generally able to functionally annotate more genes. But it also takes much longer to run, so in the interest of time, we will run Prokka. Otherwise, Prokka and Bakta are similar.\n\n\n\n\n2.1 Running Prokka\n\n\n\n\n\n\nIf you didn’t do decontamination with Kraken2, please run this!\n\n\n\nIf you didn’t perform the decontamination step with Kraken2 on the previous page, you’ll have to run the following to make sure your assembly is in the expected spot:\nmkdir results/decontam\ncp results/spades/SM04/contigs.fasta results/decontam/SM04.fasta\n(That is, you’re simply copying your original assembly to a new dir.)\n\n\nRunning Prokka is pretty simple:\nsource activate /fs/ess/PAS0471/jelmer/conda/prokka\n\nsbatch -A PAS2250 -t 15 -c 12 -o slurm-prokka.out --wrap=\"\n  prokka \\\n    --outdir results/prokka \\\n    --prefix SM04 \\\n    --cpus 12 \\\n    results/decontam/SM04.fasta\n\"\n\n\n\n2.2 Prokka output\nOnce its done after a couple of minutes, the end of your slurm-prokka.out file should look as follows:\ntail slurm-prokka.out\n[14:58:39] results/prokka/SM04.fsa\n[14:58:39] results/prokka/SM04.tbl\n[14:58:39] results/prokka/SM04.gbk\n[14:58:39] results/prokka/SM04.tsv\n[14:58:39] Annotation finished successfully.\n[14:58:39] Walltime used: 2.42 minutes\n[14:58:39] If you use this result please cite the Prokka paper:\n[14:58:39] Seemann T (2014) Prokka: rapid prokaryotic genome annotation. Bioinformatics. 30(14):2068-9.\n[14:58:39] Type 'prokka --citation' for more details.\n[14:58:39] Share and enjoy!\nLet’s take a look at the files in the output dir:\nls -lh results/prokka\ntotal 59M\n-rw-r--r-- 1 jelmer PAS0471 1.8M Feb  6 14:58 SM04.err\n-rw-r--r-- 1 jelmer PAS0471 2.0M Feb  6 14:58 SM04.faa\n-rw-r--r-- 1 jelmer PAS0471 5.4M Feb  6 14:58 SM04.ffn\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  6 14:56 SM04.fna\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  6 14:58 SM04.fsa\n-rw-r--r-- 1 jelmer PAS0471  12M Feb  6 14:58 SM04.gbk\n-rw-r--r-- 1 jelmer PAS0471 7.1M Feb  6 14:58 SM04.gff\n-rw-r--r-- 1 jelmer PAS0471  62K Feb  6 14:58 SM04.log\n-rw-r--r-- 1 jelmer PAS0471  19M Feb  6 14:58 SM04.sqn\n-rw-r--r-- 1 jelmer PAS0471 960K Feb  6 14:58 SM04.tbl\n-rw-r--r-- 1 jelmer PAS0471 322K Feb  6 14:58 SM04.tsv\n-rw-r--r-- 1 jelmer PAS0471   95 Feb  6 14:58 SM04.txt\nThe .faa files contains the proteome:\nless results/prokka/SM04.faa\n&gt;LHECFIHF_00002 Pectate lyase L\nMRTILLTVLLVVAATAQATDYYVAPNGDDNAAGTKGAPLRTIMRAQQAAKAGDTVYFRGG\nVYTYTAGINRCATRTDTVNAITLNNSGSENKPIRYWAYPGETPVFDFSAMKDDCRVKGFN\nVTGSWLHLKGLEVKGVPQQPENHLNHESWGIWNSGSHNTFEQLNLHHNMGPGLFIQNGGY\nNLVLNTDSHHNYDPYTSNGAGQSADGFGAHIKAGHPGNVFRGCRAWANSDDGFDLINAFS\nPVTIESSWAWQQGYLPGTRTKLEAGNGNGIKAGGYGGKYVPDGVKHTVRNSVAFDNKSAG\nFYANHHTLALDFINNTAFSNGVNYNMAGIAPDGSLIPLGNLSNNIAYKGRLTVNTEGLDM\nAHNSWTLPTPITDADFEDVSDTGWDAPRQPDGSLPVLRSFHLRAGGRLAGMGAFH\n&gt;LHECFIHF_00003 hypothetical protein\nMSQERYGIRRFALLNTAGYSLGLFPLENPLSVYGANNLGKSASINALQFPILARMSDMSF\nGKYSLEQSRRFYFASDTSYILVEVSLPHGPHVIGVVGRGPGGGFGHQFFAYAGELDLGHY\nQKNDTCLRQKELFSNLESQGLKAYELKPDELRRLLVGGHTSIPLDLTLIPLRSTSEQSLK\n[...output truncated...]\nThe .gff file is the main annotation file.\n\n\n\n2.3 Exploring the GFF file\nless -S results/prokka/SM04.gff\nOnce you scroll past the header…\n##gff-version 3\n##sequence-region NODE_1_length_1267796_cov_33.239498 1 1267796\n##sequence-region NODE_2_length_902255_cov_32.000245 1 902255\n##sequence-region NODE_3_length_697265_cov_34.901625 1 697265\n##sequence-region NODE_4_length_534491_cov_32.088021 1 534491\n…you reach the main annotation table:\nNODE_1_length_1267796_cov_33.239498 barrnap:0.9 rRNA    272 381 3.3e-13 +   .   ID=LHECFIHF_00001;locus_tag=LHECFIHF_00001;product=5S ribosomal RNA\nNODE_1_length_1267796_cov_33.239498 Prodigal:002006 CDS 518 1765    .   -   0   ID=LHECFIHF_00002;eC_number=4.2.2.2;Name=pelL;gene=pelL;inference=ab initio prediction:Prodigal:002006,similar to AA sequence:UniProtKB:P0C1A7;locus_tag=LHECFIHF_00002;product=Pectate lyase L\nNODE_1_length_1267796_cov_33.239498 Prodigal:002006 CDS 1939    4758    .   -   0   ID=LHECFIHF_00003;inference=ab initio prediction:Prodigal:002006;locus_tag=LHECFIHF_00003;product=hypothetical protein\nNODE_1_length_1267796_cov_33.239498 Prodigal:002006 CDS 4755    5462    .   -   0   ID=LHECFIHF_00004;inference=ab initio prediction:Prodigal:002006;locus_tag=LHECFIHF_00004;product=hypothetical protein\nNODE_1_length_1267796_cov_33.239498 Prodigal:002006 CDS 5452    6729    .   -   0   ID=LHECFIHF_00005;inference=ab initio prediction:Prodigal:002006;locus_tag=LHECFIHF_00005;product=hypothetical protein\nNODE_1_length_1267796_cov_33.239498 Prodigal:002006 CDS 6874    7680    .   +   0   ID=LHECFIHF_00006;inference=ab initio prediction:Prodigal:002006;locus_tag=LHECFIHF_00006;product=hypothetical protein\nNODE_1_length_1267796_cov_33.239498 Prodigal:002006 CDS 7673    8128    .   +   0   ID=LHECFIHF_00007;eC_number=2.3.1.266;Name=rimI;db_xref=COG:COG0456;gene=rimI;inference=ab initio prediction:Prodigal:002006,similar to AA sequence:UniProtKB:P0A944;locus_tag=LHECFIHF_00007;product=[Ribosomal protein S18]-alanine N-acetyltransferase\nNODE_1_length_1267796_cov_33.239498 Prodigal:002006 CDS 8138    8542    .   -   0   ID=LHECFIHF_00008;inference=ab initio prediction:Prodigal:002006;locus_tag=LHECFIHF_00008;product=hypothetical protein\nNODE_1_length_1267796_cov_33.239498 Prodigal:002006 CDS 8709    9347    .   +   0   ID=LHECFIHF_00009;eC_number=4.2.1.1;Name=can;db_xref=COG:COG0288;gene=can;inference=ab initio prediction:Prodigal:002006,similar to AA sequence:UniProtKB:P61517;locus_tag=LHECFIHF_00009;product=Carbonic anhydrase 2\nNODE_1_length_1267796_cov_33.239498 Prodigal:002006 CDS 9405    11069   .   -   0   ID=LHECFIHF_00010;inference=ab initio prediction:Prodigal:002006;locus_tag=LHECFIHF_00010;product=hypothetical protein\nHow many genes have been annotated?\nawk '$3 == \"CDS\"' results/prokka/SM04.gff | wc -l\n5080\nGenes with the name “hypothetical protein” were identified by the structural annotation, but were effectively not functionally annotated: we don’t have any information about their function.\nHow many “hypothetical proteins” are there?\ngrep -c \"hypothetical protein\" results/prokka/SM04.gff\n2071"
  },
  {
    "objectID": "07_annotation.html#virulence-gene-identification-with-abricate",
    "href": "07_annotation.html#virulence-gene-identification-with-abricate",
    "title": "Genome annotation",
    "section": "3 Virulence gene identification with Abricate",
    "text": "3 Virulence gene identification with Abricate\nWe will run the program Abricate to identity virulence genes using the Virulence Factor Database (--db vcfdb).\nmkdir results/abricate\n\nabricate \\\n  --db vfdb \\\n  results/decontam/SM04.fasta \\\n  &gt; results/abricate/SM04.tab\nUsing nucl database vfdb:  4366 sequences -  2024-Feb-6\nProcessing: results/decontam/SM04.fasta\nFound 21 genes in results/decontam/SM04.fasta\nTip: have a suggestion for abricate? Tell me at https://github.com/tseemann/abricate/issues\nDone.\nLet’s take a look at the output:\nless -S results/abricate/SM04.tab\n\n\nClick to show the expected output\n\n\n#FILE   SEQUENCE    START   END STRAND  GENE    COVERAGE    COVERAGE_MAP    GAPS    %COVERAGE   %IDENTITY   DATABASE    ACCESSION   PRODUCT RESISTANCE\nresults/decontam/SM04.fasta NODE_12_length_123484_cov_34.382354 35324   36088   +   crc 1-765/780   =============== 0/0 98.08   82.75   vfdb    NP_254019   (crc) catabolite repression control protein [Type IV pili (VF0082) - Adherence (VFC0001)] [Pseudomonas aeruginosa PAO1] \nresults/decontam/SM04.fasta NODE_1_length_1267796_cov_33.239498 207972  209374  -   algA    1-1403/1446 ========/====== 2/2 96.96   81.77   vfdb    NP_252241   (algA) phosphomannose isomerase / guanosine 5'-diphospho-D-mannose pyrophosphorylase [Alginate (VF0091) - Biofilm (VFC0271)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_1_length_1267796_cov_33.239498 211633  212916  -   algI    1-1284/1563 =============.. 0/0 82.15   83.02   vfdb    NP_252238   (algI) alginate o-acetyltransferase AlgI [Alginate (VF0091) - Biofilm (VFC0271)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_1_length_1267796_cov_33.239498 221445  222830  -   alg8    100-1485/1485   .============== 0/0 93.33   80.23   vfdb    NP_252231   (alg8) alginate-c5-mannuronan-epimerase AlgG [Alginate (VF0091) - Biofilm (VFC0271)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_1_length_1267796_cov_33.239498 746910  747244  +   pilZ    22-356/357  =============== 0/0 93.84   82.69   vfdb    NP_251650   (pilZ) type 4 fimbrial biogenesis protein PilZ [Type IV pili (VF0082) - Adherence (VFC0001)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_1_length_1267796_cov_33.239498 1097853 1098324 -   pvdS    43-514/564  .=======/=====. 2/4 83.33   82.91   vfdb    NP_251116   (pvdS) extracytoplasmic-function sigma-70 factor  [Pyoverdine (VF0094) - Nutritional/Metabolic factor (VFC0272)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_1_length_1267796_cov_33.239498 1112719 1114079 +   pvdH    7-1367/1410 ========/====== 4/4 96.38   80.78   vfdb    NP_251103   (pvdH) diaminobutyrate-2-oxoglutarate aminotransferase PvdH [Pyoverdine (VF0094) - Nutritional/Metabolic factor (VFC0272)] [Pseudomonas aeruginosa PAO1]    \nresults/decontam/SM04.fasta NODE_1_length_1267796_cov_33.239498 1114207 1114409 +   mbtH-like   1-203/219   ==============. 0/0 92.69   83.25   vfdb    NP_251102   (mbtH-like) MbtH-like protein from the pyoverdine cluster [Pyoverdine (VF0094) - Nutritional/Metabolic factor (VFC0272)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_3_length_697265_cov_34.901625  20055   20636   -   algU    1-582/582   =============== 0/0 100.00  81.27   vfdb    NP_249453   (algU) alginate biosynthesis protein AlgZ/FimS [Alginate (VF0091) - Biofilm (VFC0271)] [Pseudomonas aeruginosa PAO1]    \nresults/decontam/SM04.fasta NODE_3_length_697265_cov_34.901625  236211  237704  +   rpoN    1-1494/1494 ========/====== 6/12    99.60   82.20   vfdb    NP_253152   (rpoN) RNA polymerase factor sigma-54 [Type IV pili (VF0082) - Adherence (VFC0001)] [Pseudomonas aeruginosa PAO1]   \nresults/decontam/SM04.fasta NODE_5_length_350317_cov_33.463137  244380  244745  -   pilH    1-366/366   =============== 0/0 100.00  85.79   vfdb    NP_249100   (pilH) twitching motility protein PilH [Type IV pili (VF0082) - Adherence (VFC0001)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_5_length_350317_cov_33.463137  244819  245202  -   pilG    1-384/408   =============== 0/0 94.12   82.55   vfdb    NP_249099   (pilG) twitching motility protein PilG [Type IV pili (VF0082) - Adherence (VFC0001)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_5_length_350317_cov_33.463137  253182  254216  -   pilT    1-1035/1035 ========/====== 2/2 99.90   83.88   vfdb    NP_249086   (pilT) twitching motility protein PilT [Type IV pili (VF0082) - Adherence (VFC0001)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_5_length_350317_cov_33.463137  346430  347476  -   pilM    19-1065/1065    ========/====== 4/4 98.12   80.17   vfdb    NP_253731   (pilM) type IV pilus inner membrane platform protein PilM [Type IV pili (VF0082) - Adherence (VFC0001)] [Pseudomonas aeruginosa PAO1]   \nresults/decontam/SM04.fasta NODE_6_length_339735_cov_31.812540  47754   48509   +   flgG    10-762/786  ========/====== 2/3 95.80   80.82   vfdb    NP_249773   (flgG) flagellar basal-body rod protein FlgG [Flagella (VF0273) - Motility (VFC0204)] [Pseudomonas aeruginosa PAO1] \nresults/decontam/SM04.fasta NODE_6_length_339735_cov_31.812540  49404   50454   +   flgI    60-1110/1110    =============== 0/0 94.68   82.21   vfdb    NP_249775   (flgI) flagellar P-ring protein precursor FlgI [Flagella (VF0273) - Motility (VFC0204)] [Pseudomonas aeruginosa PAO1]   \nresults/decontam/SM04.fasta NODE_6_length_339735_cov_31.812540  74115   75108   +   fliG    22-1015/1017    =============== 0/0 97.74   83.80   vfdb    NP_249793   (fliG) flagellar motor switch protein G [Flagella (VF0273) - Motility (VFC0204)] [Pseudomonas aeruginosa PAO1]  \nresults/decontam/SM04.fasta NODE_6_length_339735_cov_31.812540  82920   83860   +   fliM    1-941/972   ========/====== 2/2 96.71   83.02   vfdb    NP_250134   (fliM) flagellar motor switch protein FliM [Flagella (VF0273) - Motility (VFC0204)] [Pseudomonas aeruginosa PAO1]   \nresults/decontam/SM04.fasta NODE_6_length_339735_cov_31.812540  84903   85591   +   fliP    70-758/768  .=======/====== 2/2 89.58   81.59   vfdb    NP_250137   (fliP) flagellar biosynthetic protein FliP [Flagella (VF0273) - Motility (VFC0204)] [Pseudomonas aeruginosa PAO1]   \nresults/decontam/SM04.fasta NODE_6_length_339735_cov_31.812540  91625   92433   +   fleN    10-818/843  =============== 0/0 95.97   82.82   vfdb    NP_250145   (fleN) flagellar synthesis regulator FleN [Flagella (VF0273) - Motility (VFC0204)] [Pseudomonas aeruginosa PAO1]    \nresults/decontam/SM04.fasta NODE_6_length_339735_cov_31.812540  101666  102120  +   PA1464  26-480/480  =============== 0/0 94.79   84.18   vfdb    NP_250155   (PA1464) purine-binding chemotaxis protein [Flagella (VF0273) - Motility (VFC0204)] [Pseudomonas aeruginosa PAO1]   \n\n\n\n\n\n\n\nAdditional Abricate databases\n\n\n\nYou may also want to run Abricate with other virulence gene databases, such as the BastionHub, and T3SEdb databases. Additionally, you can search for antimicrobial resistance genes with Abricate using e.g. the ResFinder and CARD databases."
  },
  {
    "objectID": "07_annotation.html#bonus-plasmid-detection-with-mob-suite",
    "href": "07_annotation.html#bonus-plasmid-detection-with-mob-suite",
    "title": "Genome annotation",
    "section": "4 Bonus: Plasmid detection with MOB-suite",
    "text": "4 Bonus: Plasmid detection with MOB-suite\nWe will run the mob_recon tool from the MOB-suite program (Robertson et al. 2018, documentation) to identity plasmids in our genomes.\nmob_recon \\\n    --infile results/decontam/SM04.fasta \\\n    --outdir results/mobsuite\n2024-02-06 15:35:34,565 mob_suite.mob_recon INFO: MOB-recon version 3.1.4  [in /fs/ess/PAS0471/jelmer/conda/mobsuite/lib/python3.8/site-packages/mob_suite/mob_recon.py:980]\n2024-02-06 15:35:34,567 mob_suite.mob_recon INFO: SUCCESS: Found program blastn at /fs/ess/PAS0471/jelmer/conda/mobsuite/bin/blastn [in /fs/ess/PAS0471/jelmer/conda/mobsuite/lib/python3.8/site-packages/mob_suite/utils.py:592]\n2024-02-06 15:35:34,569 mob_suite.mob_recon INFO: SUCCESS: Found program makeblastdb at /fs/ess/PAS0471/jelmer/conda/mobsuite/bin/makeblastdb [in /fs/ess/PAS0471/jelmer/conda/mobsuite/lib/python3.8/site-packages/mob_suite/utils.py:592]\n2024-02-06 15:35:34,571 mob_suite.mob_recon INFO: SUCCESS: Found program tblastn at /fs/ess/PAS0471/jelmer/conda/mobsuite/bin/tblastn [in /fs/ess/PAS0471/jelmer/conda/mobsuite/lib/python3.8/site-packages/mob_suite/utils.py:592]\n# [...output truncated...]\nWere any plasmids found?\ncat results/mobsuites/mobtyper_results.txt\nsample_id   num_contigs size    gc  md5 rep_type(s) rep_type_accession(s)   relaxase_type(s)    relaxase_type_accession(s)  mpf_type    mpf_type_accession(s)   orit_type(s)    orit_accession(s)   predicted_mobility  mash_nearest_neighbor   mash_neighbor_distance  mash_neighbor_identification    primary_cluster_id  secondary_cluster_id    predicted_host_range_overall_rank   predicted_host_range_overall_name   observed_host_range_ncbi_rank   observed_host_range_ncbi_name   reported_host_range_lit_rank    reported_host_range_lit_name    associated_pmid(s)\nSM04:AD116  1   48973   0.5424621730341208  fd0b4e82302b32b63f24c1d18a90d219    rep_cluster_283 001576__KY362368_00001  MOBP    CP007015_00034  MPF_T   NC_007275_00041,NZ_CM001987_00038,NC_007275_00033,NC_019328_00069,NC_007275_00028   MOBP    EU289287    conjugative CP006257    0.0572108   Pseudomonas syringae pv. syringae HS191 AD116   -   genus   Pseudomonas genus   Pseudomonas -   -   -"
  },
  {
    "objectID": "ref_shell2.html#more-on-shell-wildcards",
    "href": "ref_shell2.html#more-on-shell-wildcards",
    "title": "Unix shell introduction – continued",
    "section": "1 More on shell wildcards",
    "text": "1 More on shell wildcards\n\nFull list of shell wildcards\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n*\nAny number of any character, including nothing\n\n\n?\nAny single character\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets\n\n\n\n\n\n\n\n\n\nDon’t confuse shell wildcards with regular expressions! (Click to expand)\n\n\n\n\n\nFor those of you who know some regular expressions from coding in e.g. R, wildcards are conceptually similar to these, but the * and ? symbols don’t have the same meaning, and there are way fewer shell wildcards than regular expression symbols.\nIn particular, note that . is not a shell wildcard and thus represents a literal period when globbing.\n\n\n\n\n\n\n\n\n\nRecursive globbing (Click to expand)\n\n\n\n\n\nGlobbing does not work recursively by default, so ls *fastq.gz would only return gzipped FASTQ files in your current working dir.\nAfter running some bionformatics program, it’s relatively common to have FASTQ files in a separate directory for each sample. In cases like that, you can use ** to match any directory. For example, in the pattern below, the ** would “traverse” the fastq dir within the data dir:\nls data/**/*fastq.gz\nSM04_R1.fastq.gz    SM1031_R1.fastq.gz  SM1042_R1.fastq.gz  SM155_R1.fastq.gz  SM181_R1.fastq.gz  SM191_R1.fastq.gz  SM207_R1.fastq.gz  SM51_R1.fastq.gz\nSM04_R2.fastq.gz    SM1031_R2.fastq.gz  SM1042_R2.fastq.gz  SM155_R2.fastq.gz  SM181_R2.fastq.gz  SM191_R2.fastq.gz  SM207_R2.fastq.gz  SM51_R2.fastq.gz\nSM1030_R1.fastq.gz  SM1038_R1.fastq.gz  SM109_R1.fastq.gz   SM156_R1.fastq.gz  SM190_R1.fastq.gz  SM205_R1.fastq.gz  SM226_R1.fastq.gz  SM914_R1.fastq.gz\nSM1030_R2.fastq.gz  SM1038_R2.fastq.gz  SM109_R2.fastq.gz   SM156_R2.fastq.gz  SM190_R2.fastq.gz  SM205_R2.fastq.gz  SM226_R2.fastq.gz  SM914_R2.fastq.gz\nBut ** itself is not recursive either by default and only “traverses a single level”, so you’d need the following to list any FASTQ files that are exactly two levels deep from your current working dir:\nls **/**/*fastq.gz\nSM04_R1.fastq.gz    SM1031_R1.fastq.gz  SM1042_R1.fastq.gz  SM155_R1.fastq.gz  SM181_R1.fastq.gz  SM191_R1.fastq.gz  SM207_R1.fastq.gz  SM51_R1.fastq.gz\nSM04_R2.fastq.gz    SM1031_R2.fastq.gz  SM1042_R2.fastq.gz  SM155_R2.fastq.gz  SM181_R2.fastq.gz  SM191_R2.fastq.gz  SM207_R2.fastq.gz  SM51_R2.fastq.gz\nSM1030_R1.fastq.gz  SM1038_R1.fastq.gz  SM109_R1.fastq.gz   SM156_R1.fastq.gz  SM190_R1.fastq.gz  SM205_R1.fastq.gz  SM226_R1.fastq.gz  SM914_R1.fastq.gz\nSM1030_R2.fastq.gz  SM1038_R2.fastq.gz  SM109_R2.fastq.gz   SM156_R2.fastq.gz  SM190_R2.fastq.gz  SM205_R2.fastq.gz  SM226_R2.fastq.gz  SM914_R2.fastq.gz\nHowever, you can make ** recursive by turning on the globstar option:\nshopt -s globstar     # Turn on 'globstar'\n\nls **/*fastq.gz\nSM04_R1.fastq.gz    SM1031_R1.fastq.gz  SM1042_R1.fastq.gz  SM155_R1.fastq.gz  SM181_R1.fastq.gz  SM191_R1.fastq.gz  SM207_R1.fastq.gz  SM51_R1.fastq.gz\nSM04_R2.fastq.gz    SM1031_R2.fastq.gz  SM1042_R2.fastq.gz  SM155_R2.fastq.gz  SM181_R2.fastq.gz  SM191_R2.fastq.gz  SM207_R2.fastq.gz  SM51_R2.fastq.gz\nSM1030_R1.fastq.gz  SM1038_R1.fastq.gz  SM109_R1.fastq.gz   SM156_R1.fastq.gz  SM190_R1.fastq.gz  SM205_R1.fastq.gz  SM226_R1.fastq.gz  SM914_R1.fastq.gz\nSM1030_R2.fastq.gz  SM1038_R2.fastq.gz  SM109_R2.fastq.gz   SM156_R2.fastq.gz  SM190_R2.fastq.gz  SM205_R2.fastq.gz  SM226_R2.fastq.gz  SM914_R2.fastq.gz\nWith globstar turned on, the pattern above would find gzipped FASTQ files no matter how many dir levels deep they are (including when they are in your current working dir)."
  },
  {
    "objectID": "ref_shell2.html#command-output-redirection",
    "href": "ref_shell2.html#command-output-redirection",
    "title": "Unix shell introduction – continued",
    "section": "2 Command output redirection",
    "text": "2 Command output redirection\nAs mentioned earlier, Unix commands nearly always print their output to the screen. But you can also “redirect” the output to a file or to another command.\nWith “&gt;”, we redirect output to a file:\n\nIf the file doesn’t exist, it will be created.\nIf the file does exist, any contents will be overwritten.\n\necho \"My first line\" &gt; test.txt\ncat test.txt\nMy first line\nRedirection works not just with echo, but with every single command (or bioinformatics program) that prints output to screen:\nls &gt; my_files_on_2023-08-04.txt\ncat my_files_on_2023-08-04.txt    # Use tab completion\ndata\nmy_files_on_20230803.txt\ntest.txt\n\n\n\n\n\n\n\nA less-often used redirection: appending with &gt;&gt; (Click to expand)\n\n\n\n\n\nWith “&gt;&gt;”, we append the output to a file (that is, it won’t overwrite any existing content like &gt;):\necho \"My second line\" &gt;&gt; test.txt\ncat test.txt\nMy first line\nMy second line\nAnd to circle back to &gt;, demonstrating how this will overwrite contents:\necho \"My third line overwrote the first two!\" &gt; test.txt\ncat test.txt\nMy third line overwrote the first two!"
  },
  {
    "objectID": "ref_shell2.html#move-with-mv-and-cpmv-tips",
    "href": "ref_shell2.html#move-with-mv-and-cpmv-tips",
    "title": "Unix shell introduction – continued",
    "section": "3 Move with mv, and cp/mv tips",
    "text": "3 Move with mv, and cp/mv tips\nThe mv command is nearly identical to the cp command, except that it:\n\nMoves rather than copies files and/or dirs\nWorks recursively by default\n\nThere is no separate command for renaming, because both cp and mv allow you to provide a different name for the target. Thus, if used as follows, mv functions merely as a renamer:\nmv test.txt test_version2.txt\nAnd we can move and rename at the same time as well — let’s do that to restore our original location and name for the metadata file:\nmv test_version2.txt sandbox/test_version3.txt\n\n\n\n\n\n\nOverwriting\n\n\n\nBy default, both mv and cp will overwrite files without warning! Use the -i (for interactive) option to make it let you confirm before overwriting anything.\n\n\n\n\n\n\n\n\nRenaming rules for both cp and mv — if the destination is:\n\n\n\n\nAn existing dir, the file(s) will keep their original names.\nNot an existing dir, the path specifies the new name of the file or dir, depending on what the source is.\n\n\n\n\nExercise: Practice with mv\nIn which directory (in terms of a relative path from your working dir) would the FASTQ files end up with each of the following commands?\n\nmv data/fastq data/fastq_files\nmv data/fastq fastq\nmv data/fastq .\n\nWhat if you wanted to move the FASTQ files directly into your current working directory (from data/fastq)?\n\n\n\nSolutions (click here)\n\nIn which directory (in terms of relative path from your working dir) will the FASTQ files end up with each of the following commands?\n\nmv data/fastq data/fastq_files — in the dir fastq_files (we’ve really just renamed the dir fastq to fastq_files)\nmv data/fastq fastq — in fastq (because our source is a dir, so is the destination)\nmv data/fastq . — in fastq also! (we’d need the syntax shown below to move the individual files directly into our current dir)\n\nWhat if you wanted to move the FASTQ files directly into your current working directory?\nFor one file:\nmv data/fastq/ASPC1_A178V_R1.fastq.gz .\nFor all files:\nmv data/fastq/* ."
  },
  {
    "objectID": "ref_shell2.html#remove-files-with-rm",
    "href": "ref_shell2.html#remove-files-with-rm",
    "title": "Unix shell introduction – continued",
    "section": "4 Remove files with rm",
    "text": "4 Remove files with rm\nThe rm command removes (deletes) files and directories.\nOne important thing to note upfront is that rm will permanently and irreversibly delete files without the typical “intermediate step” of placing them in a trash bin, like you are used to with your personal computer. With a healthy dosis of fear installed, let’s dive in.\nTo remove one or more files, you can simply pass the file names as arguments to rm as with previous commands. We will also use the -v (verbose) option to have it tell us what it did:\n# This assumes you ran `touch file1 file2 file3` earlier on; do so now if needed\nrm -v file1            # Remove 1 file\nremoved ‘file1’\nrm -v file2 file3      # Remove 2 files\nremoved ‘file2’\nremoved ‘file3’\n\nRecursive rm\nAs a safety measure, rm will by default only delete files and not directories or their contents — i.e., like mkdir and cp, it refuses to act recursively by default. To remove dirs and their contents, use the -r option:\n# You should have these from 2.4 (if not, run: mkdir \"my new dir\" \"my new dir2\")\nrm -r \"my new dir\" \"my new dir2\"\nremoved directory: ‘my new dir’\nremoved directory: ‘my new dir2’\nYou should obviously be quite careful with rm -r!"
  },
  {
    "objectID": "ref_shell2.html#finding-text-with-grep-zgrep",
    "href": "ref_shell2.html#finding-text-with-grep-zgrep",
    "title": "Unix shell introduction – continued",
    "section": "Finding text with grep / zgrep",
    "text": "Finding text with grep / zgrep\ngrep allows you to search a file for any text or text patterns. By default, it will return the entire line whenever it finds a match, and it is often set up (including at OSC) to highlight, within that line, the matching text in bold red in its output.\nFor example, let’s say we wanted to print the sequences in our FASTQ file that contain “TACCGATACGC”:\nzcat data/fastq/SM04_R1.fastq.gz | grep \"TACCGATACGC\"\n# (Only part of the output is shown in a screenshot below)\n\n\n\n\n\nIf we wanted to know how many reads contain a certain sequence (e.g, the shorter and therefore more common “CCAGTA”), we can simply pipe grep’s output into wc -l:\nzcat data/fastq/SM04_R1.fastq.gz | grep \"CCAGTA\" | wc -l\n56691\ngrep has many options — one of those is -c, which will directly count matching lines instead of printing them (i.e., the command below is an alternative to the command above where we piped greps output into wc -l):\nzcat data/fastq/SM04_R1.fastq.gz | grep -c \"CCAGTA\"\n56691\ngrep even has a zgrep counterpart for gzip-compressed files, so the above can be further shortened by omitting zcat and passing the FASTQ file name as an argument to zgrep:\nzgrep -c \"CCAGTA\" data/fastq/SM04_R1.fastq.gz\n56691\nWe could also create a new FASTQ file whose sequences match our search by printing one line before each match (-B1) and two lines after it (-A2):\nzgrep -B1 -A2 --no-group-separator \"CCAGTA\" data/fastq/SM04_R1.fastq.gz &gt; CCAGTA.fastq"
  },
  {
    "objectID": "ref_shell2.html#the-sambam-format",
    "href": "ref_shell2.html#the-sambam-format",
    "title": "Unix shell introduction – continued",
    "section": "5 The SAM/BAM format",
    "text": "5 The SAM/BAM format\nThe SAM/BAM format is a format for alignments, typically of reads from FASTQ files to a reference genome FASTA file.\n\nSAM (Sequence Alignment/Map) is an uncompressed text file\nBAM (Binary Alignment/Map) is its binary, compressed counterpart. BAM files are not human-readable but most software can work with them directly.\n\nAfter a header with metadata, the main part of the file contains one line per alignment, with information such as to which contig/scaffold/chromosome and position the read mapped, and the full sequence of the original read is also included:\nA01088:17:HHVG3DRXX:1:2238:4164:16877    163     SL4.0ch00       18287   255     12M725N139M     =       19069   931     CTCAATTATATGGTGTAGACGCACAGTTCGGTGATCCTCCCGCCTAGGATATCTACTCTGCTGATTGGGAGAGCTCCACTGTTCCGGAGCCCAGTCATTTTGGTACATAACTTTTGTGTAGTCTTTTGCTCGTGTATGGGTATGGCGGGGC     FFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFF:F:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFF:FFFFFF NH:i:1  HI:i:1  AS:i:298   nM:i:0\nA01088:17:HHVG3DRXX:1:2238:4164:16877    83      SL4.0ch00       19069   255     149M    =       18287   -931    CTGCTGATTGGGAGAGCTCCACTGTTCCGGAGCCCAGTCATTTTGGTACATAACTTTTGTGTAGTCTTTTGCTCGTGTATGGGTATGGCGGGGCCCTGTCCCGTCGAGTTTCACTAATGTACTCTTAGAGGTCTGTGGACATTATGTGG       FFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFF   NH:i:1  HI:i:1  AS:i:298        nM:i:0\nAA01088:17:HHVG3DRXX:1:2143:6876:32440   163     SL4.0ch00       19077   255     125M    =       19077   125     TGGGAGAGCTCCACTGTTCCGGAGCCCAGTCATTTTGGTACATAACTTTTGTGTAGTCTTTTGCTCGTGTATGGGTATGGCGGGGCCCTGTCCCGTCGAGTTTCACTAATGTACTCTTAGAGGTC       FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFFFFF:FFFFFF:FF,FFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F:FFFFFF,FFFFFF:F   NH:i:1  HI:i:1  AS:i:248        nM:i:0\nAA01088:17:HHVG3DRXX:1:2143:6876:32440   83      SL4.0ch00       19077   255     125M    =       19077   -125    TGGGAGAGCTCCACTGTTCCGGAGCCCAGTCATTTTGGTACATAACTTTTGTGTAGTCTTTTGCTCGTGTATGGGTATGGCGGGGCCCTGTCCCGTCGAGTTTCACTAATGTACTCTTAGAGGTC       :FFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,:FFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF   NH:i:1  HI:i:1  AS:i:248        nM:i:0\n\n\nClick here for info on all SAM columns\n\n\n\n\nCol. nr.\nField\nDescription\nExample\n\n\n\n\n1\nQNAME\nQuery (=aligned read) name\n….\n\n\n2\nFLAG\nBitwise flag\n99\n\n\n3\nRNAME\nReference sequence name\nchr1\n\n\n4\nPOS\nMapping position (leftmost)\n46317\n\n\n5\nMAPQ\nMapping quality\n255\n\n\n6\nCIGAR\nCIGAR string\n150M\n\n\n7\nRNEXT\nName of read’s mate\n=\n\n\n8\nPNEXT\nPosition of read’s mate\n46517\n\n\n9\nTLEN\nTemplate length\n450\n\n\n10\nSEQ\nSequence of the read\nAGTTACCGATCCT…\n\n\n11\nQUAL\nBase quality of the read\nFFFF@HHHHHHHH…\n\n\n(optional)\nTAG\nOptional information\nSM:i:37"
  },
  {
    "objectID": "bact_slides.html#introduction",
    "href": "bact_slides.html#introduction",
    "title": "Bacterial  whole-genome analysis",
    "section": "Introduction",
    "text": "Introduction\nToday, we will assemble a bacterial genome from one of the pairs of FASTQ files that you copied yesterday morning:\ntotal 6.1G\n-rw-r--r-- 1 jelmer PAS0471 205M Feb  7 11:21 SM04_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 242M Feb  7 11:21 SM04_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 188M Feb  7 11:21 SM1030_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 221M Feb  7 11:21 SM1030_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 187M Feb  7 11:21 SM1031_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 221M Feb  7 11:21 SM1031_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 187M Feb  7 11:21 SM1038_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 224M Feb  7 11:21 SM1038_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 176M Feb  7 11:21 SM1042_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 199M Feb  7 11:21 SM1042_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 172M Feb  7 11:21 SM109_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 198M Feb  7 11:21 SM109_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 157M Feb  7 11:21 SM155_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 181M Feb  7 11:21 SM155_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 155M Feb  7 11:21 SM156_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 185M Feb  7 11:21 SM156_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 146M Feb  7 11:21 SM181_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 159M Feb  7 11:21 SM181_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 195M Feb  7 11:21 SM190_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 241M Feb  7 11:21 SM190_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 192M Feb  7 11:21 SM191_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 234M Feb  7 11:21 SM191_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 194M Feb  7 11:21 SM205_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 242M Feb  7 11:21 SM205_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 176M Feb  7 11:21 SM207_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 210M Feb  7 11:21 SM207_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 137M Feb  7 11:21 SM226_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 165M Feb  7 11:21 SM226_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 192M Feb  7 11:21 SM51_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 224M Feb  7 11:21 SM51_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 193M Feb  7 11:21 SM914_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 226M Feb  7 11:21 SM914_R2.fastq.gz"
  },
  {
    "objectID": "bact_slides.html#why-sequence-full-genomes",
    "href": "bact_slides.html#why-sequence-full-genomes",
    "title": "Bacterial  whole-genome analysis",
    "section": "Why sequence full genomes?",
    "text": "Why sequence full genomes?\nBest possible resolution for, e.g.:\n\nPhylogenetics\nPopulation structure\nEpidemiology\nAssociation analyses like GWAS\n\n\n\nThe only way to:\n\nComprehensively detect virulence genes, AMR genes, etc\nComprehensively compare gene content among genomes\nPerform a pangenome analysis"
  },
  {
    "objectID": "bact_slides.html#epidemiology",
    "href": "bact_slides.html#epidemiology",
    "title": "Bacterial  whole-genome analysis",
    "section": "Epidemiology",
    "text": "Epidemiology\n\nFrom Weisberg et al. 2021"
  },
  {
    "objectID": "bact_slides.html#genome-assemblies-through-time",
    "href": "bact_slides.html#genome-assemblies-through-time",
    "title": "Bacterial  whole-genome analysis",
    "section": "Genome assemblies through time",
    "text": "Genome assemblies through time\n\nFrom Koonin et al. 2021"
  },
  {
    "objectID": "bact_slides.html#most-frequent-sequencing-technologies",
    "href": "bact_slides.html#most-frequent-sequencing-technologies",
    "title": "Bacterial  whole-genome analysis",
    "section": "Most frequent sequencing technologies",
    "text": "Most frequent sequencing technologies\n\n\n\n\nFrom Amoutzias et al. 2022"
  },
  {
    "objectID": "bact_slides.html#the-current-best-way",
    "href": "bact_slides.html#the-current-best-way",
    "title": "Bacterial  whole-genome analysis",
    "section": "The current best way?",
    "text": "The current best way?\n\nFrom Wick et al. 2023"
  },
  {
    "objectID": "bact_slides.html#the-current-best-way-cont.",
    "href": "bact_slides.html#the-current-best-way-cont.",
    "title": "Bacterial  whole-genome analysis",
    "section": "The current best way? (cont.)",
    "text": "The current best way? (cont.)\n\nFrom Wick et al. 2023"
  },
  {
    "objectID": "bact_slides.html#sequencing-adapters---recap",
    "href": "bact_slides.html#sequencing-adapters---recap",
    "title": "Bacterial  whole-genome analysis",
    "section": "Sequencing adapters - recap",
    "text": "Sequencing adapters - recap\nAfter library prep, each DNA fragment is flanked by several types of short sequences that together make up the “adapters”:"
  },
  {
    "objectID": "bact_slides.html#paired-end-vs.-single-end-sequencing",
    "href": "bact_slides.html#paired-end-vs.-single-end-sequencing",
    "title": "Bacterial  whole-genome analysis",
    "section": "Paired-end vs. single-end sequencing",
    "text": "Paired-end vs. single-end sequencing\nIn Illumina sequencing, DNA fragments can be sequenced from both ends as shown below — this is called “paired-end” (PE) sequencing:\n\n\n\n\n\n\n\nWhen sequencing is instead single-end (SE), no reverse read is produced:"
  },
  {
    "objectID": "bact_slides.html#paired-end-sequencing",
    "href": "bact_slides.html#paired-end-sequencing",
    "title": "Bacterial  whole-genome analysis",
    "section": "Paired-end sequencing",
    "text": "Paired-end sequencing\n\nPaired-end sequencing is a way to effectively increase the read length.\nThe total size of the biological DNA fragment (without adapters) is often called the insert size:"
  },
  {
    "objectID": "bact_slides.html#insert-size-variation",
    "href": "bact_slides.html#insert-size-variation",
    "title": "Bacterial  whole-genome analysis",
    "section": "Insert size variation",
    "text": "Insert size variation\nInsert size varies based on the library prep protocol aims, and because of variation due to limited precision in size selection. In some cases, the insert size can be:\n\nShorter than the combined read length, leading to overlapping reads:\n\n\n\n\n\n\n\n\nShorter than the single read length, leading to “adapter read-through” (i.e., the ends of the resulting reads will consist of adapter sequence, which should be removed):"
  },
  {
    "objectID": "bact_slides.html#illumina-error-profile",
    "href": "bact_slides.html#illumina-error-profile",
    "title": "Bacterial  whole-genome analysis",
    "section": "Illumina error profile",
    "text": "Illumina error profile"
  },
  {
    "objectID": "bact_slides.html#illumina-error-profile-cont.",
    "href": "bact_slides.html#illumina-error-profile-cont.",
    "title": "Bacterial  whole-genome analysis",
    "section": "Illumina error profile (cont.)",
    "text": "Illumina error profile (cont.)\n\n\n\nThe different templates within a cluster get out of sync because occasionally:\n\nThey miss a base incorporation\nThey incorporate two bases at once\n\n\n\n\nBase incorporation may also terminate before the end of the template is reached\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis error profile is why, for Illumina:\n\n\n\nThere are hard limits on read lengths\nBase quality scores typically decrease along the read"
  },
  {
    "objectID": "bact_slides.html#our-pathogen",
    "href": "bact_slides.html#our-pathogen",
    "title": "Bacterial  whole-genome analysis",
    "section": "Our pathogen",
    "text": "Our pathogen\n\nPseudomonas syringae causes disease in a wide range of host plants, from Solanaceae and Leguminosae plants to citrus and stone fruit trees.\nPseudomonas syringae pv. syringae (Pss) is an emerging phytopathogen that causes Pseudomonas leaf spot (PLS) disease in pepper plants (Capsicum annuum var. annuum).\nCopper-based antimicrobials are used as chemical control methods for Pss in peppers. This has resulted in the emergence of copper-resistant strains of Pss."
  },
  {
    "objectID": "bact_slides.html#our-dataset-1",
    "href": "bact_slides.html#our-dataset-1",
    "title": "Bacterial  whole-genome analysis",
    "section": "Our dataset",
    "text": "Our dataset\n\nFrom Ranjit et al. in prep, Ohio State University\n16 Pss samples were isolated from pepper plants harboring characteristic PLS symptoms in Ohio between 2013 and 2021.\nIllumina MiSeq sequencing of these isolates: 2x300 bp reads"
  },
  {
    "objectID": "bact_slides.html#the-steps-involved",
    "href": "bact_slides.html#the-steps-involved",
    "title": "Bacterial  whole-genome analysis",
    "section": "The steps involved",
    "text": "The steps involved\n\nQC and preprocessing of the FASTQ files\nAssembly and assembly QC\nAnnotation\n\n\n\nPangenomics and phylogenetics (not included)"
  },
  {
    "objectID": "bact_slides.html#variable-genome-vs.-disease-severity",
    "href": "bact_slides.html#variable-genome-vs.-disease-severity",
    "title": "Bacterial  whole-genome analysis",
    "section": "Variable genome vs. disease severity",
    "text": "Variable genome vs. disease severity"
  },
  {
    "objectID": "bact_slides.html#pangenomes",
    "href": "bact_slides.html#pangenomes",
    "title": "Bacterial  whole-genome analysis",
    "section": "Pangenomes",
    "text": "Pangenomes\n\n\nFrom Koonin et al. 2021"
  },
  {
    "objectID": "ref_software.html#overview",
    "href": "ref_software.html#overview",
    "title": "Software management",
    "section": "Overview",
    "text": "Overview\nAt supercomputers like OSC, there are often system-wide installations of a number of bioinformatics programs. We do need to “load” such programs before we can use them. However, OSC’s collection of bioinformatics programs is unfortunately not comprehensive, and some of the available programs only come in relatively old versions.\nWe therefore also need another way to make bioinformatics programs available to ourselves. Two common methods are the Conda software management program and containers. We will talk about loading MCIC’s Conda environments, while the at-home reading covers installing software yourself with Conda, and using containers downloaded from the internet."
  },
  {
    "objectID": "ref_software.html#loading-software-at-osc-with-lmod-modules",
    "href": "ref_software.html#loading-software-at-osc-with-lmod-modules",
    "title": "Software management",
    "section": "1 Loading software at OSC with Lmod modules",
    "text": "1 Loading software at OSC with Lmod modules\nOSC administrators manage software with the “Lmod” system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it. That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.\nWe can load, unload, and search for available software modules using the module command and its various subcommands.\n\n1.1 Checking whether a program is available\nThe OSC website has a list of installed software. You can also search for available software in the shell using two subtly different module subcommands1:\n\nmodule spider lists all modules that are installed.\nmodule avail lists modules that can be directly loaded given the current environment (i.e., taking into account which other software has been loaded).\n\nSimply running module spider or module avail would spit out the full lists of installed/available programs — it is more useful to add a search term as an argument to these commands — below, we’ll search for the Conda distribution “miniconda”, with each of these two subcommands:\nmodule spider miniconda\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  miniconda3:\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        miniconda3/4.10.3-py37\n        miniconda3/4.12.0-py38\n        miniconda3/4.12.0-py39\n        miniconda3/23.3.1-py310\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"miniconda3\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider miniconda3/4.12.0-py39\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nmodule avail miniconda\n------------------------------------------------------------------------------------------------------ /apps/lmodfiles/Core -------------------------------------------------------------------------------------------------------\n   miniconda3/4.10.3-py37 (D)    miniconda3/4.12.0-py38    miniconda3/4.12.0-py39    miniconda3/23.3.1-py310\n\n  Where:\n   D:  Default Module\nAs stated at the bottom of the output below, the (D) in the module avail output above marks the default version of the program: this is the version of the program that will be loaded if we don’t specify a version ourselves (see examples below). The module spider command does not provide this information.\n\n\n\n1.2 Loading software\nAll other Lmod software functionality is also accessed using module subcommands. For instance, to make a program available to us we use the load subcommand:\n# Load a module:\nmodule load miniconda3               # Load the default version\nmodule load miniconda3/23.3.1-py310  # Load a specific version\n\n\n\n\n\n\nModules do not remain loaded across separate shell sessions\n\n\n\nModule loading does not persist across shell sessions. Whenever you get a fresh shell session (including but not limited to after logging into OSC again), you will have to (re)load any modules you want to use!\n\n\nTo check which modules have been loaded, use module list. Its output will also include automatically loaded modules, so for example, if you loaded miniconda3/23.3.1-py310, you should see the following list where the miniconda3 module is listed as the 6th entry:\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest   2) gcc-compatibility/8.4.0   3) intel/19.0.5   4) mvapich2/2.3.3   5) modules/sp2020   6) miniconda3/23.3.1-py310\nOccasionally, when you run into conflicting (mutually incompatible) modules, it can be useful to unload modules, which you can do as follows:\nmodule unload miniconda3    # Unload a specific module\nmodule purge                # Unload all modules\n\n\n\n1.3 A practical example: FastQC again\nHere, we’ll load the module for FastQC again. First, let’s confirm that we indeed cannot currently use FastQC by running the fastqc command with the --help option:\nfastqc --help\nbash: fastqc: command not found\n\n\n\n\n\n\nHelp!\n\n\n\nMany command-line programs can be run with with a --help (and/or -h) flag, and this is often a good thing to try first, since it will tell use whether we can use the program — and if we can, we immediately get some usage information.\n\n\nNext, let’s check whether FastQC is available at OSC, and if so, in which versions:\nmodule avail fastqc\nfastqc/0.11.8\nThere is only one version available (0.11.8), which means that module load fastqc and module load fastqc/0.11.8 would each load that same version.\n\n\n\n\n\n\nWhat might still be a reason to specify the version when we load the FastQC module?\n\n\n\n\n\nWhen we use the module load command inside a script, specifying a version would:\n\nEnsure that when we run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nMake it easy to see which version we used, which is something we typically report in papers.\n\n\n\n\nLet’s load the FastQC module:\nmodule load fastqc/0.11.8\nNow, we can retry our --help attempt:\nfastqc --help\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n        fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN  \n# [...truncated...]\n\nOn your own: load miniconda3\nThe miniconda3 module will allow us to use Conda software environments, which we’ll talk about more below.\n\nLet’s start with a clean sheet by running module purge.\nLoad the default version of miniconda3, and then check which version was loaded.\n\n\n\n\n\n\n\nSolution (Click here)\n\n\n\n\n\nmodule load miniconda3\n\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest   2) gcc-compatibility/8.4.0   3) intel/19.0.5   4) mvapich2/2.3.3   5) modules/sp2020   6) miniconda3/4.10.3-py37\nThe version 4.10.3-py37 was loaded.\n\n\n\n\nNow load the latest version of miniconda3 without unloading the earlier version first. What output do you get?\n\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\nLmod detected that you tried to load a different version of a software that was already loaded, so it changes the version and tells you about it:\nmodule load miniconda3/23.3.1-py310\nThe following have been reloaded with a version change:\n  1) miniconda3/4.10.3-py37 =&gt; miniconda3/23.3.1-py310"
  },
  {
    "objectID": "ref_software.html#when-software-isnt-installed-at-osc",
    "href": "ref_software.html#when-software-isnt-installed-at-osc",
    "title": "Software management",
    "section": "2 When software isn’t installed at OSC",
    "text": "2 When software isn’t installed at OSC\nIt’s not too uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than what is available. In that case, the following two are generally your best options:\n\nConda, which creates software environments that you can activate much like the Lmod modules.\nContainers, which are self-contained software environments that include operating systems, akin to mini virtual machines. While Docker containers are most well-known, OSC uses Apptainer (formerly known as Singularity) containers.\n\n\n\n\n\n\n\nOther options to install software / get it installed\n\n\n\n\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through Lmod / module commands).\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”2 and will often have difficulties with “dependencies”3.\n\n\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally for reproducible and portable software environments. They also allow you to easily maintain distinct “environments”, each with a different version of the same software, or with mutually incompatible software.\nNext, we’ll talk about Conda and using the MCIC’s Conda environments. The at-home reading includes installing software yourself with Conda, and using containers downloaded from the internet."
  },
  {
    "objectID": "ref_software.html#intro-to-conda-using-mcics-conda-environments",
    "href": "ref_software.html#intro-to-conda-using-mcics-conda-environments",
    "title": "Software management",
    "section": "3 Intro to Conda & using MCIC’s Conda environments",
    "text": "3 Intro to Conda & using MCIC’s Conda environments\nThe Conda software can create so-called environments in which one can install one or more software packages.\nAs you can see in the at-home reading below, as long as a program is available in one of the online Conda repositories (which is nearly always for bioinformatics programs), then installing it is quite straightforward, doesn’t require admin privileges, and is done with a procedure that is nearly identical regardless of the program you are installing.\nHowever, at OSC, you will probably not even have to install anything yourself, at least not if you are following “standard” workflows with common data like RNAseq. To this end, I maintain an “MCIC collection” of Conda environments that anyone can use.\nA Conda environment is just a directory, and since all the environments in this collection are in the same place at OSC, you can list the MCIC Conda environments as follows:\nls /fs/ess/PAS0471/jelmer/conda\nabricate-1.0.1  bedops-2.4.39  checkm-1.2.0   entrez-direct    htseq-2.0.2          longstitch-1.0.3  nanopolish-0.13.2    prokka            repeatmasker-4.1.2.p1         samtools                star\nagat-0.9.1      bedtools       clinker        evigene          inspector-1.2.0      mafft             ncbi-datasets        pseudofinder      repeatmodeler-2.0.3           scoary                  subread-2.0.1\nalv             bioawk         clonalframeml  fastp            interproscan-5.55    maskrc-svg        nextdenovo-env       purge_dups-1.2.6  resfinder                     seqkit                  tgsgapcloser\namrfinderplus   biopython      codan-1.2      fastqc           iqtree               medaka-1.7.2      nextflow             pycoqc-2.5.2      resistomeanalyzer-2018.09.06  seqtk                   tracy-0.7.1\nantismash       bit            cogclassifier  fastq-dl         justorthologs-0.0.2  metaxa-2.2.3      orna-2.0             qiime2-2022.8     rgi-5.2.1                     signalp-6.0             transabyss-2.0.1\nariba-2.14.6    blast          cutadapt       fasttree-2.1.11  kallisto-0.48.0      minibusco         orthofinder          qualimap-env      r-metabar                     sistr-1.1.1             transdecoder-5.5.0\nastral-5.7.8    bowtie2-2.5.0  deeploc        filtlong-env     kat-2.4.2            minimap2-2.24     orthofisher          quast-5.0.2       rnaquast-2.2.1                smartdenovo-env         treetime\naswcli          bracken-2.6.1  deeptmhmm      flye-2.9.1       knsp-3.1             mlst              panaroo              quickmerge-env    roary-3.13                    snippy-4.6.0            trimgalore\nbactopia        braker2-env    deeptmhmm2     fmlrc2-0.1.7     kofamscan            mlst_check        phylofisher          racon-1.5.0       r-rnaseq                      snp-sites-2.5.1         trimmomatic-0.39\nbactopia-dev    busco          diamond        gcta             kraken2-2.1.2        mobsuite          pilon-1.24           ragtag-2.1.0      rsem-1.3.3                    soapdenovo-trans-1.0.4  trinity-2.13.2\nbakta           bwa-0.7.17     dwgsim         gffread-0.12.7   krakentools-1.2      multiqc           pkgs                 rascaf            rseqc-env                     sortmerna-env           unicycler\nbase            bwa-mem-2.2.1  eggnogmapper   gubbins          krona                mummer4           plasmidfinder-2.1.6  rcorrector-1.0.5  r_tree                        sourmash                virulencefinder\nbbmap           cactus         emboss         hisat2           liftoff-1.6.3        nanolyse-1.2.1    plink2               r-deseq           sabre-1.0                     spades-3.15.5           wtdbg-2.5\nbcftools        cgmlst         entap-0.10.8   hmmer            links-2.0.1          nanoplot          porechop             recognizer-1.8.3  salmon                        sra-tools\nThis is organized similarly to the Lmod modules in that there’s generally one separate environment for one program (and all its dependencies), and the environment is named after that program.\nThe naming of the environments is unfortunately not entirely consistent: many environments include the version number of the program, but many others do not. (Generally speaking, for environments without version numbers, you should expect the version of the program to be very recent, as I try to keep these up-to-date4).\nThis collection includes Conda environments for several programs we need during RNAseq analysis that are not installed at OSC, such as MultiQC, TrimGalore, and SortMeRNA.\n\n\n3.1 Activating Conda environments\nConda itself is already installed at OSC through Miniconda, but we always need to load its module before we can use it:\nmodule load miniconda3\nAs mentioned above, these environments are activated and deactivated in a similar manner as with the Lmod system. But whereas we use the term “load” for Lmod modules, we use “activate” for Conda environments — it means the same thing.\nAlso like Lmod, there is a main command (conda) and several subcommands (deactivate, create, install, update) for different functionality. However, for historical reasons, the most foolproof way to activate a Conda environment is to use source activate rather than the expected conda activate — for instance:\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n(multiqc) [jelmer@p0085 rnaseq-intro]$\n\n\n\n\n\n\nConda environment indicator\n\n\n\nWhen we have an active Conda environment, its name is displayed in front of our prompt, as depicted above with (multiqc).\n\n\nAfter we have activated the MultiQC environment, we should be able to actually use the program. To test this, we’ll simply run the multiqc command with the --help option like we did for FastQC:\n\nmultiqc --help\n\n /// MultiQC 🔍 | v1.15                                                                                                                                                                                                            \n                                                                                                                                                                                                                                   \n Usage: multiqc [OPTIONS] [ANALYSIS DIRECTORY]                                                                                                                                                                                     \n                                                                                                                                                                                                                                   \n MultiQC aggregates results from bioinformatics analyses across many samples into a single report.                                                                                                                                 \n It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.                                                       \n To run, supply with one or more directory to scan for analysis results. For example, to run in the current working directory, use 'multiqc .'                                                                                     \n                                                                                                                                                                                                                                   \n╭─ Main options ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ --force            -f  Overwrite any existing reports                                                                                                                                                                           │\n│ --config           -c  Specific config file to load, after those in MultiQC dir / home dir / working dir. (PATH)                                                                                                                │\n│ --cl-config            Specify MultiQC config YAML on the command line (TEXT)                                                                                                                                                   │\n│ --filename         -n  Report filename. Use 'stdout' to print to standard out. (TEXT)                                                                                                                                           │\n│ --outdir           -o  Create report in the specified output directory. (TEXT)                                                                                                                                                  │\n│ --ignore           -x  Ignore analysis files (GLOB EXPRESSION)                                                                                                                                                                  │\n│ --ignore-samples       Ignore sample names (GLOB EXPRESSION)                                                                                                                                                                    │\n│ --ignore-symlinks      Ignore symlinked directories and files                                                                                                                                                                   │\n│ --file-list        -l  Supply a file containing a list of file paths to be searched, one per row                                                                                                                                │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n[...truncated...]\n\nUnlike Lmod / module load, Conda will by default only keep a single environment active. Therefore, when you have one environment activate and then activate another, you will switch environments:\n# After running this command, the multiqc env will be active\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# After running his command, the trimgalore env will be active...\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# ...but the multiqc env will no longer be:\nmultiqc --help\nbash: multiqc: command not found...\nHowever, the conda activate --stack option enables you to have multiple Conda environments active at once:\n# Assuming you had trimgalore activated, now add the multiqc env:\nconda activate --stack /fs/ess/PAS0471/jelmer/conda/multiqc\n\nmultiqc --help\n# (Output not shown, but this should print help info)\n\ntrim_galore --help\n# (Output not shown, but this should print help info)\nNote that the command is conda activate --stack and not source activate --stack!\n\n\n\n3.2 Lines to add to your shell script\nAs mentioned above for Lmod modules, you need to load them in every shell session you want to use them — and the same is true for Conda environments. While Conda enviroments that are loaded in your interactive shell environment will “carry over” to the environment in which your script runs (even when you submit them to the Slurm queue with sbatch; topic of the next session), it is good practice to always include the necessary code to load/activate programs in your shell scripts.\nWhen the program you will run in a script is in an Lmod module, this only involves a module load call — e.g., for FastQC:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load fastqc\nWhen the program you will run in a script is in a Conda environment, this entails a module load command to load Conda itself, followed by a source activate command to load the relevant Conda environment — e.g. for MultiQC:\n#!/bin/bash\n\n# Load software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# Strict/safe Bash settings \nset -euo pipefail\n\n\n\n\n\n\nPerils of Conda environments inside scripts\n\n\n\n\nIn the example above, the set -euo pipefail line was moved below the source activate command, because the Conda activation procedure can otherwise result in “unbound variable” errors.\nAnother unfortunate aspect of Conda environments at OSC is the following. Problems can occur when you have a Conda environment active in your interactive shell while you submit a script as a batch job that activates a different environment.\nTherefore, it is generally a good idea to not have any Conda environments active in your interactive shell when submitting batch jobs5. To deactivate the currently active Conda environment, simply type conda deactivate without any arguments:\nconda deactivate"
  },
  {
    "objectID": "ref_software.html#bonus-i-creating-your-own-conda-environments",
    "href": "ref_software.html#bonus-i-creating-your-own-conda-environments",
    "title": "Software management",
    "section": "Bonus I: Creating your own Conda environments",
    "text": "Bonus I: Creating your own Conda environments\nWhen you want to create your own Conda environments and install programs, make sure to load the most recent miniconda3 module, which is currently not the default one. This is because installation has become much quicker and less likely to fail than in earlier versions. (Note that when we are just loading environments, like above, the version doesn’t matter).\nAs of August 2023, the most recent miniconda version is 23.3.1-py310 (recall that you can list available versions with module spider):\nmodule load miniconda3/23.3.1-py310\n\nOne-time Conda configuration\nBefore we can create our own environments, we first have to do some one-time configuration6. This will set the Conda “channels” (basically, software repositories) that we want to use when we install programs, including the relative priorities among channels (since one program may be available from multiple channels).\nWe can do this configuration with the config subcommand — run the following commands in your shell:\nconda config --add channels defaults     # Added first =&gt; lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last =&gt; highest priority\nLet’s check whether the configuration was successfully saved:\nconda config --get channels\n--add channels 'defaults'   # lowest priority\n--add channels 'bioconda'\n--add channels 'conda-forge'   # highest priority\n\n\n\n3.3 Example: Creating an environment for Trim Galore!\nTo practice using Conda, we will now create a Conda environment with the program Trim Galore! installed. Trim Galore! is a commonly used tool for quality trimming and adapter trimming of FASTQ files — we’ll learn more about it in a later session, since we will use it on our RNAseq data. It does not have a system-wide installation at OSC, unfortunately.\nHere is the command to all at once create a new Conda environment and install Trim Galore! into that environment:\n\n# (Don't run this)\nconda create -y -n trim-galore -c bioconda trim-galore\n\nLet’s break that command down:\n\ncreate is the Conda subcommand to create a new environment.\n-y is a flag that prevents us from being asked to confirm installation once Conda has determined what needs to be installed.\nFollowing the -n option, we can specify the name of the environment, so -n trim-galore means that we want our environment to be called trim-galore. We can use whatever name we like for the environment, but of course a descriptive yet concise name is a good idea. Since we are making a single-program environment, it makes sense to simply name it after the program.\nFollowing the -c option, we can specify a “channel” (repository) from which we want to install, so -c bioconda indicates we want to use the bioconda channel. (Given that we’ve done some config above, this is not always necessary, but it can be good to be explicit.)\nThe trim-galore at the end of the line simply tells Conda to install the package of that name. This is a “positional” argument to the command (note that there’s no option like -s before it): we put any software package(s) we want to install at the end of the command.\n\n\nSpecifying a version\nIf we want to be explicit about the version we want to install, we can add the version after = following the package name, and may also want to include that version number in the Conda environment’s name — try running the command below:\nconda create -y -n trim-galore-0.6.10 -c bioconda trim-galore=0.6.10\nCollecting package metadata (current_repodata.json): done  \nSolving environment: done\n# [...truncated...]\n\n\n\n\n\n\nSee the full output when I ran this command (Click to expand)\n\n\n\n\n\n\n\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 23.3.1\n  latest version: 23.7.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.7.2\n\n\n\n## Package Plan ##\n\n  environment location: /fs/project/PAS0471/jelmer/conda/trimgalore-0.6.10\n\n  added / updated specs:\n    - trim-galore=0.6.10\n\n\nThe following packages will be downloaded:\n\n    | package            | build                                            |\n    | ------------------ | ------------------------------------------------ |\n    | bz2file-0.98       | py_0           9 KB  conda-forge                 |\n    | cutadapt-1.18      | py37h14c3975_1         206 KB  bioconda          |\n    | fastqc-0.12.1      | hdfd78af_0        11.1 MB  bioconda              |\n    | pigz-2.6           | h27826a3_0          87 KB  conda-forge           |\n    | python-3.7.12      | hf930737_100_cpython        57.3 MB  conda-forge |\n    | trim-galore-0.6.10 | hdfd78af_0          45 KB  bioconda              |\n    | xopen-0.7.3        | py_0          11 KB  bioconda                    |\n    ------------------------------------------------------------\n                                           Total:        68.8 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n  alsa-lib           conda-forge/linux-64::alsa-lib-1.2.9-hd590300_0 \n  bz2file            conda-forge/noarch::bz2file-0.98-py_0 \n  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4 \n  ca-certificates    conda-forge/linux-64::ca-certificates-2023.7.22-hbcca054_0 \n  cairo              conda-forge/linux-64::cairo-1.16.0-hbbf8b49_1016 \n  cutadapt           bioconda/linux-64::cutadapt-1.18-py37h14c3975_1 \n  expat              conda-forge/linux-64::expat-2.5.0-hcb278e6_1 \n  fastqc             bioconda/noarch::fastqc-0.12.1-hdfd78af_0 \n  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-hab24e00_0 \n  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n  freetype           conda-forge/linux-64::freetype-2.12.1-hca18f0e_1 \n  gettext            conda-forge/linux-64::gettext-0.21.1-h27087fc_0 \n  giflib             conda-forge/linux-64::giflib-5.2.1-h0b41bf4_3 \n  graphite2          conda-forge/linux-64::graphite2-1.3.13-h58526e2_1001 \n  harfbuzz           conda-forge/linux-64::harfbuzz-7.3.0-hdb3a94d_0 \n  icu                conda-forge/linux-64::icu-72.1-hcb278e6_0 \n  keyutils           conda-forge/linux-64::keyutils-1.6.1-h166bdaf_0 \n  krb5               conda-forge/linux-64::krb5-1.21.2-h659d440_0 \n  lcms2              conda-forge/linux-64::lcms2-2.15-haa2dc70_1 \n  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.40-h41732ed_0 \n  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 \n  libcups            conda-forge/linux-64::libcups-2.3.3-h4637d8d_4 \n  libdeflate         conda-forge/linux-64::libdeflate-1.18-h0b41bf4_0 \n  libedit            conda-forge/linux-64::libedit-3.1.20191231-he28a2e2_2 \n  libexpat           conda-forge/linux-64::libexpat-2.5.0-hcb278e6_1 \n  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 \n  libgcc-ng          conda-forge/linux-64::libgcc-ng-13.1.0-he5830b7_0 \n  libglib            conda-forge/linux-64::libglib-2.76.4-hebfc3b9_0 \n  libgomp            conda-forge/linux-64::libgomp-13.1.0-he5830b7_0 \n  libiconv           conda-forge/linux-64::libiconv-1.17-h166bdaf_0 \n  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-2.1.5.1-h0b41bf4_0 \n  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0 \n  libpng             conda-forge/linux-64::libpng-1.6.39-h753d276_0 \n  libsqlite          conda-forge/linux-64::libsqlite-3.42.0-h2797004_0 \n  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-13.1.0-hfd8a6a1_0 \n  libtiff            conda-forge/linux-64::libtiff-4.5.1-h8b53f26_0 \n  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n  libwebp-base       conda-forge/linux-64::libwebp-base-1.3.1-hd590300_0 \n  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n  libzlib            conda-forge/linux-64::libzlib-1.2.13-hd590300_5 \n  ncurses            conda-forge/linux-64::ncurses-6.4-hcb278e6_0 \n  openjdk            conda-forge/linux-64::openjdk-20.0.0-h8e330f5_0 \n  openssl            conda-forge/linux-64::openssl-3.1.2-hd590300_0 \n  pcre2              conda-forge/linux-64::pcre2-10.40-hc3806b6_0 \n  perl               conda-forge/linux-64::perl-5.32.1-4_hd590300_perl5 \n  pigz               conda-forge/linux-64::pigz-2.6-h27826a3_0 \n  pip                conda-forge/noarch::pip-23.2.1-pyhd8ed1ab_0 \n  pixman             conda-forge/linux-64::pixman-0.40.0-h36c2ea0_0 \n  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 \n  python             conda-forge/linux-64::python-3.7.12-hf930737_100_cpython \n  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n  setuptools         conda-forge/noarch::setuptools-68.0.0-pyhd8ed1ab_0 \n  sqlite             conda-forge/linux-64::sqlite-3.42.0-h2c6b66d_0 \n  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0 \n  trim-galore        bioconda/noarch::trim-galore-0.6.10-hdfd78af_0 \n  wheel              conda-forge/noarch::wheel-0.41.1-pyhd8ed1ab_0 \n  xopen              bioconda/noarch::xopen-0.7.3-py_0 \n  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-h7f98852_1002 \n  xorg-inputproto    conda-forge/linux-64::xorg-inputproto-2.3.2-h7f98852_1002 \n  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002 \n  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0 \n  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0 \n  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.6-h8ee46fc_0 \n  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0 \n  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 \n  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004 \n  xorg-libxi         conda-forge/linux-64::xorg-libxi-1.7.10-h7f98852_0 \n  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0 \n  xorg-libxt         conda-forge/linux-64::xorg-libxt-1.3.0-hd590300_1 \n  xorg-libxtst       conda-forge/linux-64::xorg-libxtst-1.2.3-h7f98852_1002 \n  xorg-recordproto   conda-forge/linux-64::xorg-recordproto-1.14.2-h7f98852_1002 \n  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002 \n  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003 \n  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007 \n  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 \n  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5 \n  zstd               conda-forge/linux-64::zstd-1.5.2-hfc55251_7 \n\n\n\nDownloading and Extracting Packages\n                                                                                                                                                                                                                                   \nPreparing transaction: done                                                                                                                                                                                                        \nVerifying transaction: done                                                                                                                                                                                                        \nExecuting transaction: done                                                                                                                                                                                                        \n#                                                                                                                                                                                                                                  \n# To activate this environment, use                                                                                                                                                                                                \n#                                                                                                                                                                                                                                  \n#     $ conda activate trimgalore-0.6.10\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n\n\n\n\nNow, you should be able to activate the enviroment (using just it’s name – see the box below):\n# Activate the environment:\nsource activate trim-galore\n\n# Test if TrimGalore can be run - note, the command is 'trim_galore': \ntrim_galore --help\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nSpecifying the full path to the environment dir\n\n\n\nYou may have noticed above that we merely gave the enviroment a name (trim-galore or trim-galore-0.6.10), and did not tell it where to put this environment. Similarly, we were able to activate the environment with just its name. Conda assigns a personal default directory for its environments, somewhere in your Home directory.\nYou can install environments in a different location with the -p (instead of -n) option, for example:\nmkdir -p /fs/scratch/PAS0471/$USER/conda\nconda create -y -p /fs/scratch/PAS0471/$USER/conda/trim-galore -c bioconda trim-galore\nAnd when you want to load someone else’s Conda environments, you’ll always have to specify the full path to environment’s dir, like you did when loading an MCIC Conda environment above.\n\n\n\n\n\n\n3.4 Finding the Conda installation info online\nMinor variations on the conda create command above can be used to install almost any program for which a Conda package is available, which is the vast majority of open-source bioinformatics programs!\nHowever, you may be wondering how we would know:\n\nWhether the program is available and what its Conda package’s name is\nWhich Conda channel we should use\nWhich versions are available\n\nMy strategy to finding this out is to simply Google the program name together with “conda”, e.g. “cutadapt conda” if I wanted to install the CutAdapt program. Let’s see that in action:\n\n\n\nClick on that first link (it should always be the first Google hit):\n\n\n\n\nBuild the installation command\nI always take the top of the two example installation commands as a template, which is here: conda install -c bioconda cutadapt.\nYou may notice the install subcommand, which we haven’t yet seen. This would install Cutadapt into the currently activated Conda environment. Since our strategy here –and my general strategy– is to create a new environment each time you’re installing a program, just installing a program into whatever environment is currently active is not a great idea. To use the install command with a new environment, the strategy would be to first create an “empty” environment, and then run the install command.\nHowever, we saw above that we can do all of this in a single command. To build this create-plus-install command, all we need to do is replace install in the example command on the Conda website by create -y -n &lt;env-name&gt;. Then, our full command (without version specification) will be:\nconda create -y -n cutadapt -c bioconda cutadapt\nTo see which version will be installed by default, and to see which older versions are available:\n\n\n\nFor almost any other program, you can use the exact same procedure to find the Conda package and install it!\n\n\n\n\n\n\nA few more Conda commands to manage your environments\n\n\n\n\nExport a plain-text “YAML” file that contains the instructions to recreate your currently-active environment (useful for reproducibility!)\nconda env export &gt; my_env.yml\nAnd you can use the following to create a Conda environment from such a YAML file:\nconda env create -n my_env --force --file my_env.yml\nRemove an environment entirely:\nconda env remove -n cutadapt\nList all your conda environments:\nconda env list\nList all packages (programs) installed in an environment — due to dependencies, this can be a long list, even if you only actively installed one program:\nconda list -p /fs/ess/PAS0471/jelmer/conda/multiqc\n\n\n\n\n\n\n\n\n\nUse one environment per program (as here) or one per research project\n\n\n\nBelow are two reasonable ways to organize your Conda environments, and their respective advantages:\n\nHave one environment per program (my preference)\n\nEasier to keep an overview of what you have installed\nNo need to reinstall the same program across different projects\nLess risk of running into problems with your environment due to mutually incompatible software and complicated dependency situations\n\nHave one environment per research project\n\nYou just need to activate that one environment when you’re working on your project.\nEasier when you need to share your entire project with someone else (or yourself) on a different (super)computer.\n\n\nEven though it might seem easier, a third alternative, to simply install all programs across all projects in one single environment, is not recommended. This doesn’t benefit reproducibility, and your environment is likely to stop functioning properly sooner or later.\n(A side note: even when you want to install a single program, multiple programs are in fact nearly always installed: the programs that your target program depends on, i.e. “dependencies”.)"
  },
  {
    "objectID": "ref_software.html#bonus-ii-using-apptainer-containers",
    "href": "ref_software.html#bonus-ii-using-apptainer-containers",
    "title": "Software management",
    "section": "Bonus II: Using Apptainer containers",
    "text": "Bonus II: Using Apptainer containers\nBesides Conda, containers are another way to use bioinformatics programs at OSC that don’t have system-wide installations.\nContainers are similar to Virtual Machines and different from Conda environments in that they come with an entire operating system. This makes creating your own container “image” (see box below on terminology) much more involved than creating a Conda environment, and we will not cover that here.\nHowever, there are pre-existing container images available for most bioinformatics programs, and they can be easily found, downloaded, and used.\n\n\n\n\n\n\nContainer terminology\n\n\n\n\nContainer image: File (Apptainer) or files (Docker) that contain the container application.\nContainer (sensu stricto): A running container image.\nDefinition file (Apptainer) / Dockerfile (Docker): A plain text file that contains the recipe to build a container image.\n\n\n\nAmong container platforms, Apptainer (formerly known as Singularity) and especially Docker are the most widely used ones. At supercomputers like OSC, however, only Apptainer containers can be used. Luckily, the Apptainer program can work with Docker container images: it will convert them on the fly.\n\nFinding container images online\nThere are several online repositories with publicly available container images, but I would recommend BioContainers https://biocontainers.pro/registry or Quay.io https://quay.io/biocontainers.\nFor example, let’s look on the BioContainers website for a TrimGalore container image:\n\n\n\n\nThe search result on the BioContainers website after entering “trim galore” in the search box.\n\n\n\nClick on the only entry that is shown, trim-galore, which will get you to a page like this:\n\n\n\n\n\nAs you can see, this website also includes Conda installation instructions — to see the container results, scroll down and you should see this:\n\n\n\n\nAfter scrolling down on the results page, you should see a recent available container image. Note that the command shown is singularity run, but we will use the more up-to-date apptainer run later.\n\n\n\nThe version tag that is shown (0.6.9--hdfd78af_0 above) pertains to the version of TrimGalore, but the result that is shown here is not will always the container image(s) with the most recent version. To see a list of all available images, click on the Packages and Containers tab towards the top, and then sort by Last Update:\n\n\n\n\nThe logo with the large S depicts Singularity/Apptainer containers.\n\n\n\nWhenever both a Singularity/Apptainer and a Docker image for the desired version of the program is available, use the Singularity/Apptainer image. This is because those don’t have to be converted, while Docker images do. But when the version you want is only available as a Docker image, that will work too: as mentioned above, it will be automatically converted to the proper format.\n\n\n\nRunning a container image\nWhen you’ve found a container image that you want to use, copy its URL from the BioContainers website. For example, for the most recent TrimGalore version as of September 2023: https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0.\nYou could also copy the full command — however, we will modify that in two ways:\n\nWe will use the more up-to-date apptainer command7\nWe’ll use the exec subcommand instead of run, which allows us to enter a custom command to run in the container (the run subcommand would only run some preset default action, which is rarely useful for our purposes).\n\nAs such, our base command to run TrimGalore in the container will be as follows:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0\n# (Don't run this, we'll need to add a TrimGalore command)\n\n\n\n\n\n\nYou can’t use the Docker URL as-is\n\n\n\nIf you want to use a Docker container, the listed quasi-URL on BioContainers will start with “quay.io”. In your apptainer exec command, you need to preface this URL with docker://. For instance:\napptainer exec docker://quay.io/biocontainers/trim-galore:0.6.10--hdfd78af_0\n\n\nAfter the code above, we would finish our command by simply entering a TrimGalore command in the exact same way as we would when running TrimGalore outside of the context of a container. For example, to just print the help info like we’ve been doing before, the TrimGalore command is:\ntrim_galore --help\nAnd to run that inside the container, our full command will be:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0 \\\n    trim_galore --help\nINFO:    Downloading network image\n321.4MiB / 321.4MiB [===================================================================================================================================] 100 % 3.0 MiB/s 0s\nWARNING: Environment variable LD_PRELOAD already has value [], will not forward new value [/apps/xalt/xalt/lib64/libxalt_init.so] from parent process environment\n\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nNote\n\n\n\n\nThe Apptainer/Singularity software does not need to be loaded at OSC, it is always automatically loaded.\nThe \\ in the code above allows us to continue a command on another line.\n\n\n\nSo, all that is different from running a program inside a container versus a locally installed program, is that you prefix apptainer exec &lt;URL&gt; when using a container.\nThe first time you run this command, the container will be downloaded, which can take a few minutes (by default it will be downloaded to ~/.apptainer/cache, but you can change this by setting the $APPTAINER_CACHEDIR environment variable). After that, the downloaded image will be used and the command should be executed about as instantaneously as when running TrimGalore outside of a container.\nYou will keep seeing the warning WARNING: Environment variable LD_PRELOAD [...] whenever you run a container, but this is nothing to worry about.\nFinally, the --help option above can also simply be replaced by a host of other TrimGalore options and arguments so as to actually trim a pair of FASTQ files, i.e. with input and output files. You can just specify the paths to those files in the same way as without a container, this will work out of the box!\n\n\n\n\n\n\nWhen to use a Container versus Conda\n\n\n\n\nCurrently, my default is to first try installation with Conda. But I will try a container when installing a program through Conda fails, or my Conda environment misbehaves (e.g., memory errors with dumped cores).\nWhen you need multiple programs in quick succession or in a single command (e.g., you’re piping the output of one program into a second program), it can be more convenient to have those programs installed in a single environmnent or container. Pre-built multi-program containers are not as easy to find. And since building your own Conda environment is easier than building your own container, this is a situation where you might prefer Conda."
  },
  {
    "objectID": "ref_software.html#footnotes",
    "href": "ref_software.html#footnotes",
    "title": "Software management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, we call module the command and e.g. spider the subcommand. But sometimes the subcommands are also simply called commands.↩︎\nWhen your personal computer asks you to “authenticate” while you are installing something, you are authenticating yourself as a user with administrator privileges. At OSC, you don’t have such privileges.↩︎\nOther software upon which the software that you are trying to install depends.↩︎\nIt isn’t feasible to keep separate environments around for many different versions of a program, mostly because Conda environments contain a very large number of files, and OSC has file number quotas. This is why I have in many cases chosen the strategy of just updating the version within the same environment.↩︎\nUnless you first deactivate any active environments in your script.↩︎\nThat is, these settings will be saved somewhere in your OSC home directory, and you never have to set them again unless you need to make changes.↩︎\nThough note that as of September 2023, the singularity command does still work, and it will probably continue to work for a while.↩︎"
  }
]